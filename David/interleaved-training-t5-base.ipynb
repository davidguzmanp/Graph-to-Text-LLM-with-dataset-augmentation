{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"2a8badda5171f6c1da75e6dcec216359e8635e393e06f848b1b87b76c1bdea5e"}},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"27dac6771009442986c337835ac2fab0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81ab3401681e449faca4585337346d4a","IPY_MODEL_ac24b2b5a91b48e6a8746e3e0a6bdd56","IPY_MODEL_099b3ea61045469caa5684f5d4b5fa38"],"layout":"IPY_MODEL_d41d0e0245a142589c8f5767ce8e0d3b"}},"81ab3401681e449faca4585337346d4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e466a23c1954b84bd29cee2299101b5","placeholder":"​","style":"IPY_MODEL_094afeea467543ae879be701e4a73d63","value":"100%"}},"ac24b2b5a91b48e6a8746e3e0a6bdd56":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa9427c68daf40d3af8410141bfe94b3","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6dfc55f38ffe49e59ccb600b0de75088","value":3}},"099b3ea61045469caa5684f5d4b5fa38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec91a1ad9e7b4bafbc79f9ddfaa0296e","placeholder":"​","style":"IPY_MODEL_4cec0b2d487b4c13ad153e6924cc8e4a","value":" 3/3 [00:00&lt;00:00, 65.75it/s]"}},"d41d0e0245a142589c8f5767ce8e0d3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e466a23c1954b84bd29cee2299101b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"094afeea467543ae879be701e4a73d63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa9427c68daf40d3af8410141bfe94b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dfc55f38ffe49e59ccb600b0de75088":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec91a1ad9e7b4bafbc79f9ddfaa0296e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cec0b2d487b4c13ad153e6924cc8e4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1af09f1f71524f44b3f79ed0427c8559":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4027186c2a1646c7a47db3e69d3ef93b","IPY_MODEL_c224ebc06fcf44dc80e8285b6578680e","IPY_MODEL_f3f96e77a1134c5c93ba218b8261ec0f"],"layout":"IPY_MODEL_45509cc5af034006bade5bff3fe9ea3b"}},"4027186c2a1646c7a47db3e69d3ef93b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7719bedd5d2544a38a46f058d0ef1a7e","placeholder":"​","style":"IPY_MODEL_305d7d10c879476ea87f924e2f802958","value":"100%"}},"c224ebc06fcf44dc80e8285b6578680e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4731ce03cfa84014bce0054c8611b4f9","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6af2a7876d954b93aca5a16930c9f079","value":3}},"f3f96e77a1134c5c93ba218b8261ec0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_528070b64e7d4bb0a891f1dbe89ae5bc","placeholder":"​","style":"IPY_MODEL_a99d41291bf84041a091180eb477e519","value":" 3/3 [00:00&lt;00:00, 106.45it/s]"}},"45509cc5af034006bade5bff3fe9ea3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7719bedd5d2544a38a46f058d0ef1a7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"305d7d10c879476ea87f924e2f802958":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4731ce03cfa84014bce0054c8611b4f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6af2a7876d954b93aca5a16930c9f079":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"528070b64e7d4bb0a891f1dbe89ae5bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a99d41291bf84041a091180eb477e519":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d15948f114040928a325e1665e0b1bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9aa2a3f7d6ec414cb6bc735391cc7462","IPY_MODEL_a72bd5b05ddc4e20bd777a1e51bff87f","IPY_MODEL_45e968b74fdc440382f08bfe83797d7f"],"layout":"IPY_MODEL_27829187a83e45859d0adf6deb1a5f12"}},"9aa2a3f7d6ec414cb6bc735391cc7462":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01cfd32ca9ea4e54a1bff6119613d64a","placeholder":"​","style":"IPY_MODEL_abcb191fcc264867b122d58caf9e3652","value":"Downloading (…)ve/main/spiece.model: 100%"}},"a72bd5b05ddc4e20bd777a1e51bff87f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1375be65005469a9c05fb385f1597db","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_194bc9079f024606b19ed153cd2d2226","value":791656}},"45e968b74fdc440382f08bfe83797d7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccf95078f5474a8a9917dc95c2d296df","placeholder":"​","style":"IPY_MODEL_8c955109118d4ffcb3b5b9db8520eb2a","value":" 792k/792k [00:00&lt;00:00, 3.16MB/s]"}},"27829187a83e45859d0adf6deb1a5f12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01cfd32ca9ea4e54a1bff6119613d64a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abcb191fcc264867b122d58caf9e3652":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1375be65005469a9c05fb385f1597db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"194bc9079f024606b19ed153cd2d2226":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ccf95078f5474a8a9917dc95c2d296df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c955109118d4ffcb3b5b9db8520eb2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"836b768308904b9e98965838df6d56cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c26f17223f9049fabbeeef900dc0f084","IPY_MODEL_096b9fdf24f44aedaa16e9f31bff5e74","IPY_MODEL_3f2e2e165fcf4575bfd2127e0f392e0d"],"layout":"IPY_MODEL_511bf71170c94239b561c6ed9c7a2014"}},"c26f17223f9049fabbeeef900dc0f084":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e0a091ca3f743b599684ec709c0724d","placeholder":"​","style":"IPY_MODEL_7f042786107f41a5be4dffec5461e220","value":"Downloading (…)lve/main/config.json: 100%"}},"096b9fdf24f44aedaa16e9f31bff5e74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98782b2ca67b42eabe9128090a4f3eec","max":1206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b61db7dc7854a6da1f96c46dfbdf72c","value":1206}},"3f2e2e165fcf4575bfd2127e0f392e0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a85ddf4d0fdf44ba815ca3f03b476407","placeholder":"​","style":"IPY_MODEL_46f3784588fe467cbddcf6e15bfcaa82","value":" 1.21k/1.21k [00:00&lt;00:00, 21.9kB/s]"}},"511bf71170c94239b561c6ed9c7a2014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e0a091ca3f743b599684ec709c0724d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f042786107f41a5be4dffec5461e220":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98782b2ca67b42eabe9128090a4f3eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b61db7dc7854a6da1f96c46dfbdf72c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a85ddf4d0fdf44ba815ca3f03b476407":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46f3784588fe467cbddcf6e15bfcaa82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76ea121789c34ab0ae3db2a11ce092dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0755e20e9e894e0782a6dc013944a602","IPY_MODEL_16d5d7274b774723913d9a9df040db90","IPY_MODEL_ca6af7cc25b745999b2e819972a5794c"],"layout":"IPY_MODEL_46a8ce2527bb48088f5a414f6df0ee73"}},"0755e20e9e894e0782a6dc013944a602":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed369610f713419fae0a9c26abee2b91","placeholder":"​","style":"IPY_MODEL_fe46615805e146788d96ca94f066fd87","value":"Downloading pytorch_model.bin: 100%"}},"16d5d7274b774723913d9a9df040db90":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f15c2f29bfa54944989bf35497436615","max":242065649,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c7e7ef1e3a34c228e0972f094716cef","value":242065649}},"ca6af7cc25b745999b2e819972a5794c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2766052cd6cd43288677e89f524222aa","placeholder":"​","style":"IPY_MODEL_c1326237db9f4a39bb312b80678be410","value":" 242M/242M [00:01&lt;00:00, 240MB/s]"}},"46a8ce2527bb48088f5a414f6df0ee73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed369610f713419fae0a9c26abee2b91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe46615805e146788d96ca94f066fd87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f15c2f29bfa54944989bf35497436615":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c7e7ef1e3a34c228e0972f094716cef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2766052cd6cd43288677e89f524222aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1326237db9f4a39bb312b80678be410":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2bd17acda9f4a4cb40387e7a16a4aad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34b7d58e7c434652ab4a9f4cecef0791","IPY_MODEL_c406fb3d9ff8428da7b08d1dbf76e81a","IPY_MODEL_ceff9ff6affd4f168cff5c526d56fcd7"],"layout":"IPY_MODEL_13c9318a176f42ecac5d6162d510395f"}},"34b7d58e7c434652ab4a9f4cecef0791":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99ea585e29ff4e2a966d1c69be32cb97","placeholder":"​","style":"IPY_MODEL_1f85b576a95d4b6a8ee70e78ecaa34a7","value":"Downloading (…)neration_config.json: 100%"}},"c406fb3d9ff8428da7b08d1dbf76e81a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c80227b65297408db3c88583a2291f7b","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d55e7db195b44955b09e0f04598a3598","value":147}},"ceff9ff6affd4f168cff5c526d56fcd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85e687ff32694c0aa1bedacf5b4bbf21","placeholder":"​","style":"IPY_MODEL_0982aa5f0748431e86626537055d9978","value":" 147/147 [00:00&lt;00:00, 2.48kB/s]"}},"13c9318a176f42ecac5d6162d510395f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99ea585e29ff4e2a966d1c69be32cb97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f85b576a95d4b6a8ee70e78ecaa34a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c80227b65297408db3c88583a2291f7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d55e7db195b44955b09e0f04598a3598":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85e687ff32694c0aa1bedacf5b4bbf21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0982aa5f0748431e86626537055d9978":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n!pip install datasets\n\n!pip install transformers \n!pip install sentencepiece\n\n!pip install sacrebleu\n'''","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahHaXoNB9iBc","outputId":"7a29568e-467f-4334-b715-11bd6c8b649d","execution":{"iopub.status.busy":"2023-07-04T17:50:53.663958Z","iopub.execute_input":"2023-07-04T17:50:53.664252Z","iopub.status.idle":"2023-07-04T17:50:53.679912Z","shell.execute_reply.started":"2023-07-04T17:50:53.664226Z","shell.execute_reply":"2023-07-04T17:50:53.678989Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\n!pip install datasets\\n\\n!pip install transformers \\n!pip install sentencepiece\\n\\n!pip install sacrebleu\\n'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom torch.nn.parallel import DataParallel\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\nmodel = DataParallel(model)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255,"referenced_widgets":["0d15948f114040928a325e1665e0b1bc","9aa2a3f7d6ec414cb6bc735391cc7462","a72bd5b05ddc4e20bd777a1e51bff87f","45e968b74fdc440382f08bfe83797d7f","27829187a83e45859d0adf6deb1a5f12","01cfd32ca9ea4e54a1bff6119613d64a","abcb191fcc264867b122d58caf9e3652","a1375be65005469a9c05fb385f1597db","194bc9079f024606b19ed153cd2d2226","ccf95078f5474a8a9917dc95c2d296df","8c955109118d4ffcb3b5b9db8520eb2a","836b768308904b9e98965838df6d56cc","c26f17223f9049fabbeeef900dc0f084","096b9fdf24f44aedaa16e9f31bff5e74","3f2e2e165fcf4575bfd2127e0f392e0d","511bf71170c94239b561c6ed9c7a2014","3e0a091ca3f743b599684ec709c0724d","7f042786107f41a5be4dffec5461e220","98782b2ca67b42eabe9128090a4f3eec","6b61db7dc7854a6da1f96c46dfbdf72c","a85ddf4d0fdf44ba815ca3f03b476407","46f3784588fe467cbddcf6e15bfcaa82","76ea121789c34ab0ae3db2a11ce092dc","0755e20e9e894e0782a6dc013944a602","16d5d7274b774723913d9a9df040db90","ca6af7cc25b745999b2e819972a5794c","46a8ce2527bb48088f5a414f6df0ee73","ed369610f713419fae0a9c26abee2b91","fe46615805e146788d96ca94f066fd87","f15c2f29bfa54944989bf35497436615","8c7e7ef1e3a34c228e0972f094716cef","2766052cd6cd43288677e89f524222aa","c1326237db9f4a39bb312b80678be410","c2bd17acda9f4a4cb40387e7a16a4aad","34b7d58e7c434652ab4a9f4cecef0791","c406fb3d9ff8428da7b08d1dbf76e81a","ceff9ff6affd4f168cff5c526d56fcd7","13c9318a176f42ecac5d6162d510395f","99ea585e29ff4e2a966d1c69be32cb97","1f85b576a95d4b6a8ee70e78ecaa34a7","c80227b65297408db3c88583a2291f7b","d55e7db195b44955b09e0f04598a3598","85e687ff32694c0aa1bedacf5b4bbf21","0982aa5f0748431e86626537055d9978"]},"id":"7Ms8N01X9MlZ","outputId":"331ae2b5-3110-47bb-e7a1-01240b6cab7c","execution":{"iopub.status.busy":"2023-07-04T17:50:53.681907Z","iopub.execute_input":"2023-07-04T17:50:53.682538Z","iopub.status.idle":"2023-07-04T17:51:23.877895Z","shell.execute_reply.started":"2023-07-04T17:50:53.682503Z","shell.execute_reply":"2023-07-04T17:51:23.876937Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15eec9bfb4fa47299bff136b66351a5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4f3b5c0c0054499bfb8384a22450fbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccc859bf9ea94e3b8813e90dbbad0f57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d34978b5621047b082867ad2078ea23b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a15f010186684d07af8f97ac82de3e68"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a79a2ff0e043e0ace92971f1c9c386"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa63a0b73c3f40aab0ad2573ffd5d2d6"}},"metadata":{}}]},{"cell_type":"code","source":"new_tokens = ['<H>', '<R>', '<T>']\nnew_tokens_vocab = {}\nnew_tokens_vocab['additional_special_tokens'] = []\nfor idx, t in enumerate(new_tokens):\n    new_tokens_vocab['additional_special_tokens'].append(t)\nnum_added_toks = tokenizer.add_special_tokens(new_tokens_vocab)\n\ntokenizer.add_tokens(\"[MASK]\")\ntokenizer.mask_token = \"[MASK]\"\ntokenizer.mask_token_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:51:23.880248Z","iopub.execute_input":"2023-07-04T17:51:23.881010Z","iopub.status.idle":"2023-07-04T17:51:23.888012Z","shell.execute_reply.started":"2023-07-04T17:51:23.880964Z","shell.execute_reply":"2023-07-04T17:51:23.887120Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class WebNLGDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.prefix = \"translate from Graph to Text: \"\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # preprocess the input graph\n        try:\n            triples = item['original_triple_sets']['otriple_set']\n            input_text = self.prefix\n            for outer_list in triples: \n                for triple in outer_list:\n                    triple_txt = triple.split(\"|\")\n                    input_text += \" <H> \" + triple_txt[0] + \" <R> \" + triple_txt[1] + \" <T> \" + triple_txt[2]\n        except (KeyError, IndexError):\n            print(\"1\")\n            print(item['original_triple_sets']['otriple_set'])\n            print(item['original_triple_sets']['otriple_set'][0])\n            print(triples)\n            input_text = self.prefix\n        # preprocess the target text\n        try:\n            target_text = item['lex']['text'][0]\n        except (KeyError, IndexError):\n            print(\"2\")\n            print(item)\n            #print(item['original_triple_sets']['otriple_set'])\n            target_text = \"\"\n        #print(item)\n        #print(input_text)\n        # encode the inputs and targets using the tokenizer\n        input_ids = tokenizer.encode(input_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n        target_ids = tokenizer.encode(target_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n        return input_ids.squeeze(0), target_ids.squeeze(0)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:51:23.890933Z","iopub.execute_input":"2023-07-04T17:51:23.891501Z","iopub.status.idle":"2023-07-04T17:51:23.902692Z","shell.execute_reply.started":"2023-07-04T17:51:23.891467Z","shell.execute_reply":"2023-07-04T17:51:23.901955Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class DartDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.prefix = \"translate from Graph to Text: \"\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # preprocess the input graph\n        try:\n            tripleset = item['tripleset']\n            input_text = self.prefix\n            for triple in tripleset: \n                triple_txt = triple\n                input_text += \" <H> \" + triple_txt[0] + \" <R> \" + triple_txt[1] + \" <T> \" + triple_txt[2]\n        except (KeyError, IndexError):\n            print(\"1\")\n            print(item['tripleset'])\n            input_text = self.prefix\n        # preprocess the target text\n        try:\n            target_text = item['annotations']['text'][0]\n        except (KeyError, IndexError):\n            print(\"2\")\n            print(item)\n            target_text = \"\"\n        # encode the inputs and targets using the tokenizer\n        input_ids = tokenizer.encode(input_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n        target_ids = tokenizer.encode(target_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n\n        #print decoding of input and target\n        #print(tokenizer.decode(input_ids[0], skip_special_tokens=True))\n        #print(tokenizer.decode(target_ids[0], skip_special_tokens=True))\n        return input_ids.squeeze(0), target_ids.squeeze(0)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:51:23.904041Z","iopub.execute_input":"2023-07-04T17:51:23.904592Z","iopub.status.idle":"2023-07-04T17:51:23.919075Z","shell.execute_reply.started":"2023-07-04T17:51:23.904540Z","shell.execute_reply":"2023-07-04T17:51:23.918145Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class E2ENLGDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.prefix = \"Translate from Meaning Representation to Text: \"\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # preprocess the input meaning representation\n        try:\n            input_text = self.prefix + item['meaning_representation']\n        except KeyError:\n            print(\"Failed to access 'meaning_representation' for item at index: \", idx)\n            input_text = self.prefix\n\n        # preprocess the target text\n        try:\n            target_text = item['human_reference']\n        except KeyError:\n            print(\"Failed to access 'human_reference' for item at index: \", idx)\n            target_text = \"\"\n\n        # encode the inputs and targets using the tokenizer\n        input_ids = tokenizer.encode(input_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n        target_ids = tokenizer.encode(target_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n\n        # print decoded input and target texts\n        #print(\"Input Text: \", tokenizer.decode(input_ids[0], skip_special_tokens=True))\n        #print(\"Target Text: \", tokenizer.decode(target_ids[0], skip_special_tokens=True))\n\n        return input_ids.squeeze(0), target_ids.squeeze(0)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:51:23.921779Z","iopub.execute_input":"2023-07-04T17:51:23.922036Z","iopub.status.idle":"2023-07-04T17:51:23.932891Z","shell.execute_reply.started":"2023-07-04T17:51:23.922015Z","shell.execute_reply":"2023-07-04T17:51:23.932017Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class BalancedCombinedDataset(Dataset):\n    def __init__(self, dataset1, dataset2, dataset3):\n        self.datasets = [dataset1, dataset2, dataset3]\n        self.lengths = [len(d) for d in self.datasets]\n        self.max_length = max(self.lengths)\n\n    def __len__(self):\n        return self.max_length * len(self.datasets)\n\n    def __getitem__(self, idx):\n        dataset_idx = idx % len(self.datasets)\n        item_idx = idx // len(self.datasets)\n        item_idx = item_idx % self.lengths[dataset_idx]  # repeat smaller datasets\n        return self.datasets[dataset_idx][item_idx]\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:51:23.934244Z","iopub.execute_input":"2023-07-04T17:51:23.934619Z","iopub.status.idle":"2023-07-04T17:51:23.946278Z","shell.execute_reply.started":"2023-07-04T17:51:23.934588Z","shell.execute_reply":"2023-07-04T17:51:23.945307Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"MAX_INPUT_LENGTH = 128\nMAX_TARGET_LENGTH = 128\ntokenizer.model_max_length = MAX_INPUT_LENGTH\nmodel.module.config.max_length = MAX_TARGET_LENGTH\n\n# set up the device (GPU or CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1jjIPRO9Mla","outputId":"c138f2cb-2966-4872-ac87-616a2d4119d1","_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-07-04T17:51:23.947247Z","iopub.execute_input":"2023-07-04T17:51:23.947511Z","iopub.status.idle":"2023-07-04T17:51:30.177335Z","shell.execute_reply.started":"2023-07-04T17:51:23.947483Z","shell.execute_reply":"2023-07-04T17:51:30.176345Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): T5ForConditionalGeneration(\n    (shared): Embedding(32128, 768)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32128, 768)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n                (relative_attention_bias): Embedding(32, 12)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-11): 11 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=768, out_features=768, bias=False)\n                (k): Linear(in_features=768, out_features=768, bias=False)\n                (v): Linear(in_features=768, out_features=768, bias=False)\n                (o): Linear(in_features=768, out_features=768, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n                (wo): Linear(in_features=2048, out_features=768, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# load the WebNLG dataset\nwebnlg_data = load_dataset('web_nlg', 'release_v3.0_en')['train']\ndart_data = load_dataset('dart', split='train', ignore_verifications=True)\ne2enlg_data = load_dataset('e2e_nlg', split='train')\n\nwebnlg_data_val = load_dataset('web_nlg', 'release_v3.0_en')['dev']\ndart_data_val = load_dataset('dart', split='validation', ignore_verifications=True)\ne2enlg_data_val = load_dataset('e2e_nlg', split='validation')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87,"referenced_widgets":["1af09f1f71524f44b3f79ed0427c8559","4027186c2a1646c7a47db3e69d3ef93b","c224ebc06fcf44dc80e8285b6578680e","f3f96e77a1134c5c93ba218b8261ec0f","45509cc5af034006bade5bff3fe9ea3b","7719bedd5d2544a38a46f058d0ef1a7e","305d7d10c879476ea87f924e2f802958","4731ce03cfa84014bce0054c8611b4f9","6af2a7876d954b93aca5a16930c9f079","528070b64e7d4bb0a891f1dbe89ae5bc","a99d41291bf84041a091180eb477e519"]},"id":"qEf48SzQ9Mla","outputId":"4c918f52-9562-4b13-f519-0f20efe57b2d","execution":{"iopub.status.busy":"2023-07-04T17:51:30.180367Z","iopub.execute_input":"2023-07-04T17:51:30.180769Z","iopub.status.idle":"2023-07-04T17:51:59.737561Z","shell.execute_reply.started":"2023-07-04T17:51:30.180736Z","shell.execute_reply":"2023-07-04T17:51:59.736601Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0169aba2063b4a8bbd6d4c1de76945d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/2.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1219bc796c91426ea5df767821f33681"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset web_nlg/release_v3.0_en (download: 24.32 MiB, generated: 15.75 MiB, post-processed: Unknown size, total: 40.07 MiB) to /root/.cache/huggingface/datasets/web_nlg/release_v3.0_en/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae8b76b1ca84f01a01076e58d5e4041"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/13211 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/1667 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5713 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset web_nlg downloaded and prepared to /root/.cache/huggingface/datasets/web_nlg/release_v3.0_en/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e61b66cf28374a9bb7321ccab0fbbcb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67c87788a3ca4797a6e71d174d791284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e66675c02664689a59709424845f4be"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset dart/default (download: 28.01 MiB, generated: 16.60 MiB, post-processed: Unknown size, total: 44.61 MiB) to /root/.cache/huggingface/datasets/dart/default/0.0.0/bbf058468e494e171c5731ee18aa81f340fca22c46f8d3726fc7335d721052a3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25f095c9461c4bddbcff459bd779e27e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea4e9e8f0d23425a981c904980e0e15b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/225k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7785eb8e21e247e8b91238fa00c9278b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd714d4a064e46d881c6e3a6b8ce55d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"316e0e4f6fe0405dba47fded257549cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/30526 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2768 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6959 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"078c04c6432445f6bb9ee3e026e49c04"}},"metadata":{}},{"name":"stdout","text":"Dataset dart downloaded and prepared to /root/.cache/huggingface/datasets/dart/default/0.0.0/bbf058468e494e171c5731ee18aa81f340fca22c46f8d3726fc7335d721052a3. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.71k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86a395c0731b4c6aa29f4fbe537a40a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bfaf8008984425bbc1f8473ffaee1b9"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset e2e_nlg/default (download: 11.27 MiB, generated: 11.38 MiB, post-processed: Unknown size, total: 22.64 MiB) to /root/.cache/huggingface/datasets/e2e_nlg/default/0.0.0/bfeceb720929c2705bd227d1cfe5eaaab102a0bdac10dad618dac1e00c737430...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba332637a914c03bf2ab05df5c701c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce209fc8461445984e0dd8b4d15624e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/103k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304df4a8fe0442a0ba8f32676f680eac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/107k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f848d9302b24c78a3b6267d7860dfda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a9be2b65b34b239cf42b0185f0c245"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/42061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/4672 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4693 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset e2e_nlg downloaded and prepared to /root/.cache/huggingface/datasets/e2e_nlg/default/0.0.0/bfeceb720929c2705bd227d1cfe5eaaab102a0bdac10dad618dac1e00c737430. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4626ffb92be4b809ec8c315220cab99"}},"metadata":{}}]},{"cell_type":"code","source":"webnlg_dataset = WebNLGDataset(webnlg_data)\ndart_dataset = DartDataset(dart_data)\ne2enlg_dataset = E2ENLGDataset(e2enlg_data)\n\ntrain_data = BalancedCombinedDataset(webnlg_dataset, dart_dataset, e2enlg_dataset)\n\nwebnlg_dataset_val = WebNLGDataset(webnlg_data_val)\ndart_dataset_val = DartDataset(dart_data_val)\ne2enlg_dataset_val = E2ENLGDataset(e2enlg_data_val)\n\n\nval_data = BalancedCombinedDataset(webnlg_dataset_val, dart_dataset_val, e2enlg_dataset_val)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:51:59.739154Z","iopub.execute_input":"2023-07-04T17:51:59.739509Z","iopub.status.idle":"2023-07-04T17:51:59.745815Z","shell.execute_reply.started":"2023-07-04T17:51:59.739474Z","shell.execute_reply":"2023-07-04T17:51:59.744583Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Adaptive pretraining","metadata":{}},{"cell_type":"markdown","source":"For STA, we fine-tuned the PLMs on a small amount of labeled data from the target task using a maximum likelihood estimation (MLE) objective. This involves training the model to maximize the likelihood of generating the correct output given the input graph and labeled data. This process helps to further adapt the PLM to the specific requirements of the target task and improve its performance on that task.","metadata":{}},{"cell_type":"code","source":"import random\n\npretrain_texts = []\nfor sample in dataset:\n    try:\n        text = sample['lex']['text'][0]\n        pretrain_texts.append(text)\n    except (KeyError, IndexError):\n        continue\n\ntokenized_inputs = tokenizer(pretrain_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\ninput_ids = tokenized_inputs['input_ids']\nattention_mask = tokenized_inputs['attention_mask']\n\npretrain_data = torch.utils.data.TensorDataset(input_ids, attention_mask)\n\npretrain_loader = torch.utils.data.DataLoader(pretrain_data, batch_size=int(60), shuffle=True)\n\npretrain_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\npretrain_criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\npretrain_epochs = 2  # Set the number of pre-training epochs\nmasking_prob = 0.15  # Probability of masking a token\n\nif tokenizer.mask_token is None:\n    # Manually set a mask token if not already defined\n    tokenizer.add_tokens(\"[MASK]\")\n    tokenizer.mask_token = \"[MASK]\"\n    tokenizer.mask_token_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n\nfor epoch in range(pretrain_epochs):\n    running_loss = 0.0\n    for inputs, attention_mask in pretrain_loader:\n        inputs = inputs.to(device)\n        attention_mask = attention_mask.to(device)\n        batch_size, seq_length = inputs.shape\n        \n        # Create a mask for randomly selected tokens\n        mask = torch.rand(inputs.shape) < masking_prob\n        \n        # Randomly replace selected tokens with [MASK] token\n        masked_inputs = inputs.clone()\n        masked_inputs[mask] = tokenizer.mask_token_id\n        \n        pretrain_optimizer.zero_grad()\n        outputs = model(input_ids=masked_inputs, attention_mask=attention_mask, decoder_input_ids=inputs)\n        \n        # Compute the loss only for the masked tokens\n        masked_logits = outputs.logits[mask]\n        masked_labels = inputs[mask]\n        loss = pretrain_criterion(masked_logits.view(-1, masked_logits.size(-1)), masked_labels.view(-1))\n        \n        loss.backward()\n        pretrain_optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    \n    epoch_loss = running_loss / len(pretrain_data)\n    print(f\"Pretrain Epoch {epoch+1}/{pretrain_epochs} - loss: {epoch_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T12:40:12.588714Z","iopub.execute_input":"2023-07-04T12:40:12.589104Z","iopub.status.idle":"2023-07-04T12:45:27.596581Z","shell.execute_reply.started":"2023-07-04T12:40:12.589072Z","shell.execute_reply":"2023-07-04T12:45:27.594646Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Pretrain Epoch 1/2 - loss: 2.0408\nPretrain Epoch 2/2 - loss: 0.1616\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For LMA, we first fine-tuned the PLMs on a small amount of task-specific data using a masked language modeling objective. This involves randomly masking some tokens in the input sequence and training the model to predict the masked tokens based on the context provided by the unmasked tokens. This process helps to adapt the PLM to the specific characteristics of the target task and improve its performance on that task.","metadata":{}},{"cell_type":"markdown","source":"# Finetuning","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n\"\"\"# Setting up the data loader\ntrain_data = WebNLGDataset(dataset)\nval_dataset = load_dataset('web_nlg', 'release_v3.0_en')['dev']\nval_data = WebNLGDataset(val_dataset)  # You need to prepare the validation dataset similarly to your training dataset\"\"\"\n\nbatch_size = 32\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)  # No need to shuffle the validation set\n\n# Setting up the optimizer and the loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n# Initialize early stopping parameters\nearly_stopping_patience = 2  # Number of epochs with no improvement after which training will be stopped\nno_improve_epochs = 0\nmin_val_loss = float('inf')\n\nnum_epochs = 1\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in tqdm(train_loader):\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=targets)\n        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    epoch_loss = running_loss / len(train_data)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {epoch_loss:.4f}\")\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            outputs = model(inputs, labels=targets)\n            loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n            val_loss += loss.item() * inputs.size(0)\n    val_loss = val_loss / len(val_data)\n    print(f\"Epoch {epoch+1}/{num_epochs} - Validation loss: {val_loss:.4f}\")\n    \n    # Check early stopping condition\n    if val_loss < min_val_loss:\n        min_val_loss = val_loss\n        no_improve_epochs = 0\n    else:\n        no_improve_epochs += 1\n        if no_improve_epochs >= early_stopping_patience:\n            print(\"Early stopping invoked.\")\n            break\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:51:59.747316Z","iopub.execute_input":"2023-07-04T17:51:59.747939Z","iopub.status.idle":"2023-07-04T19:20:31.232978Z","shell.execute_reply.started":"2023-07-04T17:51:59.747909Z","shell.execute_reply":"2023-07-04T19:20:31.231908Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"  0%|          | 0/3944 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n100%|██████████| 3944/3944 [1:24:51<00:00,  1.29s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1 - Train loss: 0.9621\nEpoch 1/1 - Validation loss: 0.8109\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the entire model\ntorch.save(model, 'model_T5_flan_base_multi_1_epoch')\nprint(\"Model saved successfully.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:20:31.235999Z","iopub.execute_input":"2023-07-04T19:20:31.236548Z","iopub.status.idle":"2023-07-04T19:20:32.647482Z","shell.execute_reply.started":"2023-07-04T19:20:31.236521Z","shell.execute_reply":"2023-07-04T19:20:32.646425Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the model\n#model = torch.load('/kaggle/input/models/model_T5_flan_small_88')\n\n# Print a confirmation message\nprint(\"Model loaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T01:01:17.010079Z","iopub.execute_input":"2023-07-04T01:01:17.010566Z","iopub.status.idle":"2023-07-04T01:01:17.715981Z","shell.execute_reply.started":"2023-07-04T01:01:17.010526Z","shell.execute_reply":"2023-07-04T01:01:17.714887Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"Model loaded successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## are we accounting for the multiple texts targets in the bleu? it doesn't look like it","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:20:32.649013Z","iopub.execute_input":"2023-07-04T19:20:32.649387Z","iopub.status.idle":"2023-07-04T19:20:46.773040Z","shell.execute_reply.started":"2023-07-04T19:20:32.649352Z","shell.execute_reply":"2023-07-04T19:20:46.771768Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting sacrebleu\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.5.5)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.2)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.7.0 sacrebleu-2.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"batch_size=32","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:20:46.775667Z","iopub.execute_input":"2023-07-04T19:20:46.777730Z","iopub.status.idle":"2023-07-04T19:20:46.783645Z","shell.execute_reply.started":"2023-07-04T19:20:46.777660Z","shell.execute_reply":"2023-07-04T19:20:46.782216Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sacrebleu import corpus_bleu\nfrom random import sample\nfrom tqdm import tqdm\n\n\n# load the WebNLG validation dataset\nvalidation_dataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\nvalidation_dataset = [sample for sample in validation_dataset if sample['lex']['text']] # filter out samples with empty targets \n# Select a subset of the validation dataset\n#subset_size = 10  # Choose the desired subset size\n#validation_subset = sample(list(validation_dataset), subset_size)\nvalidation_data = WebNLGDataset(validation_dataset)\n\n# set up the validation data loader\nvalidation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n\n# switch model to evaluation mode\nmodel.eval()\n\n# generate predictions for the validation dataset\npredictions = []\nreferences = []\nwith torch.no_grad():\n    for inputs, targets in tqdm(validation_loader, desc='Validation Progress', leave=False):\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = model.module.generate(inputs, max_length=MAX_TARGET_LENGTH, num_beams=4)\n        # convert token IDs to strings\n        predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        target_texts = tokenizer.batch_decode(targets, skip_special_tokens=True)\n        # append predicted and target texts for BLEU evaluation\n        predictions.extend(predicted_texts)\n        references.extend(target_texts)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:20:46.785424Z","iopub.execute_input":"2023-07-04T19:20:46.785775Z","iopub.status.idle":"2023-07-04T19:30:21.987012Z","shell.execute_reply.started":"2023-07-04T19:20:46.785744Z","shell.execute_reply":"2023-07-04T19:30:21.986083Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"641d4102cc444c3c8b0c0875fc41f227"}},"metadata":{}},{"name":"stderr","text":"Validation Progress:   0%|          | 0/123 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\n                                                                      \r","output_type":"stream"}]},{"cell_type":"code","source":"# calculate BLEU scores\n#bleu = corpus_bleu(predictions, [references])\n\nmultiple_references = []\nfor i in range(len(validation_dataset)):\n    multiple_references.append(validation_dataset[i]['lex']['text'])\n    \nbleu = corpus_bleu(predictions, references)\nbleu_multiple = corpus_bleu(predictions, multiple_references)\n\nprint(f\"BLEU score: {bleu.score}\")\nprint(f\"BLEU score with multiple references: {bleu_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:32:43.279254Z","iopub.execute_input":"2023-07-04T19:32:43.280046Z","iopub.status.idle":"2023-07-04T19:32:44.829448Z","shell.execute_reply.started":"2023-07-04T19:32:43.280012Z","shell.execute_reply":"2023-07-04T19:32:44.826747Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"BLEU score: 0.19767614950377796\nBLEU score with multiple references: 94.80068745915912\n","output_type":"stream"}]},{"cell_type":"code","source":"# calculate BLEU scores\n#bleu = corpus_bleu(predictions, [references])\n\nmultiple_references = []\nfor i in range(len(validation_dataset)):\n    multiple_references.append(validation_dataset[i]['lex']['text'])\n    \n# First, determine the maximum length of sublists\nmax_len = max(len(refs) for refs in multiple_references)\n\n# Then pad all sublists to that length\npadded_references = [refs * (max_len // len(refs)) + refs[:max_len % len(refs)] for refs in multiple_references]\n    \nbleu = corpus_bleu(predictions, references)\nbleu_multiple = corpus_bleu(predictions, padded_references)\n\nprint(f\"BLEU score: {bleu.score}\")\nprint(f\"BLEU score with padded references: {bleu_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:32:46.990575Z","iopub.execute_input":"2023-07-04T19:32:46.990940Z","iopub.status.idle":"2023-07-04T19:32:50.932836Z","shell.execute_reply.started":"2023-07-04T19:32:46.990910Z","shell.execute_reply":"2023-07-04T19:32:50.931783Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"BLEU score: 0.19767614950377796\nBLEU score with padded references: 87.96615516145138\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\n\nmetric = load_metric('sacrebleu')\n\n# First, determine the maximum length of sublists\nmax_len = max(len(refs) for refs in multiple_references)\n\n# Then pad all sublists to that length\npadded_references = [refs * (max_len // len(refs)) + refs[:max_len % len(refs)] for refs in multiple_references]\n\n# Now 'padded_references' is a list of lists, where each sublist has the same length.\n# We can now compute the SacreBLEU score.\n\n# Note the change in the compute line\nscore = metric.compute(predictions=predictions, references = padded_references)\n\nprint(f\"SacreBLEU score: {score['score']}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:32:53.886472Z","iopub.execute_input":"2023-07-04T19:32:53.887008Z","iopub.status.idle":"2023-07-04T19:32:58.307674Z","shell.execute_reply.started":"2023-07-04T19:32:53.886969Z","shell.execute_reply":"2023-07-04T19:32:58.306563Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5982e232a9984102baa192a2119f439c"}},"metadata":{}},{"name":"stdout","text":"SacreBLEU score: 37.65135126120528\n","output_type":"stream"}]},{"cell_type":"code","source":"from sacrebleu import corpus_chrf\n# Calculate CHR F++ scores\nchrf = corpus_chrf(predictions, [references])\nchrf_multiple = corpus_chrf(predictions, multiple_references)\nprint(f\"CHR F++ score: {chrf.score}\")\nprint(f\"CHR F++ score with multiple references: {chrf_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:33:15.768177Z","iopub.execute_input":"2023-07-04T19:33:15.768566Z","iopub.status.idle":"2023-07-04T19:33:19.810046Z","shell.execute_reply.started":"2023-07-04T19:33:15.768534Z","shell.execute_reply":"2023-07-04T19:33:19.808975Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"CHR F++ score: 58.760976605324544\nCHR F++ score with multiple references: 75.17566273466703\n","output_type":"stream"}]},{"cell_type":"code","source":"from sacrebleu import corpus_chrf\n# Calculate CHR F++ scores\nchrf = corpus_chrf(predictions, [references])\nchrf_multiple = corpus_chrf(predictions, padded_references)\nprint(f\"CHR F++ score: {chrf.score}\")\nprint(f\"CHR F++ score with multiple references: {chrf_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:33:24.458509Z","iopub.execute_input":"2023-07-04T19:33:24.458955Z","iopub.status.idle":"2023-07-04T19:33:35.014609Z","shell.execute_reply.started":"2023-07-04T19:33:24.458908Z","shell.execute_reply":"2023-07-04T19:33:35.013584Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"CHR F++ score: 58.760976605324544\nCHR F++ score with multiple references: 74.93981222366703\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bert_score","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:33:35.016557Z","iopub.execute_input":"2023-07-04T19:33:35.017148Z","iopub.status.idle":"2023-07-04T19:33:46.810009Z","shell.execute_reply.started":"2023-07-04T19:33:35.017114Z","shell.execute_reply":"2023-07-04T19:33:46.808807Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.0.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.5.3)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.30.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.28.2)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.64.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.15.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.5.5)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.3.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.0.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.39.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2023.5.7)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\nimport numpy as np\n\n\nmetric = load_metric('bertscore')\n\nassert len(predictions) == len(references), \"The number of predictions and references should be the same.\"\n\n# Compute the score\nscore = metric.compute(predictions=predictions, references=references, lang='en')\n\nprint(f\"BERTScore: {np.mean(score['precision'])}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:34:03.480034Z","iopub.execute_input":"2023-07-04T19:34:03.480480Z","iopub.status.idle":"2023-07-04T19:35:03.750532Z","shell.execute_reply.started":"2023-07-04T19:34:03.480440Z","shell.execute_reply":"2023-07-04T19:35:03.749472Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6355e24ca6124934b5f60eb9bdebe3ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed9478b4c4649eaabe212d7bddd753c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbf0f24e97fd4456a2ba91fad1270c7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f872beb0746a4d0d8ccb4247b9fc0d03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ae7d8a6a3f49eebc8c536408186483"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"BERTScore: 0.9369189899071982\n","output_type":"stream"}]},{"cell_type":"code","source":"i=5\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:36:14.723906Z","iopub.execute_input":"2023-07-04T19:36:14.724265Z","iopub.status.idle":"2023-07-04T19:36:14.730529Z","shell.execute_reply.started":"2023-07-04T19:36:14.724235Z","shell.execute_reply":"2023-07-04T19:36:14.729597Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[['Bootleg_Series_Volume_1:_The_Quine_Tapes | previousWork | Squeeze_(The_Velvet_Underground_album)', 'Squeeze_(The_Velvet_Underground_album) | subsequentWork | 1969:_The_Velvet_Underground_Live']]\n----\nThe Bootleg Series Volume 1: The Quine Tapes was preceded by Squeeze (The Velvet Underground album) and followed by 1969: The Velvet Underground Live.\n----\n['The album 1969: The Velvet Underground Live is preceded by the Velvet Underground album Squeeze, which was followed by The Quine Tapes.', 'The Velvet Underground album Bootleg Series Volume 1: The Quine Tapes was preceded by the album Squeeze, which was followed by the live album 1969: The Velvet Underground Live.', 'The Bootleg Series Volume I: The Quine Tapes is preceded by the Velvet Underground album Squeeze which was itself followed by the album 1969: The Velvet Underground Live.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=10\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:36:22.390756Z","iopub.execute_input":"2023-07-04T19:36:22.391244Z","iopub.status.idle":"2023-07-04T19:36:22.397792Z","shell.execute_reply.started":"2023-07-04T19:36:22.391210Z","shell.execute_reply":"2023-07-04T19:36:22.396771Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[['Piotr_Hallmann | birthDate | 1987-01-01'], ['Piotr_Hallmann | birthDate | 1987-08-25']]\n----\nPiotr Hallmann was born on the 25th of August 1987.\n----\n['Piotr Hallmann was born on August 25, 1987.', \"Piotr Hallmann's birthday is August 25th 1987.\", 'Piotr Hallmann was born on the 25th of August 1987.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=50\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:36:25.014134Z","iopub.execute_input":"2023-07-04T19:36:25.014730Z","iopub.status.idle":"2023-07-04T19:36:25.020744Z","shell.execute_reply.started":"2023-07-04T19:36:25.014683Z","shell.execute_reply":"2023-07-04T19:36:25.019632Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[['Alan_Shepard | birthDate | \"1923-11-18\"^^xsd:date', 'Alan_Shepard | deathPlace | California', 'Alan_Shepard | birthPlace | New_Hampshire', 'Alan_Shepard | mission | Apollo_14']]\n----\nAlan Shepard was born in New Hampshire on November 18, 1923. He was a crew member of Apollo 14 and died in California.\n----\n['Alan Shepard was a crew member of Apollo 14 who was born November 18th, 1923, in New Hampshire and died in California.', 'Alan Shepard was born in New Hampshire on November 18, 1923. He was a crew member of Apollo 14, and died later on in California.', 'Alan Shepard was born on Nov 18, 1923 in New Hampshire, was a member of the Apollo 14 crew and died in California.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=0\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:36:30.016824Z","iopub.execute_input":"2023-07-04T19:36:30.017191Z","iopub.status.idle":"2023-07-04T19:36:30.023711Z","shell.execute_reply.started":"2023-07-04T19:36:30.017156Z","shell.execute_reply":"2023-07-04T19:36:30.022439Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[['Estádio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremiação_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_Série_C', 'Campeonato_Brasileiro_Série_C | country | Brazil', 'Agremiação_Sportiva_Arapiraquense | nickname | \"\\'\\'Alvinegro\"@en', 'Agremiação_Sportiva_Arapiraquense | ground | Estádio_Municipal_Coaracy_da_Mata_Fonseca']]\n----\nAgremiaço Sportiva Arapiraquense play in the Campeonato Brasileiro Série C league in Brazil. The Estádio Municipal Coaracy da Mata Fonseca is located in Arapiraca.\n----\n['Estádio Municipal Coaracy da Mata Fonseca is the name of the ground of Agremiação Sportiva Arapiraquense in Arapiraca. Agremiação Sportiva Arapiraquense, nicknamed \"Alvinegro\", lay in the Campeonato Brasileiro Série C league from Brazil.', 'Estádio Municipal Coaracy da Mata Fonseca is the name of the ground of Agremiação Sportiva Arapiraquense in Arapiraca. Alvinegro, the nickname of Agremiação Sportiva Arapiraquense, play in the Campeonato Brasileiro Série C league from Brazil.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=70\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:36:36.476277Z","iopub.execute_input":"2023-07-04T19:36:36.476730Z","iopub.status.idle":"2023-07-04T19:36:36.483618Z","shell.execute_reply.started":"2023-07-04T19:36:36.476695Z","shell.execute_reply":"2023-07-04T19:36:36.482710Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[['Pontiac_Rageous | assembly | Michigan', 'Pontiac_Rageous | assembly | Detroit', 'Pontiac_Rageous | productionEndYear | 1997-01-01']]\n----\nThe Pontiac Rangeous was assembled in Detroit, Michigan and ended its production on January 1, 1997.\n----\n['The Pontiac Rageous assembled in Michigan with assembly line in Detroit was last produced in 1997.', 'Ending its production in 1997, the Pontiac Rageous was assembled in Detroit, Michigan.', 'Ending in 1997, the Pontiac Rageous was assembled in Detroit, Michigan.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=130\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:36:39.850178Z","iopub.execute_input":"2023-07-04T19:36:39.850563Z","iopub.status.idle":"2023-07-04T19:36:39.857188Z","shell.execute_reply.started":"2023-07-04T19:36:39.850531Z","shell.execute_reply":"2023-07-04T19:36:39.856093Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"[['McVeagh_of_the_South_Seas | director | Harry_Carey_(actor_born_1878)', 'McVeagh_of_the_South_Seas | writer | Harry_Carey_(actor_born_1878)', 'McVeagh_of_the_South_Seas | producer | The_Progressive_Motion_Picture_Company']]\n----\nThe director of McVeagh of the South Seas was Harry Carey, who was born in 1888. The film was produced by The Progressive Motion Picture Company and was directed by Harry Carey.\n----\n['\"McVeagh of the South Seas\" was written and directed by Harry Carey born in 1878 and produced by Progressive Motion Picture Company.', 'Harry Carey, an actor born in 1878, was the director and writer for the film McVeagh of the South Seas which was produced by the Progressive Motion Picture Company.', 'Harry Carey, an actor and director, was born in 1878. He was the writer of the movie McVeagh of the South Seas, which was produced by the Progressive Motion Picture Company. As well as writing the film script, Carey also acted a role in the movie, and directed it.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=1861\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:40:02.311514Z","iopub.execute_input":"2023-07-04T19:40:02.311873Z","iopub.status.idle":"2023-07-04T19:40:02.319482Z","shell.execute_reply.started":"2023-07-04T19:40:02.311844Z","shell.execute_reply":"2023-07-04T19:40:02.317366Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[['Akeem_Ayers | currentteam | \"Los Angeles Rams\"@en', 'Akeem_Ayers | debutTeam | Tennessee_Titans']]\n----\nAkeem Ayers made his debut for the Tennessee Titans and now plays for the Los Angeles Rams.\n----\n['Akeem Ayers made his debut for the Tennessee Titans and currently plays for the Los Angeles Rams.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=1860\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:40:25.158298Z","iopub.execute_input":"2023-07-04T19:40:25.158675Z","iopub.status.idle":"2023-07-04T19:40:25.165225Z","shell.execute_reply.started":"2023-07-04T19:40:25.158647Z","shell.execute_reply":"2023-07-04T19:40:25.163887Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"[['Turn_Me_On_(album) | runtime | 2106.0', 'Turn_Me_On_(album) | artist | The_Honeymoon_Killers_(American_band)', 'Turn_Me_On_(album) | genre | Punk_blues', 'Turn_Me_On_(album) | producer | The_Honeymoon_Killers_(American_band)', 'Turn_Me_On_(album) | genre | Noise_rock', 'Turn_Me_On_(album) | previousWork | Let_It_Breed']]\n----\nThe album Turn Me On, produced by The Honeymoon Killers, is a punk blues album with a run time of 2106.0.\n----\n['Turn Me On by the Honeymoon Killers is a punk blues album in the noise rock genre. The run time is 35.1 minutes and was preceded by the Let it Breed album.']\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\ncount_empty_text = 0\nfor sample in dataset:\n    if not sample['lex']['text']:\n        count_empty_text += 1\n\nprint(f\"Number of samples with empty 'lex' 'text' field: {count_empty_text}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:43:27.934730Z","iopub.execute_input":"2023-07-04T19:43:27.935355Z","iopub.status.idle":"2023-07-04T19:43:29.638078Z","shell.execute_reply.started":"2023-07-04T19:43:27.935316Z","shell.execute_reply":"2023-07-04T19:43:29.636954Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dfe400a19494e078d81cffd78613762"}},"metadata":{}},{"name":"stdout","text":"Number of samples with empty 'lex' 'text' field: 1779\n","output_type":"stream"}]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"### testing the other datasets","metadata":{}},{"cell_type":"code","source":"from sacrebleu import corpus_bleu\nfrom random import sample\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nbatch_size = 32\nMAX_INPUT_LENGTH = 128\nMAX_TARGET_LENGTH = 128\ntokenizer.model_max_length = MAX_INPUT_LENGTH\nmodel.module.config.max_length = MAX_TARGET_LENGTH\n\n\n# load the WebNLG validation dataset\nvalidation_dataset = load_dataset('dart', split='test', ignore_verifications=True)\n# Select a subset of the validation dataset\nvalidation_data = DartDataset(validation_dataset)\n\n# set up the validation data loader\nvalidation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n\n# switch model to evaluation mode\nmodel.eval()\n\n# generate predictions for the validation dataset\npredictions = []\nreferences = []\nwith torch.no_grad():\n    for inputs, targets in tqdm(validation_loader, desc='Validation Progress', leave=False):\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = model.module.generate(inputs, max_length=MAX_TARGET_LENGTH, num_beams=4)\n        # convert token IDs to strings\n        predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        target_texts = tokenizer.batch_decode(targets, skip_special_tokens=True)\n        # append predicted and target texts for BLEU evaluation\n        predictions.extend(predicted_texts)\n        references.extend(target_texts)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:43:55.386382Z","iopub.execute_input":"2023-07-04T19:43:55.387277Z","iopub.status.idle":"2023-07-04T19:52:00.759157Z","shell.execute_reply.started":"2023-07-04T19:43:55.387232Z","shell.execute_reply":"2023-07-04T19:52:00.757866Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"                                                                      \r","output_type":"stream"}]},{"cell_type":"code","source":"# calculate BLEU scores\n#bleu = corpus_bleu(predictions, [references])\n \nbleu = corpus_bleu(predictions, references)\n\nprint(f\"BLEU score: {bleu.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:53:26.612106Z","iopub.execute_input":"2023-07-04T19:53:26.612528Z","iopub.status.idle":"2023-07-04T19:53:27.267340Z","shell.execute_reply.started":"2023-07-04T19:53:26.612496Z","shell.execute_reply":"2023-07-04T19:53:27.266353Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"BLEU score: 0.3704640151983367\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\n\nmetric = load_metric('sacrebleu')\n\nassert len(predictions) == len(references), \"The number of predictions and references should be the same.\"\n\nlist_references = []\nfor reference in references:\n    list_references.append([reference])\n\n# Note the change in the compute line\nscore = metric.compute(predictions=predictions, references=list_references)\n\nprint(f\"SacreBLEU score: {score['score']}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:53:30.586289Z","iopub.execute_input":"2023-07-04T19:53:30.587022Z","iopub.status.idle":"2023-07-04T19:53:32.702784Z","shell.execute_reply.started":"2023-07-04T19:53:30.586983Z","shell.execute_reply":"2023-07-04T19:53:32.701574Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"SacreBLEU score: 36.49161740730786\n","output_type":"stream"}]},{"cell_type":"code","source":"from sacrebleu import corpus_chrf\n# Calculate CHR F++ scores\nchrf = corpus_chrf(predictions, [references])\nprint(f\"CHR F++ score: {chrf.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:53:41.211468Z","iopub.execute_input":"2023-07-04T19:53:41.211822Z","iopub.status.idle":"2023-07-04T19:53:44.192132Z","shell.execute_reply.started":"2023-07-04T19:53:41.211795Z","shell.execute_reply":"2023-07-04T19:53:44.191164Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"CHR F++ score: 63.939258929880026\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U nltk","metadata":{"execution":{"iopub.status.busy":"2023-07-03T23:46:51.162787Z","iopub.execute_input":"2023-07-03T23:46:51.163208Z","iopub.status.idle":"2023-07-03T23:47:04.143357Z","shell.execute_reply.started":"2023-07-03T23:46:51.163177Z","shell.execute_reply":"2023-07-03T23:47:04.142161Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.5.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\nInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\nnltk.download('wordnet')\n\n# Assumptions:\n# predictions is a list of predicted sentences\n# references is a list of reference sentences\n\nassert len(predictions) == len(references), \"The number of predictions and references should be the same.\"\n\nscores = []\n\nfor prediction, reference in zip(predictions, references):\n    tokenized_prediction = word_tokenize(prediction)\n    tokenized_reference = word_tokenize(reference)\n    score = nltk.translate.meteor_score.meteor_score([tokenized_reference], tokenized_prediction)\n    scores.append(score)\n\naverage_meteor_score = sum(scores) / len(scores)\nprint(f\"Average METEOR score: {average_meteor_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:03:53.837602Z","iopub.execute_input":"2023-07-04T00:03:53.838495Z","iopub.status.idle":"2023-07-04T00:03:53.896965Z","shell.execute_reply.started":"2023-07-04T00:03:53.838423Z","shell.execute_reply":"2023-07-04T00:03:53.895512Z"},"trusted":true},"execution_count":49,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction, reference \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, references):\n\u001b[0;32m---> 12\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43msacrebleu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_meteor\u001b[49m(prediction, [reference])\n\u001b[1;32m     13\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(score\u001b[38;5;241m.\u001b[39mscore)\n\u001b[1;32m     15\u001b[0m average_meteor_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores)\n","\u001b[0;31mAttributeError\u001b[0m: module 'sacrebleu' has no attribute 'sentence_meteor'"],"ename":"AttributeError","evalue":"module 'sacrebleu' has no attribute 'sentence_meteor'","output_type":"error"}]},{"cell_type":"code","source":"!pip install bert_score","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:14:58.804341Z","iopub.execute_input":"2023-07-04T00:14:58.804716Z","iopub.status.idle":"2023-07-04T00:15:10.712348Z","shell.execute_reply.started":"2023-07-04T00:14:58.804688Z","shell.execute_reply":"2023-07-04T00:15:10.711091Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.0.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.5.3)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.30.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.28.2)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.64.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.15.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.5.5)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.3.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.0.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.39.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2023.5.7)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\nimport numpy as np\n\n\nmetric = load_metric('bertscore')\n\nassert len(predictions) == len(references), \"The number of predictions and references should be the same.\"\n\n# Compute the score\nscore = metric.compute(predictions=predictions, references=references, lang='en')\n\nprint(f\"BERTScore: {np.mean(score['precision'])}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:53:49.954887Z","iopub.execute_input":"2023-07-04T19:53:49.955253Z","iopub.status.idle":"2023-07-04T19:54:44.068316Z","shell.execute_reply.started":"2023-07-04T19:53:49.955221Z","shell.execute_reply":"2023-07-04T19:54:44.066445Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"BERTScore: 0.9464624207909023\n","output_type":"stream"}]},{"cell_type":"code","source":"import datasets\ndatasets.list_metrics()","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:14:02.165684Z","iopub.execute_input":"2023-07-04T00:14:02.166069Z","iopub.status.idle":"2023-07-04T00:14:02.314862Z","shell.execute_reply.started":"2023-07-04T00:14:02.166038Z","shell.execute_reply":"2023-07-04T00:14:02.313897Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"['accuracy',\n 'bertscore',\n 'bleu',\n 'bleurt',\n 'brier_score',\n 'cer',\n 'character',\n 'charcut_mt',\n 'chrf',\n 'code_eval',\n 'comet',\n 'competition_math',\n 'coval',\n 'cuad',\n 'exact_match',\n 'f1',\n 'frugalscore',\n 'glue',\n 'google_bleu',\n 'indic_glue',\n 'mae',\n 'mahalanobis',\n 'mape',\n 'mase',\n 'matthews_correlation',\n 'mauve',\n 'mean_iou',\n 'meteor',\n 'mse',\n 'nist_mt',\n 'pearsonr',\n 'perplexity',\n 'poseval',\n 'precision',\n 'r_squared',\n 'recall',\n 'rl_reliability',\n 'roc_auc',\n 'rouge',\n 'sacrebleu',\n 'sari',\n 'seqeval',\n 'smape',\n 'spearmanr',\n 'squad',\n 'squad_v2',\n 'super_glue',\n 'ter',\n 'trec_eval',\n 'wer',\n 'wiki_split',\n 'xnli',\n 'xtreme_s',\n 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n 'BucketHeadP65/confusion_matrix',\n 'BucketHeadP65/roc_curve',\n 'Drunper/metrica_tesi',\n 'Felipehonorato/eer',\n 'He-Xingwei/sari_metric',\n 'JP-SystemsX/nDCG',\n 'Josh98/nl2bash_m',\n 'Kyle1668/squad',\n 'Muennighoff/code_eval',\n 'NCSOFT/harim_plus',\n 'Natooz/ece',\n 'NikitaMartynov/spell-check-metric',\n 'Pipatpong/perplexity',\n 'Splend1dchan/cosine_similarity',\n 'Viona/fuzzy_reordering',\n 'Viona/kendall_tau',\n 'Yeshwant123/mcc',\n 'abdusah/aradiawer',\n 'abidlabs/mean_iou',\n 'abidlabs/mean_iou2',\n 'andstor/code_perplexity',\n 'angelina-wang/directional_bias_amplification',\n 'aryopg/roc_auc_skip_uniform_labels',\n 'brian920128/doc_retrieve_metrics',\n 'bstrai/classification_report',\n 'chanelcolgate/average_precision',\n 'ckb/unigram',\n 'codeparrot/apps_metric',\n 'cpllab/syntaxgym',\n 'dvitel/codebleu',\n 'ecody726/bertscore',\n 'fschlatt/ner_eval',\n 'giulio98/codebleu',\n 'guydav/restrictedpython_code_eval',\n 'harshhpareek/bertscore',\n 'hpi-dhc/FairEval',\n 'hynky/sklearn_proxy',\n 'hyperml/balanced_accuracy',\n 'jpxkqx/peak_signal_to_noise_ratio',\n 'jpxkqx/signal_to_reconstruction_error',\n 'k4black/codebleu',\n 'kaggle/ai4code',\n 'langdonholmes/cohen_weighted_kappa',\n 'lhy/hamming_loss',\n 'lhy/ranking_loss',\n 'lvwerra/accuracy_score',\n 'manueldeprada/beer',\n 'mfumanelli/geometric_mean',\n 'omidf/squad_precision_recall',\n 'posicube/mean_reciprocal_rank',\n 'sakusakumura/bertscore',\n 'sma2023/wil',\n 'spidyidc/bertscore',\n 'tialaeMceryu/unigram',\n 'transZ/sbert_cosine',\n 'transZ/test_parascore',\n 'transformersegmentation/segmentation_scores',\n 'unnati/kendall_tau_distance',\n 'weiqis/pajm',\n 'ybelkada/cocoevaluate',\n 'yonting/average_precision_score',\n 'yuyijiong/quad_match_score']"},"metadata":{}}]},{"cell_type":"code","source":"pip install pyter3\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:00:20.183180Z","iopub.execute_input":"2023-07-04T00:00:20.183614Z","iopub.status.idle":"2023-07-04T00:00:31.552899Z","shell.execute_reply.started":"2023-07-04T00:00:20.183579Z","shell.execute_reply":"2023-07-04T00:00:31.550957Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting pyter3\n  Downloading pyter3-0.3-py3-none-any.whl (4.1 kB)\nInstalling collected packages: pyter3\nSuccessfully installed pyter3-0.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pyter\n\n# Assumptions:\n# predictions is a list of predicted sentences\n# references is a list of reference sentences\n\nassert len(predictions) == len(references), \"The number of predictions and references should be the same.\"\n\nscores = []\n\nfor prediction, reference in zip(predictions, references):\n    score = pyter.ter(reference.split(), prediction.split())\n    scores.append(score)\n\naverage_ter_score = sum(scores) / len(scores)\nprint(f\"Average TER score: {average_ter_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T00:00:31.555503Z","iopub.execute_input":"2023-07-04T00:00:31.556041Z","iopub.status.idle":"2023-07-04T00:01:40.429705Z","shell.execute_reply.started":"2023-07-04T00:00:31.556001Z","shell.execute_reply":"2023-07-04T00:01:40.428665Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Average TER score: 1.0390399552765\n","output_type":"stream"}]},{"cell_type":"code","source":"i=0\nprint(validation_dataset[i])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:56:32.690096Z","iopub.execute_input":"2023-07-04T19:56:32.690488Z","iopub.status.idle":"2023-07-04T19:56:32.697051Z","shell.execute_reply.started":"2023-07-04T19:56:32.690456Z","shell.execute_reply":"2023-07-04T19:56:32.695831Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"{'tripleset': [['Hawaii Five-O', 'NOTES', 'Episode: The Flight of the Jewels'], ['[TABLECONTEXT]', '[TITLE]', 'Jeff Daniels'], ['[TABLECONTEXT]', 'TITLE', 'Hawaii Five-O']], 'subtree_was_extended': True, 'annotations': {'source': ['WikiTableQuestions_lily'], 'text': ['Jeff Daniels played in the Hawaii Five-O episode The Flight of the Jewels']}}\n----\nJeff Daniels aired the episode The Flight of the Jewels in Hawaii Five-O.\n----\nJeff Daniels played in the Hawaii Five-O episode The Flight of the Jewels\n","output_type":"stream"}]},{"cell_type":"code","source":"i=10\nprint(validation_dataset[i])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:56:52.455237Z","iopub.execute_input":"2023-07-04T19:56:52.455755Z","iopub.status.idle":"2023-07-04T19:56:52.462901Z","shell.execute_reply.started":"2023-07-04T19:56:52.455715Z","shell.execute_reply":"2023-07-04T19:56:52.461805Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"{'tripleset': [[\"ricoh women's british open 1\", 'WINNING_SCORE', '74-67-71-73=285'], ['2 aug 2009', 'TOURNAMENT', \"ricoh women's british open 1\"]], 'subtree_was_extended': True, 'annotations': {'source': ['WikiSQL_decl_sents'], 'text': ['The date of the winning score 74-67-71-73=285 is 2 aug 2009.']}}\n----\nThe winning score of the tournament on 2 Aug 2009 was 74-67-71-73=285\n----\nThe date of the winning score 74-67-71-73=285 is 2 aug 2009.\n","output_type":"stream"}]},{"cell_type":"code","source":"i=30\nprint(validation_dataset[i])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:57:13.196053Z","iopub.execute_input":"2023-07-04T19:57:13.196630Z","iopub.status.idle":"2023-07-04T19:57:13.208873Z","shell.execute_reply.started":"2023-07-04T19:57:13.196589Z","shell.execute_reply":"2023-07-04T19:57:13.207910Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"{'tripleset': [['the kissing game', 'DIRECTED_BY', 'patrick duffy']], 'subtree_was_extended': False, 'annotations': {'source': ['WikiSQL_decl_sents'], 'text': ['Patrick Duffy directed \"The Kissing Game\".']}}\n----\nPatrick duffy directed the episode titled \"the kissing game\".\n----\nPatrick Duffy directed \"The Kissing Game\".\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}