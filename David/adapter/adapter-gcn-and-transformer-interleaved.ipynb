{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"execution":{"iopub.status.busy":"2023-07-01T13:58:45.643852Z","iopub.execute_input":"2023-07-01T13:58:45.644102Z","iopub.status.idle":"2023-07-01T13:59:19.347884Z","shell.execute_reply.started":"2023-07-01T13:58:45.644078Z","shell.execute_reply":"2023-07-01T13:59:19.346671Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.10.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.28.2)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.1.0)\nBuilding wheels for collected packages: torch_geometric\n  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=b140590f97b5a32637bb66b88a37f33d92f778069a92823c498c89de44e27ce6\n  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\nSuccessfully built torch_geometric\nInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\nmodel = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\ninput_ids = tokenizer.encode(\"answer: hello, how are you\", return_tensors=\"pt\")  # Batch size 1\noutputs = model.generate(input_ids)\nprint([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs])","metadata":{"execution":{"iopub.status.busy":"2023-07-01T13:58:02.444321Z","iopub.execute_input":"2023-07-01T13:58:02.445022Z","iopub.status.idle":"2023-07-01T13:58:05.953758Z","shell.execute_reply.started":"2023-07-01T13:58:02.444989Z","shell.execute_reply":"2023-07-01T13:58:05.952741Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"['good']\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch_geometric.nn import GCNConv\nfrom transformers import T5Model, T5TokenizerFast\nimport networkx as nx\nfrom torch_geometric.data import Data\nimport xml.etree.ElementTree as ET\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom torch_geometric.data import Batch\nfrom transformers import T5Model\nfrom torch_geometric.nn import GCNConv\nfrom datasets import load_dataset\nfrom torch_geometric.data import Data\nfrom transformers import T5TokenizerFast\nimport networkx as nx\nimport torch\nimport re\nimport matplotlib.pyplot as plt\n\n#model_name = \"t5-small\"\nmodel_name = \"google/flan-t5-small\"\n\n\nclass WebNLGDataset(Dataset):\n    def __init__(self, dataset, max_edges=512):\n        self.dataset = dataset\n        self.tokenizer = T5TokenizerFast.from_pretrained(model_name)\n        self.node_to_idx = {}  # Node to index mapping\n        self.max_edges = max_edges\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        data_dict = self.dataset[idx]\n        text = data_dict['lex']['text'][0]\n        triples = data_dict['original_triple_sets']['otriple_set'][0]\n\n        graph_nx = self.triples_to_graph(triples)\n        edge_index = self.get_edge_index(graph_nx)\n\n        encoding = self.tokenizer.encode_plus(text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n\n        graph_data = Data(x=encoding['input_ids'].squeeze(dim=0), edge_index=edge_index)\n        graph_data.attention_mask = encoding['attention_mask'].squeeze(dim=0)\n        graph_data.y = encoding['input_ids'].squeeze(dim=0)\n        \n        #print(\"Original Sample: \\n\", text, \"\\n\", triples)  # Print the original sample      \n        #print(\"NetworkX Graph: \\n\", graph_nx.edges) # Print the NetworkX graph\n        #self.visualize_graph(graph_nx)\n\n        return graph_data\n\n    def triples_to_graph(self, triples):\n        self.node_to_idx = {}  # reset for each new graph\n        graph_nx = nx.MultiDiGraph()\n        for triple in triples:\n            triple = re.sub(r'\\([^)]*\\)', '', triple).split('|')  # remove brackets and split by '|'\n            subject, relation, obj = map(str.strip, triple)\n\n            # Add string node names to the graph\n            if subject not in self.node_to_idx:\n                self.node_to_idx[subject] = len(self.node_to_idx)\n            if obj not in self.node_to_idx:\n                self.node_to_idx[obj] = len(self.node_to_idx)\n\n            graph_nx.add_edge(subject, obj, key=relation)\n        return graph_nx\n\n    def get_edge_index(self, graph_nx):\n        edge_index = torch.tensor([[self.node_to_idx[n] for n in edge[:2]] for edge in graph_nx.edges]).t().contiguous()\n        return edge_index\n    \n    def visualize_graph(self, graph_nx):\n        plt.figure(figsize=(8, 6))\n        pos = nx.spring_layout(graph_nx)  # positions for all nodes\n        nx.draw(graph_nx, pos, with_labels=True)\n        labels = nx.get_edge_attributes(graph_nx, 'key')\n        nx.draw_networkx_edge_labels(graph_nx, pos, edge_labels=labels)\n        plt.show()\n\n    \nclass AdapterBlock(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(AdapterBlock, self).__init__()\n        self.layer_norm = nn.LayerNorm(input_dim, eps=1e-6)\n        self.gcn = GCNConv(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc = nn.Linear(hidden_dim, input_dim)  # Restore dimension to input_dim\n        self.res_fc = nn.Linear(input_dim, input_dim)  # Transform the residual\n\n    def forward(self, x, edge_index):\n        x_res = self.res_fc(x)  # Transform the residual tensor\n        x = self.layer_norm(x)\n        x = self.gcn(x, edge_index)\n        x = self.relu(x)\n        x = self.fc(x)\n        return x + x_res  # Now you can add the tensors\n\n\nfrom transformers import T5ForConditionalGeneration\n\nclass TransformerGCN(nn.Module):\n    def __init__(self, vocab_size, adapter_dim):\n        super(TransformerGCN, self).__init__()\n        self.transformer = T5ForConditionalGeneration.from_pretrained(model_name)\n        self.hidden_size = self.transformer.config.hidden_size  # Get the hidden size from the config\n        self.reduce_dim = nn.Linear(32128, adapter_dim)  # Use the hidden size instead of 768\n\n        # Freeze the parameters of the T5 model\n        for param in self.transformer.parameters():\n            param.requires_grad = False\n\n        self.adapter_blocks = nn.ModuleList([\n        AdapterBlock(block.layer[1].DenseReluDense.wo.weight.size(0), adapter_dim) for block in self.transformer.encoder.block\n        ])\n\n        self.output_head = nn.Linear(adapter_dim, vocab_size)\n\n    def forward(self, input_ids, attention_mask, edge_index):\n        if input_ids.dim() == 1:  # If the input is 1D (batch size 1)\n            input_ids = input_ids.unsqueeze(0)  # Add a batch dimension\n        if attention_mask.dim() == 1:  # Same for the attention_mask\n            attention_mask = attention_mask.unsqueeze(0)\n\n        shifted_input_ids = torch.cat([torch.zeros((input_ids.size(0), 1), dtype=torch.long, device=input_ids.device), input_ids[:, :-1]], dim=-1)\n\n        input_embeds = self.transformer.get_input_embeddings()(input_ids)\n        hidden_states = input_embeds\n        for block, adapter_block in zip(self.transformer.encoder.block, self.adapter_blocks):\n            hidden_states, _ = block(hidden_states, attention_mask=attention_mask, encoder_hidden_states=None, encoder_attention_mask=None)\n            hidden_states = adapter_block(hidden_states, edge_index)\n\n        transformer_outputs = self.transformer(inputs_embeds=hidden_states, attention_mask=attention_mask, decoder_input_ids=shifted_input_ids)\n        transformer_outputs = self.reduce_dim(transformer_outputs[0])\n        return self.output_head(transformer_outputs)\n\n\nclass ModifiedT5Block(nn.Module):\n    def __init__(self, original_block, adapter_dim):\n        super(ModifiedT5Block, self).__init__()\n        self.original_block = original_block\n        self.adapter = AdapterBlock(original_block.layer[1].DenseReluDense.wi.weight.size(-1), adapter_dim)\n\n    def forward(self, x, edge_index, **kwargs):\n        x, _ = self.original_block(x, **kwargs)\n        return self.adapter(x, edge_index)\n\n\n\nfrom torch_geometric.data import DataLoader as GeometricDataLoader\n\ndef train(model, dataloader, epochs, device):\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # Lower learning rate\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') # Add learning rate scheduler\n    tokenizer = T5TokenizerFast.from_pretrained(model_name)\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n    for epoch in range(epochs):\n        model.train()\n        i = 0\n        for data in tqdm(dataloader):\n            data = data.to(device) # Moving batch to device\n            optimizer.zero_grad()\n\n            outputs = model(input_ids=data.x, attention_mask=data.attention_mask, edge_index=data.edge_index)\n            loss = criterion(outputs.view(-1, outputs.size(-1)), data.y.view(-1))\n            loss.backward()\n            optimizer.step()\n            \n            if i % 100 == 0:\n                print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}\")\n            \n            i += 1 \n\n# Usage\ndataset_dict = load_dataset('web_nlg', 'webnlg_challenge_2017')['train']\ndataset = WebNLGDataset(dataset_dict)\nvocab_size = len(dataset.tokenizer)\nmodel = TransformerGCN(vocab_size=vocab_size, adapter_dim=512)\ndataloader = GeometricDataLoader(dataset, batch_size=2)\ntrain(model, dataloader, epochs=2, device=torch.device('cuda'))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-01T13:59:27.857766Z","iopub.execute_input":"2023-07-01T13:59:27.858212Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbc3317a358a46e48c9a525f59f4cae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/2.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2f47407c5a846698484fe4e98c1e061"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset web_nlg/webnlg_challenge_2017 (download: 24.32 MiB, generated: 8.99 MiB, post-processed: Unknown size, total: 33.31 MiB) to /root/.cache/huggingface/datasets/web_nlg/webnlg_challenge_2017/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a42c43caff3547789cedfe377388ba1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/6940 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4615 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset web_nlg downloaded and prepared to /root/.cache/huggingface/datasets/web_nlg/webnlg_challenge_2017/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77da8d6592664185ae746c9536e4d66e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2ed4a53b87d4a4bbf7308a51c3ab1cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0c799cb80d84736908987bad7a33480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18fcb65dd9494a3ea090e42dc898a942"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d2ca87e30444425926002ed03f5bf20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fbe8d107bcf43f0a40bfbaa70b35b57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"519f1f245a484fceb178f3cc33dae95f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ab704724aa04fe69fcd4367654be632"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\n  0%|          | 2/3470 [00:01<48:09,  1.20it/s]  ","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 0, Loss: 26.284656524658203\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 102/3470 [00:21<10:46,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 100, Loss: 8.19456672668457\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 202/3470 [00:40<10:26,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 200, Loss: 8.687564849853516\n","output_type":"stream"},{"name":"stderr","text":"  9%|▊         | 302/3470 [01:00<10:06,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 300, Loss: 6.294469833374023\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 402/3470 [01:19<09:54,  5.16it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 400, Loss: 4.097095489501953\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 502/3470 [01:39<09:43,  5.09it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 500, Loss: 5.831324577331543\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 602/3470 [01:58<09:09,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 600, Loss: 6.2011566162109375\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 702/3470 [02:17<08:50,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 700, Loss: 7.284809112548828\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 802/3470 [02:37<08:30,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 800, Loss: 8.466331481933594\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 902/3470 [02:56<08:12,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 900, Loss: 6.41180419921875\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 1002/3470 [03:16<07:51,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1000, Loss: 6.61683464050293\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 1102/3470 [03:35<07:33,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1100, Loss: 6.747409820556641\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▍      | 1202/3470 [03:55<07:14,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1200, Loss: 4.775228023529053\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 1302/3470 [04:14<06:55,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1300, Loss: 5.036314964294434\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 1402/3470 [04:34<06:38,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1400, Loss: 5.78169059753418\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 1502/3470 [04:53<06:17,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1500, Loss: 5.161454677581787\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 1602/3470 [05:12<05:56,  5.24it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1600, Loss: 4.452596664428711\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 1702/3470 [05:32<05:39,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1700, Loss: 5.453591823577881\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 1802/3470 [05:51<05:19,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1800, Loss: 3.9197356700897217\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▍    | 1902/3470 [06:11<05:00,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 1900, Loss: 4.28248405456543\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 2002/3470 [06:30<04:40,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2000, Loss: 5.168229103088379\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 2102/3470 [06:50<04:22,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2100, Loss: 3.1320719718933105\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 2202/3470 [07:09<04:04,  5.18it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2200, Loss: 4.894715785980225\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▋   | 2302/3470 [07:28<03:44,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2300, Loss: 4.2428507804870605\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 2402/3470 [07:48<03:25,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2400, Loss: 4.1129631996154785\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2502/3470 [08:07<03:04,  5.24it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2500, Loss: 4.916940689086914\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▍  | 2602/3470 [08:27<02:46,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2600, Loss: 5.902958393096924\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 2702/3470 [08:46<02:26,  5.24it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2700, Loss: 5.421992778778076\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 2802/3470 [09:06<02:08,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2800, Loss: 3.4193310737609863\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▎ | 2902/3470 [09:25<01:49,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 2900, Loss: 4.30832052230835\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 3002/3470 [09:45<01:29,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 3000, Loss: 4.683661937713623\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 3102/3470 [10:04<01:10,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 3100, Loss: 5.6105122566223145\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 3202/3470 [10:24<00:51,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 3200, Loss: 5.874905109405518\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 3302/3470 [10:43<00:32,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 3300, Loss: 2.9546291828155518\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 3402/3470 [11:02<00:13,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Iteration: 3400, Loss: 2.3941736221313477\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3470/3470 [11:16<00:00,  5.13it/s]\n  0%|          | 2/3470 [00:00<10:52,  5.31it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 0, Loss: 2.8929851055145264\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 102/3470 [00:19<10:44,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 100, Loss: 5.401103496551514\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 202/3470 [00:39<10:26,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 200, Loss: 7.0125412940979\n","output_type":"stream"},{"name":"stderr","text":"  9%|▊         | 302/3470 [00:58<10:10,  5.19it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 300, Loss: 3.633180856704712\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 402/3470 [01:18<09:44,  5.25it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 400, Loss: 3.374939203262329\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 502/3470 [01:37<09:26,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 500, Loss: 6.556642055511475\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 602/3470 [01:57<09:11,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 600, Loss: 6.159722805023193\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 702/3470 [02:16<08:58,  5.14it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 700, Loss: 5.892805576324463\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 802/3470 [02:36<08:31,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 800, Loss: 6.814853191375732\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 902/3470 [02:55<08:13,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 900, Loss: 6.195101261138916\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 1002/3470 [03:15<08:04,  5.10it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1000, Loss: 5.049078941345215\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 1102/3470 [03:34<07:35,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1100, Loss: 7.34785795211792\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▍      | 1202/3470 [03:54<07:15,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1200, Loss: 4.701527118682861\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 1302/3470 [04:13<06:57,  5.19it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1300, Loss: 4.832247734069824\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 1402/3470 [04:33<06:41,  5.15it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1400, Loss: 3.7869534492492676\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 1502/3470 [04:52<06:22,  5.14it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1500, Loss: 4.177012920379639\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 1602/3470 [05:12<05:59,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1600, Loss: 3.381134510040283\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 1702/3470 [05:31<05:39,  5.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1700, Loss: 4.6269378662109375\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 1802/3470 [05:51<05:22,  5.18it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1800, Loss: 3.7507476806640625\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▍    | 1902/3470 [06:10<05:00,  5.22it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 1900, Loss: 3.545541524887085\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 2002/3470 [06:30<04:40,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 2000, Loss: 4.112112522125244\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 2102/3470 [06:49<04:23,  5.20it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 2100, Loss: 2.4518654346466064\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 2202/3470 [07:09<04:02,  5.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 2200, Loss: 3.874729633331299\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▋   | 2302/3470 [07:28<03:49,  5.09it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Iteration: 2300, Loss: 3.923366069793701\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 2356/3470 [07:39<03:36,  5.15it/s]","output_type":"stream"}]},{"cell_type":"code","source":"def test(model, dataloader, device):\n    model = model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        for data in dataloader:\n            data = data.to(device) \n\n            data.x = data.x.unsqueeze(0)  # Add batch dimension\n            data.attention_mask = data.attention_mask.unsqueeze(0) \n\n            outputs = model.transformer.generate(input_ids=data.x, attention_mask=data.attention_mask, decoder_start_token_id=model.transformer.config.pad_token_id)\n\n            # Convert the tensor outputs to text using the tokenizer\n            output_text = [dataset.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in outputs]\n\n            # print input and output\n            print(f\"Input: {dataset.tokenizer.decode(data.x[0].tolist(), skip_special_tokens=True)}\")  # Convert tensor to list\n            print(f\"Output: {output_text}\")\n\n# Usage\ndataset_dict = load_dataset('web_nlg', 'webnlg_challenge_2017')['test']\ndataset_dict = [sample for sample in dataset_dict if sample['lex']['text']] # filter out samples with empty targets \ndataset = WebNLGDataset(dataset_dict)\ndataloader = GeometricDataLoader(dataset, batch_size=5)\ntest(model, dataloader, device=torch.device('cuda'))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-01T01:13:50.657114Z","iopub.execute_input":"2023-07-01T01:13:50.658039Z","iopub.status.idle":"2023-07-01T01:14:09.386585Z","shell.execute_reply.started":"2023-07-01T01:13:50.658006Z","shell.execute_reply":"2023-07-01T01:14:09.384769Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd6051b6c28d4ea7ad3d6d123b62fb43"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Input: Aaron S Daggett was awarded the Purple Heart. The Battle of Mine Run was one fought by Aaron S Daggett. Stellendam, Netherlands is the birthplace of Ab Klink. Abdul Rahman Ya'kub was in office while Tuanku Bujang Tuanku Othman was Vice President. Abdul Taib Mahmud belongs to the party of Parti Bumiputera Sarawak.\nOutput: [\"Ab Klink wurde von Abdul Rahman Ya'kub in Amt angewart\"]\nInput: Abdul Taib Mahmud's successor was Sulaiman Abdul Rahman Taib. Abdulsalami Abubakar ended his career on 1999-05-29. Abdulsalami Abubakar was born in Minna. Abdulsalami Abubakar's birthplace was Niger State. Abner W. Sibal ended his military career January 3, 1965.\nOutput: ['Abdulsalami Abubakar hat seine militärische Karriere Ende Ende 1999']\nInput: Abner W Sibal died in Alexandria, Virginia. Adam Holloway was born in Kent. Adam Holloway's residence is Gravesend. The alma mater of Adenan Satem is the University of Adelaide. Adolf Schärf's place of birth was Mikulov.\nOutput: ['Adam Holloway wurde in Kent geboren.']\nInput: Adonis Georgiadis was in office while Antonis Samaras was Prime Minister. Adonis Georgiadis worked as Deputy Minister for Development, Competitiveness and Shipping. Agnes Kant was born in Hessisch Oldendorf. Marietta, Ohio was the birthplace of Agnes Ward White. Airey Neave started his career on 30th June 1953.\nOutput: ['Adonis Georgiadis arbeitete als Deputy Minister for Development, Competitiveness and Shipping']\nInput: Airey Neave was awarded the Military Cross. Knightsbridge, London is the birthplace of Airey Neave. Airey Neave's military rank is Lieutenant Colonel. Airey Neave began his military career in 1935. Albert B.White is a member of the Republican Party of the United States.\nOutput: ['Airey Neave began his military career in 1935.']\nInput: The spouse of Albert B. White was Agnes Ward White. Albert Jennings Fountain served in the Union Army. Alberto Teisaire's nationality is Argentinian. Alberto Teisaire was a Rear Admiral in the Argentine Navy. Alfons Gorbach's place of death was in Austria.\nOutput: ['Alberto Teisaire war ein Argentinier, der als Rear Admiral in der']\nInput: Alfred Moore Scales started his career on 1875-03-04. Alfred Moore Scales took part in the Battle of Chancellorsville. Darien, Connecticut is the birthplace of Alfred N. Phillips. Alfred N Phillips was Mayor of Stamford, Connecticut. Allan Shivers started his career from January 21, 1947.\nOutput: ['Alfred Moore Scales began his career in 1875, 1875, 1803, 18']\nInput: Allan Shivers served in the United States Army. Allan Shivers had the successor Price Daniel. Alvah Sabin was an active politician beginning March 4, 1853. Alvah Sabin was a member of the United States Whig Party. Robert E Lee was a commander in the American Civil War.\nOutput: ['Alvah Sabin war ein aktiver Politiker, der am 4. März 1853 ']\nInput: In Austria, the language is Austrian German. Robert E Lee was commander in the Battle of Antietam. The Battle of Cold Harbor took place during the American Civil War. Gabriela Michetti is a leader in Buenos Aires. Colin Powell was a commander in the Gulf War.\nOutput: ['Gabriela Michetti ist ein österreichischer Führer in Buenos Aires.']\nInput: Dwight D. Eisenhower was the predecessor to John F. Kennedy. The University of Vienna is the Alma mater of Josef Klaus. Francisco Franco was the predecessor of Juan Carlos I of Spain. Juan Perón was a member of the Labour Party in Argentina. The leader of the Netherlands is Mark Rutte.\nOutput: ['Francisco Franco war der Vorgänger von Juan Carlos I von Spanien.']\nInput: The United States Army was involved in battles in the Spanish-American War. The birthplace of William M. O. Dawson was Bloomington Maryland. Dodge Coronet is the alternative name for the 1955 Dodge. The 1955 Dodge engine is 230 cubic inches. The A-Rosa Luna was christened on the 7th of April 2005.\nOutput: ['Der 1955 Dodge Coronet ist ein Ersatzname für den 1955 Dodge.']\nInput: The location of AIDA Cruises is Rostock. AIDA Cruises is the operator of the AIDAluna. The AIDAstella was christened on the 16th of March, 2013. The AIDAstella was completed on March 11th 2013. The AIDAstella had its maiden voyage on March 17, 2013.\nOutput: ['Die AIDAstella wurde am 16. März 2013 eröffnet.']\nInput: The ship AIDAstella is operated by AIDA Cruises. The launch site of ALV X-1 was Mid Atlantic Regional Spaceport. The AMC Matador is also known as American Motors Matador. The ARA Veinticinco de Mayo (V-2) was sold to the Netherlands on the 1st of April 1948. The Abarth 1000 GT Coupé has a straight-four engine.\nOutput: ['Das AMC Matador ist ein amerikanisches Schiff, das es aufgrund']\nInput: The Acura TLX has a Honda K engine. The Acura TLX has an AWD vehicle layout. The Acura TLX is related to the Honda Accord. The transmission of the Acura TLX is a 9-speed ZF 9HP automatic (V6). The Alfa Romeo 164 was assembled in Arese.\nOutput: ['Die Acura TLX ist mit einem 9-speed ZF 9HP-Automatik']\nInput: Alfa Romeo 164 and Lancia Thema are related types of transportation. The Alfa Romeo 164 has a 4-speed automatic (ZF 4HP18QE) transmission. The Alhambra was built by the Samuda Brothers. The Alhambra ship beam is 8.3m. The Alhambra had wrecked.\nOutput: ['Die Alhambra wurde von den Samuda Brothers gebaut.']\nInput: The Alhambra had a top speed of 18.52 km/h. Alvis Speed 25 has a straight-six engine. Production, of the Alvis Speed 25, began in 1937. The American submarine NR-1 has a ship draft of 4.6m. The Antares rocket's final flight was on the 28th October 2014.\nOutput: ['Die Alhambra hat eine Spitzengeschwindigkeit von 18,52 km/h.']\nInput: The Antares rocket was launched at the Mid-Atlantic Regional Spaceport Launch Pad 0. Gabriela Michetti is the leader of Argentina. Guiana Space Centre was the launch site of the Ariane 5. The Ariane 5 had its first voyage August 11, 2005. The Aston Martin V8 is a convertible.\nOutput: ['Das Ariane 5 wurde auf dem mittleren Atlantik-Raumschiff']\nInput: The manufacturer of the Aston Martin V8 is the Aston Martin. The Aston Martin Virage is made by the company Aston Martin. Atlas II had its final flight on March 16th 1998. Atlas II was launched from Vandenberg Air Force Base. Audi was founded by August Horch.\nOutput: ['Atlas II wurde von der ersten Serie Atlas II gebaut.']\nInput: The Audi A1 is assembled by Audi Brussels. The Seat Ibiza and the Audi A1 are both cars and therefore a related means of transportation. Caterpillar Inc. is located in Peoria, Illinois. Costa Crociere is located in Genoa. The DeSoto Custom and Chrysler Newport are related types of transport.\nOutput: ['Die Costa Crociere ist in Genoa.']\nInput: The Fiat Croma is related to the Alfa Romeo 164. Germans are the people of Germany. Norbert Lammert is a leader in Germany. The leader of Germany is Stanislaw Tillich. Gruppo Bertone was founded by Giovanni Bertone.\nOutput: ['Norbert Lammert ist ein führender deutscher Staatsmann.']\nInput: The Guiana Space Centre has its headquarters at Kourou in French Guiana. The manufacturer of the Honda K engine is Honda. The Star Spangled Banner is the national anthem of the united States. Native Americans are on of the ethnic groups in the United States. The Chairman of A C Milan is Silvio Berlusconi.\nOutput: ['Der Hauptsitz des Space Centre Guiana ist Silvio Berlusconi, der Präsident']\nInput: Aaron Boogaard's birthplace is Canada. Aaron Hunt played for the club SV Werder Bremen. Abel Hernandez is in the Uruguay Olympic football team. Abel Hernandez's club is the Uruguay National football team. The footballer Abner was born in Brazil.\nOutput: ['Abel Hernandez ist der Mitglied der Uruguay Olympic team.']\nInput: Adam Maher's birthplace is Diemen. Adam Maher was born in the Netherlands. Adam Maher played for the Netherlands national under-17 football team. Adam Maher's club is PSV Eindhoven. Ahmad Kadhim Assad plays for the Iraq national football team.\nOutput: ['Adam Maher spielt für die irakischeische Fußball.']\nInput: Akeem Adams was born in Point Fortin. Akeem Dent's former team is the Atlanta Falcons. Akeem Priestley's club is the Connecticut Huskies. Al-Khor Sports Club is in the Qatar Stars League. Basim Qasim is the manager of Al-Zawra'a SC.\nOutput: ['Akeem Adams wurde geboren in Point Fortin.']\nInput: Al Kharaitiyat SC play in the Qatar Stars League. Alaa Abdul-Zahra was born in Baghdad. Alaa Abdul-Zahra's club is Sanat Mes Kerman F.C. Alan Martin played football for Hamilton Academical F.C. Alan Martin is a footballer with the Scotland national under-19 football team club.\nOutput: ['Alan Martin ist ein Fußballer bei der Hamilton Academical F.C.']\nInput: Aleksander Barkov Jr's birthdate is 1995-09-02. Aleksander Barkov Jr has played for the Florida Panthers. Aleksander Barkov Jr is 1.905m tall. Aleksander Barkov Jr played in the National Hockey League. Aleksandr Prudnikov's date of birth is 1989-02-24.\nOutput: ['Aleksander Barkov Jr spielt in der National Hockey League.']\nInput: Aleksandr Prudnikov plays for FC Amkar Perm. Aleksandre Guruli was born in Batumi. Aleksandre Guruli plays for FC Samtredia. Aleksandre Guruli's club is the Georgia national under 21 football team. Alessio Romagnoli plays for A.S. Roma.\nOutput: ['Aleksandre Guruli spielt für FC Samtredia.']\nInput: Alessio Romagnoli plays for the A.S. Roma youth team. Alex Plante was born in Manitoba. Alex Tyus's league is the Turkish Basketball Super League. Atlanta Falcons play in Atlanta. Don Sweeney is general manager for the Boston Bruins.\nOutput: ['Alex Tyus spielt in der NBA, es spielt in der NBA.']\nInput: Budapest is a city in Hungary. English is the language spoken in Canada. In Canada, the language is Slavey. The Columbus Blue Jackets' city is Columbus, Ohio. Oleh Luzhny is the manager of FC Karpaty Lviv.\nOutput: ['Die Sprache ist Slavey.']\nInput: Valery Petrakov is the manager of FC Torpedo Moscow. Gábor Kubatov is the chairman of Ferencvárosi TC. The language of Finland is the Finnish language. The coach of the Italian national under 16 football team is Daniele Zoratto. Maccabi Ashdod BC's coach is Zvi Sherf.\nOutput: ['Gábor Kubatov ist der Leiter des FC Torpedo Moscow.']\nInput: Motherwell FC's ground is Fir Park. Jean-Michel Aulas is the owner of Olympique Lyonnais. The St. Louis Rams are based in the city of St. Louis. Tennessee Titans are based in Nashville, Tennessee. Aaron Bertram started performing in 1998.\nOutput: ['Die St. Louis Rams sind in St. Louis based in Nashville, Tennessee.']\nInput: Aaron Deer collaborates with The Horns of Happiness. Aaron Deer has been signed to the record label Kill Rock Stars. Aaron Turner falls in the genre of avant-garde metal music. Aaron Turner plays progressive metal music. Abradab has a background as a solo singer.\nOutput: ['Aaron Deer ist ein Mitglied des Labels Kill Rock Stars.']\nInput: Abradab was born on November 12th 1978. The birth place of Abradab is Poland. The musical genre of Abradab is hip hop music. Agustn Barboza plays Guarania style of music. Ahmet Ertegun was born in Istanbul, Turkey.\nOutput: ['Ahmet Ertegun, a sa s a s ']\nInput: Alan Frew is a performer of rock music. Albennie Jones has worked with the musical artist Sammy Price. Aleksandra Kovac is a solo singer. Aleksandra Kovac was born in Yugoslavia. The musical genre of Aleksandra Kovac is rhythm and blues.\nOutput: ['Albennie Jones ist ein Musiker, der mit dem Musiker Sammy Price zusammenarbeit']\nInput: Aleksandra Kova's genre is soul music. The musical genre of Alex Day is Synthpop. Alfredo Zitarrosa is associated with fellow musician Héctor Numa Moraes. Montevideo is the birth place of Alfredo Zitarrosa. Alfredo Zitarrosa died in Uruguay.\nOutput: ['Alfredo Zitarrosa ist ein Musiker, der sich mit der Musik von Aleks']\nInput: Alfredo Zitarrosa was signed to the RCA Records label. Alison O'Donnell started performing in 1963. Alison O'Donnell plays jazz music. Alison O'Donnell is a singer and also plays the bodhran, percussion and autoharp. Allen Forrest was born on February 4, 1981.\nOutput: ['Allen Forrest ist ein amerikanischer Sänger und spielt auch die bodhran, ']\nInput: Allen Forrest plays pop music. The location of Alligator Records is Chicago. Anders Osborne is associated with the musical artist Johnny Sansone. Anders Osborne performs rhythm and blues music. Anders Osborne was signed to the record label 'Alligator Records'.\nOutput: ['Allen Forrest spielt Popmusik.']\nInput: Andra is a rhythm and blues singer. Andrew Rayel began his musical career in 2009. Andrew Rayel is associated with Christian Burns. The birth place of Andrew Rayel is Chisinau, Moldova. The musician, Andrew White, is associated with the musical artist, Marry Banilow.\nOutput: ['Andrew Rayel ist ein Musiker, der mit Christian Burns zusammenlebt.']\nInput: Andrew White is a musician signed to the record label B-Unique Records. Asunción is a part of Gran Asunción. Christian Burns plays the guitar. Hip hop music has the sub genre Gangsta rap. Hip hop music originated from funk music.\nOutput: ['Asunción ist ein Teil des Gran Asunción.']\nInput: The stylistic origin of indie rock is new wave music. Manchester is part of Greater Manchester. Polydor Records are based in London. The distribution company for RCA Records is Sony Music Entertainment. Rock and roll music originated from blues music.\nOutput: ['Die Distributionsfirma für RCA Records ist Sony Music Entertainment.']\nInput: Synthpop is a form of pop music. People from the United Kingdom are called British people. (19255) 1994 VK8 has a density of 2.0 grams per cubic centimetre. (29075) 1950 DA was discovered by Carl A Wirtanen. (410777) 2009 FD has an absolute magnitude of 22.1.\nOutput: ['(410777) 1994 VK8 hat eine Dichte von 2,0 g/']\nInput: (410777) 2009 FD has an apoapsis of 259776702.47055 kilometres. (66063) 1998 RO1 has an apoapsis of 254989570.60815 kilometres. 66063 1998 RO1 has an epoch date of 4 November 2013. 1000 Piazzia has an escape velocity of 0.0252 kilometres per second. 1001 Gaussia was formerly known as 1923 OAA907 XC.\nOutput: ['66063 1998 RO1 hat eine apoapse von 259776']\nInput: the maximum temperature of 1001 Gaussia is 165 kelvins. 101 Helena has an apoapsis of 441092000.0 (kilometres). The epoch of 101 Helena is 2006-12-31. 101 Helena has a mass of 3.0 kgs. 1036 Ganymed has an average speed of 16.86 km per sec.\nOutput: ['101 Helena hat eine apoapse von 41092000.0 (km']\nInput: 103 Hera has an absolute magnitude of 7.66. 103 Hera's mass is 7.9kg. The asteroid called 107 Camilla has an absolute magnitude of 7.08. 107 Camilla was discovered by A Storrs. 1089 Tama had the former name of \"A894 VA; A904 VD;\".\nOutput: ['103 Hera hat eine masse von 7,9 kg.']\nInput: 1089 Tama has an orbital period of 1202.846 days. 1097 Vicia has an average speed of 17.92 kilometres per second. The epoch of 1097 Vicia is on 13 January 2016. The rotation period of 1097 Vicia is 95040.0. Christian Heinrich Friedrich Peters was the discoverer of 109 Felicitas.\nOutput: ['1089 Tama hat eine durchschnittliche Geschwindigkeit von 0 0 0 ']\nInput: 109 Felicitas has a periapsis of 283326000000.0. The average speed of 10 Hygiea is 16.76 km per secs. The periapsis of 1101 Clematis is 445895000000.0. 110 Lydia has an epoch date of December 31st 2006. 11264 Claudiomaccone has a temperature of 173.0 (kelvins).\nOutput: ['11264 Claudiomaccone hat eine periapse von 458']\nInput: James Craig Watson's alma mater was the University of Michigan. N R Pogson was born in Nottingham. Aaron S. Daggett fought in the Battle of Fredericksburg and received the Purple Heart. Aaron S Daggett fought at the Battle of Gettysburg and was awarded the Purple Heart. Stephen Yong Kuet Tze is Abdul Rahman Ya'kub's deputy. Abdul Taib Mahmud was Abdul Rahman Ya'kub's successor.\nOutput: ['N R Pogson, geboren in Nottingham, ist ein Mitglied der Universitat']\nInput: Abdul Taid Mahmud was born in Sarawak and is part of the Parti Pesaka Bumiputera Bersatu. Abdulsalami Abubakar, was born in Minna, and has the Technical Institute Kaduna, for an almar mater. Abdulsalami Abubakar was born in Niger State and was the Chief of the Defence Staff in Nigeria. Al-Amin Daggash was succeeded as Chief of the Defence Staff in Nigeria by Abdulsalami Abubakar. Abdulsalami Abubakar, ended his career on 1999-05-29, and was succeeded by Olusegun Obasanjo.\nOutput: ['Abdulsalami Abubakar, a stud in Universidad, a']\nInput: Abdulsalami Abubakar was also in office while Mr. Mike Akhigbe was Vice President. Olusegun Obasanjo succeeded Mr. Abubaka. Joseph Stalin was a commander in World War II in which Abner W Sibal also fought. Abner W. Sibal serve in the United States army which took part in the Korean War. Adam Holloway went to school at Magdalene College Cambridge and was born in Kent. Born in Kent, Adam Holloway now lives in Gravesend.\nOutput: ['Adam Holloway, geboren in Kent, lebt in Gravesend.']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m dataset \u001b[38;5;241m=\u001b[39m WebNLGDataset(dataset_dict)\n\u001b[1;32m     25\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m GeometricDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m     10\u001b[0m data\u001b[38;5;241m.\u001b[39mattention_mask \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Convert the tensor outputs to text using the tokenizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m output_text \u001b[38;5;241m=\u001b[39m [dataset\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(g, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m outputs]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1522\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2339\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2339\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2347\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1720\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1717\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1720\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1735\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1090\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1078\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1079\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m     )\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1090\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:753\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    750\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> 753\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:341\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 341\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[1;32m    343\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:252\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    253\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"!pip install sacrebleu ","metadata":{"execution":{"iopub.status.busy":"2023-06-30T23:17:37.061001Z","iopub.execute_input":"2023-06-30T23:17:37.061401Z","iopub.status.idle":"2023-06-30T23:17:48.707662Z","shell.execute_reply.started":"2023-06-30T23:17:37.061369Z","shell.execute_reply":"2023-06-30T23:17:48.706412Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting sacrebleu\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.5.5)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.2)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.7.0 sacrebleu-2.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from sacrebleu import corpus_bleu\nfrom random import sample\nfrom tqdm import tqdm\nfrom torch_geometric.data import DataLoader as GeometricDataLoader\n\n# load the WebNLG validation dataset\nvalidation_dataset = load_dataset('web_nlg', 'webnlg_challenge_2017')['test']\nvalidation_dataset = [sample for sample in validation_dataset if sample['lex']['text']]  # filter out samples with empty targets\n\nvalidation_data = WebNLGDataset(validation_dataset)\n\n# set up the validation data loader\nvalidation_loader = GeometricDataLoader(validation_data, batch_size=1, shuffle=False)\n\n# switch model to evaluation mode\nmodel.eval()\n\ndevice = 'cuda'\n\n# generate predictions for the validation dataset\npredictions = []\nreferences = []\nwith torch.no_grad():\n    for data in tqdm(validation_loader, desc='Validation Progress', leave=False):\n        data.x = data.x.to(device).unsqueeze(0)  # add batch dimension\n        data.attention_mask = data.attention_mask.to(device).unsqueeze(0)  # add batch dimension\n\n        outputs = model.transformer.generate(input_ids=data.x, attention_mask=data.attention_mask, decoder_start_token_id=model.transformer.config.pad_token_id)\n        # convert token IDs to strings\n        predicted_texts = dataset.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        target_texts = dataset.tokenizer.batch_decode(data.y, skip_special_tokens=True)\n        # append predicted and target texts for BLEU evaluation\n        predictions.extend(predicted_texts)\n        references.extend(target_texts)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-30T23:38:16.533429Z","iopub.execute_input":"2023-06-30T23:38:16.534404Z","iopub.status.idle":"2023-06-30T23:52:22.493726Z","shell.execute_reply.started":"2023-06-30T23:38:16.534370Z","shell.execute_reply":"2023-06-30T23:52:22.492154Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0fd832f1cf043149ea830474e255c9c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n  warnings.warn(out)\nValidation Progress:   0%|          | 0/2753 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n                                                                        \r","output_type":"stream"}]},{"cell_type":"code","source":"# calculate BLEU scores\n#bleu = corpus_bleu(predictions, [references])\n\nmultiple_references = []\nfor i in range(len(validation_dataset)):\n    multiple_references.append(validation_dataset[i]['lex']['text'])\n    \n#bleu = corpus_bleu(predictions, references)\nbleu_multiple = corpus_bleu(predictions, multiple_references)\n\n#print(f\"BLEU score: {bleu.score}\")\nprint(f\"BLEU score with multiple references: {bleu_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-30T23:52:32.050420Z","iopub.execute_input":"2023-06-30T23:52:32.050776Z","iopub.status.idle":"2023-06-30T23:52:32.502744Z","shell.execute_reply.started":"2023-06-30T23:52:32.050748Z","shell.execute_reply":"2023-06-30T23:52:32.501759Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"BLEU score with multiple references: 100.00000000000004\n","output_type":"stream"}]},{"cell_type":"code","source":"len(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T23:52:37.481718Z","iopub.execute_input":"2023-06-30T23:52:37.482090Z","iopub.status.idle":"2023-06-30T23:52:37.488482Z","shell.execute_reply.started":"2023-06-30T23:52:37.482060Z","shell.execute_reply":"2023-06-30T23:52:37.487464Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"2753"},"metadata":{}}]},{"cell_type":"code","source":"len(validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T23:52:40.575623Z","iopub.execute_input":"2023-06-30T23:52:40.575980Z","iopub.status.idle":"2023-06-30T23:52:40.582272Z","shell.execute_reply.started":"2023-06-30T23:52:40.575952Z","shell.execute_reply":"2023-06-30T23:52:40.581158Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"2753"},"metadata":{}}]},{"cell_type":"code","source":"i=1860\nprint(validation_dataset[i])\nprint('------------------------')\nprint(validation_dataset[i]['original_triple_sets']['otriple_set'])\nprint('------------------------')\nprint(predictions[i])\nprint('------------------------')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-06-30T23:53:22.682673Z","iopub.execute_input":"2023-06-30T23:53:22.683053Z","iopub.status.idle":"2023-06-30T23:53:22.689803Z","shell.execute_reply.started":"2023-06-30T23:53:22.683024Z","shell.execute_reply":"2023-06-30T23:53:22.688806Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"{'category': 'Astronaut', 'size': 7, 'eid': 'Id970', 'original_triple_sets': {'otriple_set': [['William_Anders | dateOfRet | \"1969-09-01\"^^xsd:date', 'William_Anders | mission | Apollo_8', 'William_Anders | nationality | United_States', 'William_Anders | birthPlace | British_Hong_Kong', 'Apollo_8 | crew2Up | Buzz_Aldrin', 'Apollo_8 | crewMembers | Frank_Borman', 'Apollo_8 | operator | NASA']]}, 'modified_triple_sets': {'mtriple_set': [['William_Anders | dateOfRetirement | \"1969-09-01\"', 'William_Anders | was a crew member of | Apollo_8', 'William_Anders | nationality | United_States', 'William_Anders | birthPlace | British_Hong_Kong', 'Apollo_8 | backup pilot | Buzz_Aldrin', 'Apollo_8 | crewMembers | Frank_Borman', 'Apollo_8 | operator | NASA']]}, 'shape': '', 'shape_type': '', 'lex': {'comment': ['good', 'good', 'good'], 'lid': ['Id1', 'Id2', 'Id3'], 'text': [\"William Anders was born in British Hong Kong and is a U.S Citizen. William was a member of the Apollo 8 crew (along with Frank Borman) which was operated by NASA's backup pilot Buzz Aldrin. William retired on September 1st in 1969.\", 'William Anders was born in British Hong Kong but is an American. He was a member of Apollo 8 which is operated by NASA. His backup pilot was Buzz Aldrin. Anders retired in 1960-09-01.', \"William Anders was from the US and he was born in British Hong Kong. Alongside Frank Borman, he crewed the NASA operated Apollo 8 before he retired on1969-09-01. Apollo 8's backup pilot was Buzz Aldrin.\"], 'lang': ['', '', '']}, 'test_category': 'testdata_with_lex', 'dbpedia_links': [], 'links': []}\n------------------------\n[['William_Anders | dateOfRet | \"1969-09-01\"^^xsd:date', 'William_Anders | mission | Apollo_8', 'William_Anders | nationality | United_States', 'William_Anders | birthPlace | British_Hong_Kong', 'Apollo_8 | crew2Up | Buzz_Aldrin', 'Apollo_8 | crewMembers | Frank_Borman', 'Apollo_8 | operator | NASA']]\n------------------------\nis a U.S Citizen. William Anders was born in Hong Kong\n------------------------\n[\"William Anders was born in British Hong Kong and is a U.S Citizen. William was a member of the Apollo 8 crew (along with Frank Borman) which was operated by NASA's backup pilot Buzz Aldrin. William retired on September 1st in 1969.\", 'William Anders was born in British Hong Kong but is an American. He was a member of Apollo 8 which is operated by NASA. His backup pilot was Buzz Aldrin. Anders retired in 1960-09-01.', \"William Anders was from the US and he was born in British Hong Kong. Alongside Frank Borman, he crewed the NASA operated Apollo 8 before he retired on1969-09-01. Apollo 8's backup pilot was Buzz Aldrin.\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the postprocessed output for the first item in the dataset\ndataset[-3]","metadata":{"execution":{"iopub.status.busy":"2023-06-30T21:26:06.582363Z","iopub.status.idle":"2023-06-30T21:26:06.583082Z","shell.execute_reply.started":"2023-06-30T21:26:06.582820Z","shell.execute_reply":"2023-06-30T21:26:06.582843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}