{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"2a8badda5171f6c1da75e6dcec216359e8635e393e06f848b1b87b76c1bdea5e"}},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"27dac6771009442986c337835ac2fab0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81ab3401681e449faca4585337346d4a","IPY_MODEL_ac24b2b5a91b48e6a8746e3e0a6bdd56","IPY_MODEL_099b3ea61045469caa5684f5d4b5fa38"],"layout":"IPY_MODEL_d41d0e0245a142589c8f5767ce8e0d3b"}},"81ab3401681e449faca4585337346d4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e466a23c1954b84bd29cee2299101b5","placeholder":"​","style":"IPY_MODEL_094afeea467543ae879be701e4a73d63","value":"100%"}},"ac24b2b5a91b48e6a8746e3e0a6bdd56":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa9427c68daf40d3af8410141bfe94b3","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6dfc55f38ffe49e59ccb600b0de75088","value":3}},"099b3ea61045469caa5684f5d4b5fa38":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec91a1ad9e7b4bafbc79f9ddfaa0296e","placeholder":"​","style":"IPY_MODEL_4cec0b2d487b4c13ad153e6924cc8e4a","value":" 3/3 [00:00&lt;00:00, 65.75it/s]"}},"d41d0e0245a142589c8f5767ce8e0d3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e466a23c1954b84bd29cee2299101b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"094afeea467543ae879be701e4a73d63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa9427c68daf40d3af8410141bfe94b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dfc55f38ffe49e59ccb600b0de75088":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec91a1ad9e7b4bafbc79f9ddfaa0296e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cec0b2d487b4c13ad153e6924cc8e4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1af09f1f71524f44b3f79ed0427c8559":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4027186c2a1646c7a47db3e69d3ef93b","IPY_MODEL_c224ebc06fcf44dc80e8285b6578680e","IPY_MODEL_f3f96e77a1134c5c93ba218b8261ec0f"],"layout":"IPY_MODEL_45509cc5af034006bade5bff3fe9ea3b"}},"4027186c2a1646c7a47db3e69d3ef93b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7719bedd5d2544a38a46f058d0ef1a7e","placeholder":"​","style":"IPY_MODEL_305d7d10c879476ea87f924e2f802958","value":"100%"}},"c224ebc06fcf44dc80e8285b6578680e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4731ce03cfa84014bce0054c8611b4f9","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6af2a7876d954b93aca5a16930c9f079","value":3}},"f3f96e77a1134c5c93ba218b8261ec0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_528070b64e7d4bb0a891f1dbe89ae5bc","placeholder":"​","style":"IPY_MODEL_a99d41291bf84041a091180eb477e519","value":" 3/3 [00:00&lt;00:00, 106.45it/s]"}},"45509cc5af034006bade5bff3fe9ea3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7719bedd5d2544a38a46f058d0ef1a7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"305d7d10c879476ea87f924e2f802958":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4731ce03cfa84014bce0054c8611b4f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6af2a7876d954b93aca5a16930c9f079":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"528070b64e7d4bb0a891f1dbe89ae5bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a99d41291bf84041a091180eb477e519":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d15948f114040928a325e1665e0b1bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9aa2a3f7d6ec414cb6bc735391cc7462","IPY_MODEL_a72bd5b05ddc4e20bd777a1e51bff87f","IPY_MODEL_45e968b74fdc440382f08bfe83797d7f"],"layout":"IPY_MODEL_27829187a83e45859d0adf6deb1a5f12"}},"9aa2a3f7d6ec414cb6bc735391cc7462":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01cfd32ca9ea4e54a1bff6119613d64a","placeholder":"​","style":"IPY_MODEL_abcb191fcc264867b122d58caf9e3652","value":"Downloading (…)ve/main/spiece.model: 100%"}},"a72bd5b05ddc4e20bd777a1e51bff87f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1375be65005469a9c05fb385f1597db","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_194bc9079f024606b19ed153cd2d2226","value":791656}},"45e968b74fdc440382f08bfe83797d7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccf95078f5474a8a9917dc95c2d296df","placeholder":"​","style":"IPY_MODEL_8c955109118d4ffcb3b5b9db8520eb2a","value":" 792k/792k [00:00&lt;00:00, 3.16MB/s]"}},"27829187a83e45859d0adf6deb1a5f12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01cfd32ca9ea4e54a1bff6119613d64a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abcb191fcc264867b122d58caf9e3652":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1375be65005469a9c05fb385f1597db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"194bc9079f024606b19ed153cd2d2226":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ccf95078f5474a8a9917dc95c2d296df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c955109118d4ffcb3b5b9db8520eb2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"836b768308904b9e98965838df6d56cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c26f17223f9049fabbeeef900dc0f084","IPY_MODEL_096b9fdf24f44aedaa16e9f31bff5e74","IPY_MODEL_3f2e2e165fcf4575bfd2127e0f392e0d"],"layout":"IPY_MODEL_511bf71170c94239b561c6ed9c7a2014"}},"c26f17223f9049fabbeeef900dc0f084":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e0a091ca3f743b599684ec709c0724d","placeholder":"​","style":"IPY_MODEL_7f042786107f41a5be4dffec5461e220","value":"Downloading (…)lve/main/config.json: 100%"}},"096b9fdf24f44aedaa16e9f31bff5e74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98782b2ca67b42eabe9128090a4f3eec","max":1206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b61db7dc7854a6da1f96c46dfbdf72c","value":1206}},"3f2e2e165fcf4575bfd2127e0f392e0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a85ddf4d0fdf44ba815ca3f03b476407","placeholder":"​","style":"IPY_MODEL_46f3784588fe467cbddcf6e15bfcaa82","value":" 1.21k/1.21k [00:00&lt;00:00, 21.9kB/s]"}},"511bf71170c94239b561c6ed9c7a2014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e0a091ca3f743b599684ec709c0724d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f042786107f41a5be4dffec5461e220":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98782b2ca67b42eabe9128090a4f3eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b61db7dc7854a6da1f96c46dfbdf72c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a85ddf4d0fdf44ba815ca3f03b476407":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46f3784588fe467cbddcf6e15bfcaa82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76ea121789c34ab0ae3db2a11ce092dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0755e20e9e894e0782a6dc013944a602","IPY_MODEL_16d5d7274b774723913d9a9df040db90","IPY_MODEL_ca6af7cc25b745999b2e819972a5794c"],"layout":"IPY_MODEL_46a8ce2527bb48088f5a414f6df0ee73"}},"0755e20e9e894e0782a6dc013944a602":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed369610f713419fae0a9c26abee2b91","placeholder":"​","style":"IPY_MODEL_fe46615805e146788d96ca94f066fd87","value":"Downloading pytorch_model.bin: 100%"}},"16d5d7274b774723913d9a9df040db90":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f15c2f29bfa54944989bf35497436615","max":242065649,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8c7e7ef1e3a34c228e0972f094716cef","value":242065649}},"ca6af7cc25b745999b2e819972a5794c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2766052cd6cd43288677e89f524222aa","placeholder":"​","style":"IPY_MODEL_c1326237db9f4a39bb312b80678be410","value":" 242M/242M [00:01&lt;00:00, 240MB/s]"}},"46a8ce2527bb48088f5a414f6df0ee73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed369610f713419fae0a9c26abee2b91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe46615805e146788d96ca94f066fd87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f15c2f29bfa54944989bf35497436615":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c7e7ef1e3a34c228e0972f094716cef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2766052cd6cd43288677e89f524222aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1326237db9f4a39bb312b80678be410":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2bd17acda9f4a4cb40387e7a16a4aad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34b7d58e7c434652ab4a9f4cecef0791","IPY_MODEL_c406fb3d9ff8428da7b08d1dbf76e81a","IPY_MODEL_ceff9ff6affd4f168cff5c526d56fcd7"],"layout":"IPY_MODEL_13c9318a176f42ecac5d6162d510395f"}},"34b7d58e7c434652ab4a9f4cecef0791":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99ea585e29ff4e2a966d1c69be32cb97","placeholder":"​","style":"IPY_MODEL_1f85b576a95d4b6a8ee70e78ecaa34a7","value":"Downloading (…)neration_config.json: 100%"}},"c406fb3d9ff8428da7b08d1dbf76e81a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c80227b65297408db3c88583a2291f7b","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d55e7db195b44955b09e0f04598a3598","value":147}},"ceff9ff6affd4f168cff5c526d56fcd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85e687ff32694c0aa1bedacf5b4bbf21","placeholder":"​","style":"IPY_MODEL_0982aa5f0748431e86626537055d9978","value":" 147/147 [00:00&lt;00:00, 2.48kB/s]"}},"13c9318a176f42ecac5d6162d510395f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99ea585e29ff4e2a966d1c69be32cb97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f85b576a95d4b6a8ee70e78ecaa34a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c80227b65297408db3c88583a2291f7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d55e7db195b44955b09e0f04598a3598":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85e687ff32694c0aa1bedacf5b4bbf21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0982aa5f0748431e86626537055d9978":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n!pip install datasets\n\n!pip install transformers \n!pip install sentencepiece\n\n!pip install sacrebleu\n'''","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahHaXoNB9iBc","outputId":"7a29568e-467f-4334-b715-11bd6c8b649d","execution":{"iopub.status.busy":"2023-05-24T20:39:06.853402Z","iopub.execute_input":"2023-05-24T20:39:06.853745Z","iopub.status.idle":"2023-05-24T20:39:06.869756Z","shell.execute_reply.started":"2023-05-24T20:39:06.853716Z","shell.execute_reply":"2023-05-24T20:39:06.868644Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\n!pip install datasets\\n\\n!pip install transformers \\n!pip install sentencepiece\\n\\n!pip install sacrebleu\\n'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom torch.nn.parallel import DataParallel\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\nmodel = DataParallel(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255,"referenced_widgets":["0d15948f114040928a325e1665e0b1bc","9aa2a3f7d6ec414cb6bc735391cc7462","a72bd5b05ddc4e20bd777a1e51bff87f","45e968b74fdc440382f08bfe83797d7f","27829187a83e45859d0adf6deb1a5f12","01cfd32ca9ea4e54a1bff6119613d64a","abcb191fcc264867b122d58caf9e3652","a1375be65005469a9c05fb385f1597db","194bc9079f024606b19ed153cd2d2226","ccf95078f5474a8a9917dc95c2d296df","8c955109118d4ffcb3b5b9db8520eb2a","836b768308904b9e98965838df6d56cc","c26f17223f9049fabbeeef900dc0f084","096b9fdf24f44aedaa16e9f31bff5e74","3f2e2e165fcf4575bfd2127e0f392e0d","511bf71170c94239b561c6ed9c7a2014","3e0a091ca3f743b599684ec709c0724d","7f042786107f41a5be4dffec5461e220","98782b2ca67b42eabe9128090a4f3eec","6b61db7dc7854a6da1f96c46dfbdf72c","a85ddf4d0fdf44ba815ca3f03b476407","46f3784588fe467cbddcf6e15bfcaa82","76ea121789c34ab0ae3db2a11ce092dc","0755e20e9e894e0782a6dc013944a602","16d5d7274b774723913d9a9df040db90","ca6af7cc25b745999b2e819972a5794c","46a8ce2527bb48088f5a414f6df0ee73","ed369610f713419fae0a9c26abee2b91","fe46615805e146788d96ca94f066fd87","f15c2f29bfa54944989bf35497436615","8c7e7ef1e3a34c228e0972f094716cef","2766052cd6cd43288677e89f524222aa","c1326237db9f4a39bb312b80678be410","c2bd17acda9f4a4cb40387e7a16a4aad","34b7d58e7c434652ab4a9f4cecef0791","c406fb3d9ff8428da7b08d1dbf76e81a","ceff9ff6affd4f168cff5c526d56fcd7","13c9318a176f42ecac5d6162d510395f","99ea585e29ff4e2a966d1c69be32cb97","1f85b576a95d4b6a8ee70e78ecaa34a7","c80227b65297408db3c88583a2291f7b","d55e7db195b44955b09e0f04598a3598","85e687ff32694c0aa1bedacf5b4bbf21","0982aa5f0748431e86626537055d9978"]},"id":"7Ms8N01X9MlZ","outputId":"331ae2b5-3110-47bb-e7a1-01240b6cab7c","execution":{"iopub.status.busy":"2023-07-08T19:59:57.728547Z","iopub.execute_input":"2023-07-08T19:59:57.728900Z","iopub.status.idle":"2023-07-08T20:00:14.606036Z","shell.execute_reply.started":"2023-07-08T19:59:57.728870Z","shell.execute_reply":"2023-07-08T20:00:14.605027Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"536e38ba683a4d66bcba47af44ef5d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a452b1ba4dc042eca5fbe2858302d9c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b8d5919cdfe4f18922f9447f852d274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6414ada606b2433395c73aa65533bcd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a94eb8f1316e40a9ab4d5220deebbaf9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16f5cc74e8847bd8bf2754de1595a35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce68d3655f5436ab7685be73397cc3a"}},"metadata":{}}]},{"cell_type":"code","source":"new_tokens = ['<H>', '<R>', '<T>']\nnew_tokens_vocab = {}\nnew_tokens_vocab['additional_special_tokens'] = []\nfor idx, t in enumerate(new_tokens):\n    new_tokens_vocab['additional_special_tokens'].append(t)\nnum_added_toks = tokenizer.add_special_tokens(new_tokens_vocab)\n\ntokenizer.add_tokens(\"[MASK]\")\ntokenizer.mask_token = \"[MASK]\"\ntokenizer.mask_token_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:00:14.608325Z","iopub.execute_input":"2023-07-08T20:00:14.608747Z","iopub.status.idle":"2023-07-08T20:00:14.618486Z","shell.execute_reply.started":"2023-07-08T20:00:14.608712Z","shell.execute_reply":"2023-07-08T20:00:14.617449Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class WebNLGDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.prefix = \"translate from Graph to Text: \"\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # preprocess the input graph\n        try:\n            triples = item['modified_triple_sets']['mtriple_set']\n            input_text = self.prefix\n            for outer_list in triples: \n                for triple in outer_list:\n                    triple_txt = triple.split(\"|\")\n                    input_text += \" <H> \" + triple_txt[0] + \" <R> \" + triple_txt[1] + \" <T> \" + triple_txt[2]\n        except (KeyError, IndexError):\n            print(\"1\")\n            print(item['modified_triple_sets']['mtriple_set'])\n            print(item['modified_triple_sets']['mtriple_set'][0])\n            print(triples)\n            input_text = self.prefix\n        # preprocess the target text\n        try:\n            target_text = item['lex']['text'][0]\n        except (KeyError, IndexError):\n            print(\"2\")\n            print(item)\n            #print(item['modified_triple_sets']['mtriple_set'])\n            target_text = \"\"\n        #print(item)\n        #print(input_text)\n        # encode the inputs and targets using the tokenizer\n        input_ids = tokenizer.encode(input_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n        target_ids = tokenizer.encode(target_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n        return input_ids.squeeze(0), target_ids.squeeze(0)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:00:14.620237Z","iopub.execute_input":"2023-07-08T20:00:14.621203Z","iopub.status.idle":"2023-07-08T20:00:14.632906Z","shell.execute_reply.started":"2023-07-08T20:00:14.621167Z","shell.execute_reply":"2023-07-08T20:00:14.632105Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"MAX_INPUT_LENGTH = 128\nMAX_TARGET_LENGTH = 128\ntokenizer.model_max_length = MAX_INPUT_LENGTH\nmodel.module.config.max_length = MAX_TARGET_LENGTH\n\n# set up the device (GPU or CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1jjIPRO9Mla","outputId":"c138f2cb-2966-4872-ac87-616a2d4119d1","_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-07-08T20:00:14.635294Z","iopub.execute_input":"2023-07-08T20:00:14.635985Z","iopub.status.idle":"2023-07-08T20:00:20.073454Z","shell.execute_reply.started":"2023-07-08T20:00:14.635945Z","shell.execute_reply":"2023-07-08T20:00:20.072475Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): T5ForConditionalGeneration(\n    (shared): Embedding(32128, 512)\n    (encoder): T5Stack(\n      (embed_tokens): Embedding(32128, 512)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=384, bias=False)\n                (k): Linear(in_features=512, out_features=384, bias=False)\n                (v): Linear(in_features=512, out_features=384, bias=False)\n                (o): Linear(in_features=384, out_features=512, bias=False)\n                (relative_attention_bias): Embedding(32, 6)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n                (wo): Linear(in_features=1024, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-7): 7 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=384, bias=False)\n                (k): Linear(in_features=512, out_features=384, bias=False)\n                (v): Linear(in_features=512, out_features=384, bias=False)\n                (o): Linear(in_features=384, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n                (wo): Linear(in_features=1024, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder): T5Stack(\n      (embed_tokens): Embedding(32128, 512)\n      (block): ModuleList(\n        (0): T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=384, bias=False)\n                (k): Linear(in_features=512, out_features=384, bias=False)\n                (v): Linear(in_features=512, out_features=384, bias=False)\n                (o): Linear(in_features=384, out_features=512, bias=False)\n                (relative_attention_bias): Embedding(32, 6)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=384, bias=False)\n                (k): Linear(in_features=512, out_features=384, bias=False)\n                (v): Linear(in_features=512, out_features=384, bias=False)\n                (o): Linear(in_features=384, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n                (wo): Linear(in_features=1024, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (1-7): 7 x T5Block(\n          (layer): ModuleList(\n            (0): T5LayerSelfAttention(\n              (SelfAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=384, bias=False)\n                (k): Linear(in_features=512, out_features=384, bias=False)\n                (v): Linear(in_features=512, out_features=384, bias=False)\n                (o): Linear(in_features=384, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (1): T5LayerCrossAttention(\n              (EncDecAttention): T5Attention(\n                (q): Linear(in_features=512, out_features=384, bias=False)\n                (k): Linear(in_features=512, out_features=384, bias=False)\n                (v): Linear(in_features=512, out_features=384, bias=False)\n                (o): Linear(in_features=384, out_features=512, bias=False)\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (2): T5LayerFF(\n              (DenseReluDense): T5DenseGatedActDense(\n                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n                (wo): Linear(in_features=1024, out_features=512, bias=False)\n                (dropout): Dropout(p=0.1, inplace=False)\n                (act): NewGELUActivation()\n              )\n              (layer_norm): T5LayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (final_layer_norm): T5LayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# load the WebNLG dataset\ndataset = load_dataset('web_nlg', 'release_v3.0_en')['train']\ndataset_val = load_dataset('web_nlg', 'release_v3.0_en')['dev']","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87,"referenced_widgets":["1af09f1f71524f44b3f79ed0427c8559","4027186c2a1646c7a47db3e69d3ef93b","c224ebc06fcf44dc80e8285b6578680e","f3f96e77a1134c5c93ba218b8261ec0f","45509cc5af034006bade5bff3fe9ea3b","7719bedd5d2544a38a46f058d0ef1a7e","305d7d10c879476ea87f924e2f802958","4731ce03cfa84014bce0054c8611b4f9","6af2a7876d954b93aca5a16930c9f079","528070b64e7d4bb0a891f1dbe89ae5bc","a99d41291bf84041a091180eb477e519"]},"id":"qEf48SzQ9Mla","outputId":"4c918f52-9562-4b13-f519-0f20efe57b2d","execution":{"iopub.status.busy":"2023-07-08T20:00:20.075156Z","iopub.execute_input":"2023-07-08T20:00:20.075537Z","iopub.status.idle":"2023-07-08T20:00:34.609961Z","shell.execute_reply.started":"2023-07-08T20:00:20.075503Z","shell.execute_reply":"2023-07-08T20:00:34.609007Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee2bef0d37445e4a1d55adafd10ebe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/2.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c227e8c91d4ebcae818a6ca4bab707"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset web_nlg/release_v3.0_en (download: 24.32 MiB, generated: 15.75 MiB, post-processed: Unknown size, total: 40.07 MiB) to /root/.cache/huggingface/datasets/web_nlg/release_v3.0_en/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed120c6f8eea44cdbe990aa0255ada5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/13211 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/1667 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5713 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset web_nlg downloaded and prepared to /root/.cache/huggingface/datasets/web_nlg/release_v3.0_en/0.0.0/28ffb892f7f42450dd9558684aa43bcaf44b1b3bf0d77cb8d73534646af88dda. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32f7aca3908d435a8745d9b41b4ae6de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26fa8edbcacd475786243c49a4d4039c"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch\nimport numpy as np\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:00:34.611314Z","iopub.execute_input":"2023-07-08T20:00:34.612254Z","iopub.status.idle":"2023-07-08T20:00:34.625109Z","shell.execute_reply.started":"2023-07-08T20:00:34.612219Z","shell.execute_reply":"2023-07-08T20:00:34.624209Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Adaptive pretraining","metadata":{}},{"cell_type":"markdown","source":"For STA, we fine-tuned the PLMs on a small amount of labeled data from the target task using a maximum likelihood estimation (MLE) objective. This involves training the model to maximize the likelihood of generating the correct output given the input graph and labeled data. This process helps to further adapt the PLM to the specific requirements of the target task and improve its performance on that task.","metadata":{}},{"cell_type":"code","source":"import random\n\npretrain_batch_size = 60\n\npretrain_texts = []\nfor sample in dataset:\n    try:\n        text = sample['lex']['text'][0]\n        pretrain_texts.append(text)\n    except (KeyError, IndexError):\n        continue\n\ntokenized_inputs = tokenizer(pretrain_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\ninput_ids = tokenized_inputs['input_ids']\nattention_mask = tokenized_inputs['attention_mask']\n\npretrain_data = torch.utils.data.TensorDataset(input_ids, attention_mask)\n\npretrain_loader = torch.utils.data.DataLoader(pretrain_data, batch_size=pretrain_batch_size, shuffle=True)\n\npretrain_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\npretrain_criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\npretrain_epochs = 5  # Set the number of pre-training epochs\nmasking_prob = 0.15  # Probability of masking a token\n\n\n# Prepare validation data\nval_texts = []\nfor sample in dataset_val:\n    try:\n        text = sample['lex']['text'][0]\n        val_texts.append(text)\n    except (KeyError, IndexError):\n        continue\n\ntokenized_inputs_val = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\ninput_ids_val = tokenized_inputs_val['input_ids']\nattention_mask_val = tokenized_inputs_val['attention_mask']\n\nval_data = TensorDataset(input_ids_val, attention_mask_val)\n\nval_loader = DataLoader(val_data, batch_size=pretrain_batch_size, shuffle=True)\n\nearly_stopping = EarlyStopping(patience=2, verbose=True)\n\nif tokenizer.mask_token is None:\n    # Manually set a mask token if not already defined\n    tokenizer.add_tokens(\"[MASK]\")\n    tokenizer.mask_token = \"[MASK]\"\n    tokenizer.mask_token_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n\nfor epoch in range(pretrain_epochs):\n    running_loss = 0.0\n    for inputs, attention_mask in pretrain_loader:\n        inputs = inputs.to(device)\n        attention_mask = attention_mask.to(device)\n        batch_size, seq_length = inputs.shape\n        \n        # Create a mask for randomly selected tokens\n        mask = torch.rand(inputs.shape) < masking_prob\n        \n        # Randomly replace selected tokens with [MASK] token\n        masked_inputs = inputs.clone()\n        masked_inputs[mask] = tokenizer.mask_token_id\n        \n        pretrain_optimizer.zero_grad()\n        outputs = model(input_ids=masked_inputs, attention_mask=attention_mask, decoder_input_ids=inputs)\n        \n        # Compute the loss only for the masked tokens\n        masked_logits = outputs.logits[mask]\n        masked_labels = inputs[mask]\n        loss = pretrain_criterion(masked_logits.view(-1, masked_logits.size(-1)), masked_labels.view(-1))\n        \n        loss.backward()\n        pretrain_optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    \n    epoch_loss = running_loss / len(pretrain_data)\n    print(f\"Pretrain Epoch {epoch+1}/{pretrain_epochs} - loss: {epoch_loss:.4f}\")\n    \n    # Validation\n    model.eval()\n    val_running_loss = 0.0\n    for val_inputs, val_attention_mask in val_loader:\n        val_inputs = val_inputs.to(device)\n        val_attention_mask = val_attention_mask.to(device)\n        batch_size, seq_length = val_inputs.shape\n\n        mask = torch.rand(val_inputs.shape) < masking_prob\n        masked_inputs = val_inputs.clone()\n        masked_inputs[mask] = tokenizer.mask_token_id\n\n        with torch.no_grad():\n            outputs = model(input_ids=masked_inputs, attention_mask=val_attention_mask, decoder_input_ids=val_inputs)\n            masked_logits = outputs.logits[mask]\n            masked_labels = val_inputs[mask]\n            val_loss = pretrain_criterion(masked_logits.view(-1, masked_logits.size(-1)), masked_labels.view(-1))\n\n        val_running_loss += val_loss.item() * val_inputs.size(0)\n\n    epoch_val_loss = val_running_loss / len(val_data)\n    print(f\"Val Epoch {epoch+1}/{pretrain_epochs} - loss: {epoch_val_loss:.4f}\")\n\n    early_stopping(epoch_val_loss, model)\n\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:00:34.626607Z","iopub.execute_input":"2023-07-08T20:00:34.627517Z","iopub.status.idle":"2023-07-08T20:13:43.201879Z","shell.execute_reply.started":"2023-07-08T20:00:34.627483Z","shell.execute_reply":"2023-07-08T20:13:43.200854Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Pretrain Epoch 1/5 - loss: 2.0259\nVal Epoch 1/5 - loss: 0.3217\nValidation loss decreased (inf --> 0.321702).  Saving model ...\nPretrain Epoch 2/5 - loss: 0.1742\nVal Epoch 2/5 - loss: 0.0906\nValidation loss decreased (0.321702 --> 0.090591).  Saving model ...\nPretrain Epoch 3/5 - loss: 0.0660\nVal Epoch 3/5 - loss: 0.0527\nValidation loss decreased (0.090591 --> 0.052695).  Saving model ...\nPretrain Epoch 4/5 - loss: 0.0343\nVal Epoch 4/5 - loss: 0.0311\nValidation loss decreased (0.052695 --> 0.031082).  Saving model ...\nPretrain Epoch 5/5 - loss: 0.0254\nVal Epoch 5/5 - loss: 0.0254\nValidation loss decreased (0.031082 --> 0.025352).  Saving model ...\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/checkpoint.pt'))","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:14:54.440207Z","iopub.execute_input":"2023-07-08T20:14:54.440582Z","iopub.status.idle":"2023-07-08T20:14:54.713485Z","shell.execute_reply.started":"2023-07-08T20:14:54.440552Z","shell.execute_reply":"2023-07-08T20:14:54.712452Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"For LMA, we first fine-tuned the PLMs on a small amount of task-specific data using a masked language modeling objective. This involves randomly masking some tokens in the input sequence and training the model to predict the masked tokens based on the context provided by the unmasked tokens. This process helps to adapt the PLM to the specific characteristics of the target task and improve its performance on that task.","metadata":{}},{"cell_type":"markdown","source":"# Finetuning","metadata":{}},{"cell_type":"code","source":"# set up the data loader\ntrain_data = WebNLGDataset(dataset)\nbatch_size = 32 #16\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n\nval_data = load_dataset('web_nlg', 'release_v3.0_en')['dev']\nval_data = WebNLGDataset(val_data)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)","metadata":{"id":"pOfRTwjZ9Mla","execution":{"iopub.status.busy":"2023-07-08T20:14:27.781375Z","iopub.execute_input":"2023-07-08T20:14:27.781771Z","iopub.status.idle":"2023-07-08T20:14:28.547962Z","shell.execute_reply.started":"2023-07-08T20:14:27.781740Z","shell.execute_reply":"2023-07-08T20:14:28.546903Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56fdff34779a42fb909de02fb9a68a25"}},"metadata":{}}]},{"cell_type":"code","source":"# set up the optimizer and the loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) #3e-5\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\nearly_stopping = EarlyStopping(patience=2, verbose=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:14:28.960732Z","iopub.execute_input":"2023-07-08T20:14:28.961175Z","iopub.status.idle":"2023-07-08T20:14:28.974083Z","shell.execute_reply.started":"2023-07-08T20:14:28.961135Z","shell.execute_reply":"2023-07-08T20:14:28.973123Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, targets in train_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs, labels=targets)\n        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    epoch_loss = running_loss / len(train_data)\n    print(f\"Train loss: {epoch_loss:.4f}\")\n    \n    model.eval()\n    running_val_loss = 0.0\n    with torch.no_grad():\n        for val_inputs, val_targets in val_loader:\n            val_inputs = val_inputs.to(device)\n            val_targets = val_targets.to(device)\n            val_outputs = model(val_inputs, labels=val_targets)\n            val_loss = criterion(val_outputs.logits.view(-1, val_outputs.logits.size(-1)), val_targets.view(-1))\n            running_val_loss += val_loss.item() * val_inputs.size(0)\n    epoch_val_loss = running_val_loss / len(val_data)\n    print(f\"Val loss: {epoch_val_loss:.4f}\")\n    \n    early_stopping(epoch_val_loss, model)\n    \n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LYG15tgj9Mla","outputId":"0e9adabd-0a23-4320-db12-8890b1a83361","execution":{"iopub.status.busy":"2023-07-08T20:14:59.819096Z","iopub.execute_input":"2023-07-08T20:14:59.819494Z","iopub.status.idle":"2023-07-08T20:32:54.875283Z","shell.execute_reply.started":"2023-07-08T20:14:59.819461Z","shell.execute_reply":"2023-07-08T20:32:54.873930Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Train loss: 1.1169\nVal loss: 0.7848\nValidation loss decreased (inf --> 0.784807).  Saving model ...\nTrain loss: 0.9152\nVal loss: 0.7387\nValidation loss decreased (0.784807 --> 0.738684).  Saving model ...\nTrain loss: 0.8498\nVal loss: 0.7164\nValidation loss decreased (0.738684 --> 0.716376).  Saving model ...\nTrain loss: 0.8016\nVal loss: 0.6964\nValidation loss decreased (0.716376 --> 0.696396).  Saving model ...\nTrain loss: 0.7691\nVal loss: 0.6802\nValidation loss decreased (0.696396 --> 0.680212).  Saving model ...\n","output_type":"stream"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/checkpoint.pt'))","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the entire model\ntorch.save(model, 'model_T5_flan_small_2020_v3')\nprint(\"Model saved successfully.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:34:49.312822Z","iopub.execute_input":"2023-07-08T20:34:49.313193Z","iopub.status.idle":"2023-07-08T20:34:49.874623Z","shell.execute_reply.started":"2023-07-08T20:34:49.313161Z","shell.execute_reply":"2023-07-08T20:34:49.873619Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model saved successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the model\n#model = torch.load('/kaggle/input/models/model_T5_flan_small_multi')\n\n# Print a confirmation message\nprint(\"Model loaded successfully.\")","metadata":{"execution":{"iopub.status.busy":"2023-07-05T14:36:05.231909Z","iopub.execute_input":"2023-07-05T14:36:05.232475Z","iopub.status.idle":"2023-07-05T14:36:07.655113Z","shell.execute_reply.started":"2023-07-05T14:36:05.232431Z","shell.execute_reply":"2023-07-05T14:36:07.654022Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model loaded successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## are we accounting for the multiple texts targets in the bleu? it doesn't look like it","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu\nbatch_size=32","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:34:58.640932Z","iopub.execute_input":"2023-07-08T20:34:58.641295Z","iopub.status.idle":"2023-07-08T20:35:11.687660Z","shell.execute_reply.started":"2023-07-08T20:34:58.641262Z","shell.execute_reply":"2023-07-08T20:35:11.686383Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting sacrebleu\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.5.5)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.2)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.7.0 sacrebleu-2.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from sacrebleu import corpus_bleu\nfrom random import sample\nfrom tqdm import tqdm\n\n\n# load the WebNLG validation dataset\nvalidation_dataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\nvalidation_dataset = [sample for sample in validation_dataset if sample['lex']['text']] # filter out samples with empty targets \n# Select a subset of the validation dataset\n#subset_size = 10  # Choose the desired subset size\n#validation_subset = sample(list(validation_dataset), subset_size)\nvalidation_data = WebNLGDataset(validation_dataset)\n\n# set up the validation data loader\nvalidation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n\n# switch model to evaluation mode\nmodel.eval()\n\n# generate predictions for the validation dataset\npredictions = []\nreferences = []\nwith torch.no_grad():\n    for inputs, targets in tqdm(validation_loader, desc='Validation Progress', leave=False):\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        outputs = model.module.generate(inputs, max_length=MAX_TARGET_LENGTH, num_beams=4)\n        # convert token IDs to strings\n        predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        target_texts = tokenizer.batch_decode(targets, skip_special_tokens=True)\n        # append predicted and target texts for BLEU evaluation\n        predictions.extend(predicted_texts)\n        references.extend(target_texts)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:35:31.620290Z","iopub.execute_input":"2023-07-08T20:35:31.621342Z","iopub.status.idle":"2023-07-08T20:41:01.066866Z","shell.execute_reply.started":"2023-07-08T20:35:31.621305Z","shell.execute_reply":"2023-07-08T20:41:01.065848Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe38f7a94a4f4524a7d3b94d95ed9597"}},"metadata":{}},{"name":"stderr","text":"Validation Progress:   0%|          | 0/123 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\n                                                                      \r","output_type":"stream"}]},{"cell_type":"code","source":"# Writing predictions to a .txt file\nwith open(\"predictions\", \"w\") as f:\n    for prediction in predictions:\n        f.write(prediction + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:41:01.068931Z","iopub.execute_input":"2023-07-08T20:41:01.070542Z","iopub.status.idle":"2023-07-08T20:41:01.079289Z","shell.execute_reply.started":"2023-07-08T20:41:01.070504Z","shell.execute_reply":"2023-07-08T20:41:01.078452Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# calculate BLEU scores\n#bleu = corpus_bleu(predictions, [references])\n\nmultiple_references = []\nfor i in range(len(validation_dataset)):\n    multiple_references.append(validation_dataset[i]['lex']['text'])\n    \nbleu = corpus_bleu(predictions, references)\nbleu_multiple = corpus_bleu(predictions, multiple_references)\n\nprint(f\"BLEU score: {bleu.score}\")\nprint(f\"BLEU score with multiple references: {bleu_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:41:01.080691Z","iopub.execute_input":"2023-07-08T20:41:01.081587Z","iopub.status.idle":"2023-07-08T20:41:02.550649Z","shell.execute_reply.started":"2023-07-08T20:41:01.081552Z","shell.execute_reply":"2023-07-08T20:41:02.549654Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"BLEU score: 0.14943395343706725\nBLEU score with multiple references: 86.03213037538669\n","output_type":"stream"}]},{"cell_type":"code","source":"# Getting the maximum length of the sublists in multiple_references\nmax_length = max(len(sublist) for sublist in multiple_references)\n\n# Writing multiple_references to separate .txt files\nfor i in range(max_length):\n    with open(f\"references{i}\", \"w\") as f:\n        for ref_list in multiple_references:\n            # Writing the ith element if it exists, otherwise an empty line\n            if i < len(ref_list):\n                f.write(ref_list[i] + \"\\n\")\n            else:\n                f.write(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:41:02.553207Z","iopub.execute_input":"2023-07-08T20:41:02.554006Z","iopub.status.idle":"2023-07-08T20:41:02.573735Z","shell.execute_reply.started":"2023-07-08T20:41:02.553967Z","shell.execute_reply":"2023-07-08T20:41:02.572854Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# calculate BLEU scores\n#bleu = corpus_bleu(predictions, [references])\n\nmultiple_references = []\nfor i in range(len(validation_dataset)):\n    multiple_references.append(validation_dataset[i]['lex']['text'])\n    \n# First, determine the maximum length of sublists\nmax_len = max(len(refs) for refs in multiple_references)\n\n# Then pad all sublists to that length\npadded_references = [refs * (max_len // len(refs)) + refs[:max_len % len(refs)] for refs in multiple_references]\n    \nbleu = corpus_bleu(predictions, references)\nbleu_multiple = corpus_bleu(predictions, padded_references)\n\nprint(f\"BLEU score: {bleu.score}\")\nprint(f\"BLEU score with padded references: {bleu_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:39:08.784027Z","iopub.execute_input":"2023-07-05T00:39:08.784420Z","iopub.status.idle":"2023-07-05T00:39:12.491724Z","shell.execute_reply.started":"2023-07-05T00:39:08.784371Z","shell.execute_reply":"2023-07-05T00:39:12.489939Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"BLEU score: 0.16214501382472107\nBLEU score with padded references: 80.7204465338761\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\n\nmetric = load_metric('sacrebleu')\n\n# First, determine the maximum length of sublists\nmax_len = max(len(refs) for refs in multiple_references)\n\n# Then pad all sublists to that length\npadded_references = [refs * (max_len // len(refs)) + refs[:max_len % len(refs)] for refs in multiple_references]\n\n# Now 'padded_references' is a list of lists, where each sublist has the same length.\n# We can now compute the SacreBLEU score.\n\n# Note the change in the compute line\nscore = metric.compute(predictions=predictions, references = padded_references)\n\nprint(f\"SacreBLEU score: {score['score']}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:39:29.889557Z","iopub.execute_input":"2023-07-05T00:39:29.889918Z","iopub.status.idle":"2023-07-05T00:39:34.151307Z","shell.execute_reply.started":"2023-07-05T00:39:29.889887Z","shell.execute_reply":"2023-07-05T00:39:34.149323Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3977d215683d4a51941b587a2cad1d1b"}},"metadata":{}},{"name":"stdout","text":"SacreBLEU score: 31.99615265661011\n","output_type":"stream"}]},{"cell_type":"code","source":"from sacrebleu import corpus_chrf\n# Calculate CHR F++ scores\nchrf = corpus_chrf(predictions, [references])\nchrf_multiple = corpus_chrf(predictions, multiple_references)\nprint(f\"CHR F++ score: {chrf.score}\")\nprint(f\"CHR F++ score with multiple references: {chrf_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:39:46.532954Z","iopub.execute_input":"2023-07-05T00:39:46.533322Z","iopub.status.idle":"2023-07-05T00:39:50.531546Z","shell.execute_reply.started":"2023-07-05T00:39:46.533289Z","shell.execute_reply":"2023-07-05T00:39:50.530539Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"CHR F++ score: 54.00287217727219\nCHR F++ score with multiple references: 77.50627157244912\n","output_type":"stream"}]},{"cell_type":"code","source":"from sacrebleu import corpus_chrf\n# Calculate CHR F++ scores\nchrf = corpus_chrf(predictions, [references])\nchrf_multiple = corpus_chrf(predictions, padded_references)\nprint(f\"CHR F++ score: {chrf.score}\")\nprint(f\"CHR F++ score with multiple references: {chrf_multiple.score}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:40:05.832831Z","iopub.execute_input":"2023-07-05T00:40:05.833194Z","iopub.status.idle":"2023-07-05T00:40:16.362275Z","shell.execute_reply.started":"2023-07-05T00:40:05.833163Z","shell.execute_reply":"2023-07-05T00:40:16.361205Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"CHR F++ score: 54.00287217727219\nCHR F++ score with multiple references: 70.48380146771281\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bert_score","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:48:24.676609Z","iopub.execute_input":"2023-07-08T20:48:24.677603Z","iopub.status.idle":"2023-07-08T20:48:37.431189Z","shell.execute_reply.started":"2023-07-08T20:48:24.677564Z","shell.execute_reply":"2023-07-08T20:48:37.429969Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.0.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.5.3)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.30.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.28.2)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.64.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.15.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.5.5)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.3.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.0.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.39.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2023.5.7)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.6.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nInstalling collected packages: bert_score\nSuccessfully installed bert_score-0.3.13\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"### from datasets import load_metric\nimport numpy as np\n\n\nmetric = load_metric('bertscore')\n\nassert len(predictions) == len(references), \"The number of predictions and references should be the same.\"\n\n# Compute the score\nscore = metric.compute(predictions=predictions, references=references, lang='en')\n\nprint(f\"BERTScore: {np.mean(score['precision'])}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:50:19.189171Z","iopub.execute_input":"2023-07-08T20:50:19.189685Z","iopub.status.idle":"2023-07-08T20:51:05.769717Z","shell.execute_reply.started":"2023-07-08T20:50:19.189650Z","shell.execute_reply":"2023-07-08T20:51:05.768505Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"BERTScore: 0.9178577929841242\n","output_type":"stream"}]},{"cell_type":"code","source":"i=5\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:50:29.291362Z","iopub.execute_input":"2023-07-05T17:50:29.291743Z","iopub.status.idle":"2023-07-05T17:50:29.297731Z","shell.execute_reply.started":"2023-07-05T17:50:29.291710Z","shell.execute_reply":"2023-07-05T17:50:29.296448Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"[['Bootleg_Series_Volume_1:_The_Quine_Tapes | previousWork | Squeeze_(The_Velvet_Underground_album)', 'Squeeze_(The_Velvet_Underground_album) | subsequentWork | 1969:_The_Velvet_Underground_Live']]\n----\nThe Bootleg Series Volume 1: The Queen Teapes was preceded by Squeeze, which was followed by 1969: The Velvet Underground Live.\n----\n['The album 1969: The Velvet Underground Live is preceded by the Velvet Underground album Squeeze, which was followed by The Quine Tapes.', 'The Velvet Underground album Bootleg Series Volume 1: The Quine Tapes was preceded by the album Squeeze, which was followed by the live album 1969: The Velvet Underground Live.', 'The Bootleg Series Volume I: The Quine Tapes is preceded by the Velvet Underground album Squeeze which was itself followed by the album 1969: The Velvet Underground Live.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=10\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:50:37.461245Z","iopub.execute_input":"2023-07-05T17:50:37.461789Z","iopub.status.idle":"2023-07-05T17:50:37.469987Z","shell.execute_reply.started":"2023-07-05T17:50:37.461646Z","shell.execute_reply":"2023-07-05T17:50:37.468844Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"[['Piotr_Hallmann | birthDate | 1987-01-01'], ['Piotr_Hallmann | birthDate | 1987-08-25']]\n----\nPiotr Hallmann was born on the 25th of July 1987.\n----\n['Piotr Hallmann was born on August 25, 1987.', \"Piotr Hallmann's birthday is August 25th 1987.\", 'Piotr Hallmann was born on the 25th of August 1987.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=50\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:50:40.514816Z","iopub.execute_input":"2023-07-05T17:50:40.515175Z","iopub.status.idle":"2023-07-05T17:50:40.521377Z","shell.execute_reply.started":"2023-07-05T17:50:40.515146Z","shell.execute_reply":"2023-07-05T17:50:40.520402Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"[['Alan_Shepard | birthDate | \"1923-11-18\"^^xsd:date', 'Alan_Shepard | deathPlace | California', 'Alan_Shepard | birthPlace | New_Hampshire', 'Alan_Shepard | mission | Apollo_14']]\n----\nAlan Shepard was born in New Hampshire on November 18th, 1923 and died in California. He was a crew member of Apollo 14 and was a member of the Apollo 14 crew.\n----\n['Alan Shepard was a crew member of Apollo 14 who was born November 18th, 1923, in New Hampshire and died in California.', 'Alan Shepard was born in New Hampshire on November 18, 1923. He was a crew member of Apollo 14, and died later on in California.', 'Alan Shepard was born on Nov 18, 1923 in New Hampshire, was a member of the Apollo 14 crew and died in California.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=0\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:50:50.565289Z","iopub.execute_input":"2023-07-05T17:50:50.565680Z","iopub.status.idle":"2023-07-05T17:50:50.572119Z","shell.execute_reply.started":"2023-07-05T17:50:50.565636Z","shell.execute_reply":"2023-07-05T17:50:50.570823Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"[['Estádio_Municipal_Coaracy_da_Mata_Fonseca | location | Arapiraca', 'Agremiação_Sportiva_Arapiraquense | league | Campeonato_Brasileiro_Série_C', 'Campeonato_Brasileiro_Série_C | country | Brazil', 'Agremiação_Sportiva_Arapiraquense | nickname | \"\\'\\'Alvinegro\"@en', 'Agremiação_Sportiva_Arapiraquense | ground | Estádio_Municipal_Coaracy_da_Mata_Fonseca']]\n----\nAgremiaço Sportiva Arapiraquense play in the Campeonato Brasileiro Série C league in Brazil. They play in the Campeonato Brasileiro Série C league.\n----\n['Estádio Municipal Coaracy da Mata Fonseca is the name of the ground of Agremiação Sportiva Arapiraquense in Arapiraca. Agremiação Sportiva Arapiraquense, nicknamed \"Alvinegro\", lay in the Campeonato Brasileiro Série C league from Brazil.', 'Estádio Municipal Coaracy da Mata Fonseca is the name of the ground of Agremiação Sportiva Arapiraquense in Arapiraca. Alvinegro, the nickname of Agremiação Sportiva Arapiraquense, play in the Campeonato Brasileiro Série C league from Brazil.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=70\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:50:59.102751Z","iopub.execute_input":"2023-07-05T17:50:59.103121Z","iopub.status.idle":"2023-07-05T17:50:59.109365Z","shell.execute_reply.started":"2023-07-05T17:50:59.103092Z","shell.execute_reply":"2023-07-05T17:50:59.108233Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"[['Pontiac_Rageous | assembly | Michigan', 'Pontiac_Rageous | assembly | Detroit', 'Pontiac_Rageous | productionEndYear | 1997-01-01']]\n----\nPontiac Rigous was assembled in Detroit and finished its production on January 1st, 1997.\n----\n['The Pontiac Rageous assembled in Michigan with assembly line in Detroit was last produced in 1997.', 'Ending its production in 1997, the Pontiac Rageous was assembled in Detroit, Michigan.', 'Ending in 1997, the Pontiac Rageous was assembled in Detroit, Michigan.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=130\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:51:25.819632Z","iopub.execute_input":"2023-07-05T17:51:25.820536Z","iopub.status.idle":"2023-07-05T17:51:25.826936Z","shell.execute_reply.started":"2023-07-05T17:51:25.820493Z","shell.execute_reply":"2023-07-05T17:51:25.825726Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[['McVeagh_of_the_South_Seas | director | Harry_Carey_(actor_born_1878)', 'McVeagh_of_the_South_Seas | writer | Harry_Carey_(actor_born_1878)', 'McVeagh_of_the_South_Seas | producer | The_Progressive_Motion_Picture_Company']]\n----\nThe film McVeagh of the South Seas was produced by The Progressive Movement Picture Company and was produced by The Progressive Movement Company. The director of McVeagh of the South Seas is Harry Carey.\n----\n['\"McVeagh of the South Seas\" was written and directed by Harry Carey born in 1878 and produced by Progressive Motion Picture Company.', 'Harry Carey, an actor born in 1878, was the director and writer for the film McVeagh of the South Seas which was produced by the Progressive Motion Picture Company.', 'Harry Carey, an actor and director, was born in 1878. He was the writer of the movie McVeagh of the South Seas, which was produced by the Progressive Motion Picture Company. As well as writing the film script, Carey also acted a role in the movie, and directed it.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=1861\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:42:43.521556Z","iopub.execute_input":"2023-07-05T00:42:43.521961Z","iopub.status.idle":"2023-07-05T00:42:43.529086Z","shell.execute_reply.started":"2023-07-05T00:42:43.521928Z","shell.execute_reply":"2023-07-05T00:42:43.527978Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"[['Akeem_Ayers | currentteam | \"Los Angeles Rams\"@en', 'Akeem_Ayers | debutTeam | Tennessee_Titans']]\n----\nAkeem Ayers' debut team was the Tennessee Titans and he currently plays for the Los Angeles Rams.\n----\n['Akeem Ayers made his debut for the Tennessee Titans and currently plays for the Los Angeles Rams.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=1860\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T17:51:45.914174Z","iopub.execute_input":"2023-07-05T17:51:45.914885Z","iopub.status.idle":"2023-07-05T17:51:45.920919Z","shell.execute_reply.started":"2023-07-05T17:51:45.914853Z","shell.execute_reply":"2023-07-05T17:51:45.919685Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"[['Turn_Me_On_(album) | runtime | 2106.0', 'Turn_Me_On_(album) | artist | The_Honeymoon_Killers_(American_band)', 'Turn_Me_On_(album) | genre | Punk_blues', 'Turn_Me_On_(album) | producer | The_Honeymoon_Killers_(American_band)', 'Turn_Me_On_(album) | genre | Noise_rock', 'Turn_Me_On_(album) | previousWork | Let_It_Breed']]\n----\nThe Honeymoon Killers and the American band The Honeymoon Killers are a musical genre of Punk blues. It was produced by The Honeymoon Killers and has a runtime of 2106.0.\n----\n['Turn Me On by the Honeymoon Killers is a punk blues album in the noise rock genre. The run time is 35.1 minutes and was preceded by the Let it Breed album.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=-10\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-05T00:48:38.811970Z","iopub.execute_input":"2023-07-05T00:48:38.812348Z","iopub.status.idle":"2023-07-05T00:48:38.818656Z","shell.execute_reply.started":"2023-07-05T00:48:38.812316Z","shell.execute_reply":"2023-07-05T00:48:38.817729Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"[['McVeagh_of_the_South_Seas | imdbId | 0004319', 'McVeagh_of_the_South_Seas | director | Cyril_Bruce', 'McVeagh_of_the_South_Seas | director | Harry_Carey_(actor_born_1878)', 'McVeagh_of_the_South_Seas | starring | Harry_Carey_(actor_born_1878)', 'McVeagh_of_the_South_Seas | writer | Harry_Carey_(actor_born_1878)', 'McVeagh_of_the_South_Seas | producer | The_Progressive_Motion_Picture_Company']]\n----\nThe film McVeagh of the South Seas stars Harry Carey, Cyril Bruce and Cyril Bruce. The director of McVeagh of the South Seas is Cyril Bruce and the imdb ID is 0004319.\n----\n['Actor Harry Carey, born in 1878, and Cyril Bruce directed McVeagh of the South Seas which was registered in IMDb with the ID 0004319. Harry Carey also wrote and acted in the movie which was produced by the Progressive Motion Picture Company.']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=-9\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:51:05.779607Z","iopub.execute_input":"2023-07-08T20:51:05.780582Z","iopub.status.idle":"2023-07-08T20:51:05.791869Z","shell.execute_reply.started":"2023-07-08T20:51:05.780549Z","shell.execute_reply":"2023-07-08T20:51:05.790951Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[['Mermaid_(Train_song) | recordLabel | Columbia_Records', 'Mermaid_(Train_song) | recordLabel | Sony_Music_Entertainment', \"Mermaid_(Train_song) | precededBy | This'll_Be_My_Year\", 'Mermaid_(Train_song) | runtime | 3.16', 'Mermaid_(Train_song) | genre | Reggae']]\n----\nMermaid (Train Song) was preceded by This'll Be My Year and was recorded by Columbia Records and Sony Music Entertainment. It has a runtime of 3.16 minutes.\n----\n['The Train song Mermaid is a 3.16 minute song in the reggae genre which is on Columbia Records. It had been released under the Sony Music Entertainment label and was preceded by \"This\\'ll Be my Year.\"']\n","output_type":"stream"}]},{"cell_type":"code","source":"i=-5\nprint(validation_dataset[i]['modified_triple_sets']['mtriple_set'])\nprint('----')\nprint(predictions[i])\nprint('----')\nprint(multiple_references[i])","metadata":{"execution":{"iopub.status.busy":"2023-07-08T20:51:05.772090Z","iopub.execute_input":"2023-07-08T20:51:05.772487Z","iopub.status.idle":"2023-07-08T20:51:05.778378Z","shell.execute_reply.started":"2023-07-08T20:51:05.772442Z","shell.execute_reply":"2023-07-08T20:51:05.777439Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[['Trane | type | Subsidiary', 'Trane | foundingDate | 1913-01-01', 'Trane | location | Ireland', 'Trane | foundationPlace | La_Crosse,_Wisconsin', 'Trane | numberOfEmployees | 29000', 'Trane | product | HVAC', 'Trane | industry | Building_materials']]\n----\nTrane, a company in the building materials industry, was founded in La Crosse, Wisconsin on January 1, 1913. It is a subsidiary of Trane, a company in the building materials industry. Trane was founded on January 1, 1913 in La Crosse, Wisconsin. Trane is a subsidiary in the building materials industry.\n----\n['Trane is a subsidiary company that was founded in La Crosse, Wisconsin on 1913-01-01 but later moved to Ireland. Their 29,000 employees produce building materials, prominent among which are HVAC products.']\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5Tokenizer\nfrom datasets import load_dataset\n\n# Define the tokenizer\n#tokenizer = T5Tokenizer.from_pretrained('t5-small')\n\n# Load the WebNLG dataset\ndataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\ndataset = [sample for sample in dataset if sample['lex']['text']]\n\n# Create an instance of WebNLGDataset\nwebnlg_dataset = WebNLGDataset(dataset)\n\n# Define the index of the example you want to test\nexample_index = 70\n\n# Get the input and target texts for the example at the specified index\ninput_text, target_text = webnlg_dataset[example_index]\n\n# Decode the input and target texts using the tokenizer\ndecoded_input_text = tokenizer.decode(input_text, skip_special_tokens=True)\ndecoded_target_text = tokenizer.decode(target_text, skip_special_tokens=True)\n\n# Print the preprocessed input and target texts\nprint(\"Input Text:\", decoded_input_text)\nprint(\"Target Text:\", decoded_target_text)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T18:36:30.294773Z","iopub.execute_input":"2023-05-19T18:36:30.295189Z","iopub.status.idle":"2023-05-19T18:36:31.861708Z","shell.execute_reply.started":"2023-05-19T18:36:30.295152Z","shell.execute_reply":"2023-05-19T18:36:31.860533Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b598640a8aaf4538bace35a707a4555e"}},"metadata":{}},{"name":"stdout","text":"{'category': 'MeanOfTransportation', 'size': 1, 'eid': 'Id71', 'original_triple_sets': {'otriple_set': [['Alfa_Romeo_164 | relatedMeanOfTransportation | Lancia_Thema'], ['Alfa_Romeo_164 | related | Lancia_Thema']]}, 'modified_triple_sets': {'mtriple_set': [['Alfa_Romeo_164 | relatedMeanOfTransportation | Lancia_Thema']]}, 'shape': '', 'shape_type': '', 'lex': {'comment': ['good', 'good'], 'lid': ['Id1', 'Id2'], 'text': ['Alfa Romeo 164 and Lancia Thema are related types of transportation.', 'The related transport to the Alfa Romeo 164 is the Lancia Thema.'], 'lang': ['', '']}, 'test_category': 'testdata_unseen_with_lex', 'dbpedia_links': [], 'links': []}\ntranslate from Graph to Text:  <H> Alfa_Romeo_164  <R>  relatedMeanOfTransportation  <T>  Lancia_Thema <H> Alfa_Romeo_164  <R>  related  <T>  Lancia_Thema\nInput Text: translate from Graph to Text: Alfa_Romeo_164 relatedMeanOfTransportation Lancia_Thema Alfa_Romeo_164 related Lancia_Thema\nTarget Text: Alfa Romeo 164 and Lancia Thema are related types of transportation.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## seeing how many empty targets there are in the testing set","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\ncount_empty_text = 0\nfor sample in dataset:\n    if not sample['lex']['text']:\n        count_empty_text += 1\n\nprint(f\"Number of samples with empty 'lex' 'text' field: {count_empty_text}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-18T18:48:18.862931Z","iopub.execute_input":"2023-05-18T18:48:18.863745Z","iopub.status.idle":"2023-05-18T18:48:20.403634Z","shell.execute_reply.started":"2023-05-18T18:48:18.863701Z","shell.execute_reply":"2023-05-18T18:48:20.402308Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af2dc9ec2d9f4cc5a3473a3a50d95272"}},"metadata":{}},{"name":"stdout","text":"Number of samples with empty 'lex' 'text' field: 1862\n","output_type":"stream"}]},{"cell_type":"code","source":"total_samples = len(dataset)\nprint(f\"Total number of samples in the test dataset: {total_samples}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-18T18:48:20.405561Z","iopub.execute_input":"2023-05-18T18:48:20.405994Z","iopub.status.idle":"2023-05-18T18:48:20.412699Z","shell.execute_reply.started":"2023-05-18T18:48:20.405940Z","shell.execute_reply":"2023-05-18T18:48:20.411048Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Total number of samples in the test dataset: 4615\n","output_type":"stream"}]}]}