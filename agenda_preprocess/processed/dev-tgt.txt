we develop a framework based on bayesian model averaging to explain how animals cope with uncertainty about contingencies in classical conditioning experiments . traditional accounts of conditioning # t parameters within a # xed generative model of reinforcer delivery ; uncertainty over the model structure is not considered . we apply the theory to explain the puzzling relationship between second-order conditioning and conditioned inhibition , two similar conditioning regimes that nonetheless result in strongly divergent behavioral outcomes . according to the theory , second-order conditioning results when limited experience leads animals to prefer a simpler world model that produces spurious correlations ; conditioned inhibition results when a more complex model is justi # ed by additional experience .
in this paper we present a novel slanted-plane model which reasons jointly about occlusion boundaries as well as depth . we formulate the problem as one of inference in a hybrid mrf composed of both continuous -lrb- i.e. , slanted 3d planes -rrb- and discrete -lrb- i.e. , occlusion boundaries -rrb- random variables . this allows us to define potentials encoding the ownership of the pixels that compose the boundary between segments , as well as potentials encoding which junctions are physically possible . our slanted-plane model outperforms the state-of-the-art on middlebury high resolution imagery -lsb- 1 -rsb- as well as in the more challenging kitti dataset -lsb- 2 -rsb- , while being more efficient than existing slanted plane mrf methods , taking on average 2 minutes to perform inference on high resolution imagery .
mobile phone data provides rich dynamic information on human activities in social network analysis . in this paper , we represent data from two different modalities of data as a graph and functions defined on the vertex set of the graph . we propose a regularization framework for the joint utilization of these two modalities of data , which enables us to model evolution of social network information and efficiently classify relationships among mobile phone users . simulations based on real world data demonstrate the potential application of our regularization framework in dynamic scenarios , and present competitive results to baseline methods for combining multimodal data in the learning and clustering communities .
in this paper , we present an efficient look-ahead technique based on both the language model look-ahead and the acoustic model look-ahead , for the time-synchronous beam search in the large vocabulary speech recognition . in this so-call stage based look-ahead technique , two predicting processes with different hypothesis evaluating criteria are organized by stages according to the different requirements for pruning the unlikely surviving hypotheses . furthermore , in order to reduce the efforts for distributing the acoustic model look-ahead over the lexical tree more effectively , the lm rank based pruning is integrated with the extension of each new phoneme node . the recognition experiments performed on the 50k-word mandarin dictation task show that a reduction by 10 percents in the search effort in comparison with the standard word-conditioned search using acoustic model look-ahead only , and a reduction of 25 percents in the word error rates in comparison with the search algorithm without any look-ahead can be achieved .
most algorithms for 3d reconstruction from images use cost functions based on ssd , which assume that the surfaces being reconstructed are visible to all cameras . this makes it difficult to reconstruct objects which are partially occluded . recently , researchers working with large camera arrays have shown it is possible to '' see through '' oc-clusions using a technique called synthetic aperture focus-ing . this suggests that we can design alternative cost functions that are robust to occlusions using synthetic apertures . our paper explores this design space . we compare classical shape from stereo with shape from synthetic aperture focus . we also describe two variants of multi-view stereo based on color medians and entropy that increase robustness to oc-clusions . we present an experimental comparison of these cost functions on complex light fields , measuring their accuracy against the amount of occlusion .
voice-rate is an experimental dialog system that makes product and business ratings available to consumers via a toll-free phone number . by calling voice-rate , users can access the ratings of more than one million products , a quarter million local businesses -lrb- restaurants -rrb- , and three thousand national businesses . this paper describes the voice rate system , and solutions to three key technical challenges : robust name-matching , efficient disambiguation , and review synthesis for telephone playback . voice-rate can be accessed by calling 1-877-456-data -lrb- toll-free -rrb- within the u.s.
on-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data , i.e. the movement of the pen , is recorded directly . however , the raw data can be difficult to interpret because each letter is spread over many pen locations . as a consequence , sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms , such as hmms . in this paper we describe a system capable of directly transcribing raw on-line handwriting data . the system consists of a recurrent neural network trained for sequence labelling , combined with a proba-bilistic language model . in experiments on an unconstrained on-line database , we record excellent results using either raw or pre-processed data , well outperforming a state-of-the-art hmm in both cases .
we introduce a novel approach for source number estimation through an adaptive fuzzy c-means clustering . spatial feature vectors are extracted from microphone observations , weighted for reliability and then clustered in a full-band manner using an adaptive variation on the fuzzy c-means . a number of quality measures are combined to produce a weighted sum which is used to find the optimal number of clusters at each iteration of the clustering algorithm . experimental evaluations using real-world recordings from a reverberant room -lrb- rt 60 = 390 ms -rrb- demonstrated encouraging performance in both even-and under-determined conditions .
part of the long lasting cultural heritage of china is the classical ancient chinese poems which follow strict formats and complicated linguistic rules . automatic chinese poetry composition by programs is considered as a challenging problem in computational linguistics and requires high artificial intelligence assistance , and has not been well addressed . in this paper , we formulate the poetry composition task as an optimization problem based on a gener-ative summarization framework under several constraints . given the user specified writing intents , the system retrieves candidate terms out of a large poem corpus , and then orders these terms to fit into poetry formats , satisfying tonal and rhythm requirements . the optimization process under constraints is conducted via iterative term substitutions till convergence , and outputs the subset with the highest utility as the generated poem . for experiments , we perform generation on large datasets of 61,960 classic poems from tang and song dynasty of china . a comprehensive evaluation , using both human judgments and rouge scores , has demonstrated the effectiveness of our proposed approach .
this paper gives an attempt to explore the manifold in the label space for multi-label learning . traditional label space is logical , where no manifold exists . in order to study the label manifold , the label space should be extended to a euclidean space . however , the label man-ifold is not explicitly available from the training examples . fortunately , according to the smoothness assumption that the points close to each other are more likely to share a label , the local topological structure can be shared between the feature manifold and the label manifold . based on this , we propose a novel method called ml 2 , i.e. , multi-label manifold learning , to reconstruct and exploit the label manifold . to our best knowledge , it is one of the first attempts to explore the manifold in the label space in multi-label learning . extensive experiments show that the performance of multi-label learning can be improved significantly with the label manifold .
we consider stochastic motion planning in single-source single-destination robotic relay networks , under a cooperative beam-forming framework . assuming that the communication medium constitutes a spatiotemporal stochastic field , we propose a 2-stage stochastic programming formulation of the problem of specifying the positions of the relays , such that the expected reciprocal of their total beamforming power is maximized . stochastic decision making is made on the basis of random causal csi . recognizing the intractability of the original problem , we propose a lower bound relaxation , resulting to a nontrivial optimization problem with respect to the relay locations , which is equivalent to a small set of simple , tractable subproblems . our 2-stage stochastic programming formulation results in spatial controllers with a predictive character ; at each time slot , the new relay positions should be such that the expected power reciprocal at the next time slot is maximized . quite interestingly , the optimal control policy to the stochastic decision making is purely selective ; under a certain sense , only the best relay should move .
acoustic models have been used in numerous studies over the past thirty years to simulate the percepts elicited by auditory neural prostheses . in these acoustic models , incoming signals are processed the same way as in a cochlear implant speech processor . the percepts that would be caused by electrical stimulation in a real cochlear implant are simulated by modulating the amplitude of either noise bands or sinusoids . despite their practical usefulness these acoustic models have never been convincingly validated . this study presents a tool to conduct such validation using subjects who have a cochlear implant in one ear and have near perfect hearing in the other ear , allowing for the first time a direct perceptual comparison of the output of acoustic models to the stimulation provided by a cochlear implant .
the adequacies of the simulation-based assessment of speech recognition systems under noisy conditions are investigated and discussed . to evaluate the speech recognition systems in various environments , it is desirable to collect the test data uttered in the corresponding environments but it is not realistic since enormous works are required . to conduct evaluations of the speech recognition systems properly , it is promising to simulate evaluation experiments in the target environments as described below : comparatively small test data are collected , and test data of the target environment are generated by computing convolution of the impulse response of the target environment with the collected data . however , it is well known that changes of the acoustic characteristics are caused by lombard effect , and so it is not necessarily obvious whether the simulation can precisely approximate the experiment in actual environment . this paper clarifies the condition to perform effective simulations of the noisy speech recognition , focusing on the influence of impulse response accuracies and lombard effects on the speech recognition performance .
spoken language understanding performs automatic concept labeling and segmentation of speech utterances . for this task , many approaches have been proposed based on both genera-tive and discriminative models . while all these methods have shown remarkable accuracy on manual transcription of spoken utterances , robustness to noisy automatic transcription is still an open issue . in this paper we study algorithms for spoken language understanding combining complementary learning models : stochastic finite state transducers produce a list of hypotheses , which are re-ranked using a discriminative algorithm based on kernel methods . our experiments on two different spoken dialog corpora , media and luna , show that the combined generative-discriminative model reaches the state-of-the-art such as conditional random fields on manual transcriptions , and generative-discriminative model is robust to noisy automatic transcriptions , outperforming , in some cases , the state-of-the-art .
many bayesian learning methods for massive data benefit from working with small subsets of observations . in particular , significant progress has been made in scalable bayesian learning via stochastic approximation . however , bayesian learning methods in distributed computing environments are often problem-or distribution-specific and use ad hoc techniques . we propose a novel general approach to bayesian inference that is scalable and robust to corruption in the data . our technique is based on the idea of splitting the data into several non-overlapping subgroups , evaluating the posterior distribution given each independent subgroup , and then combining the results . our main contribution is the proposed aggregation step which is based on finding the geometric median of subset posterior distributions . presented theoretical and numerical results confirm the advantages of our approach .
in this paper , we consider the problem of dictionary learning for sparse representations . several algorithms dealing with this problem can be found in the literature . one of them , introduced by sezer et al. in -lsb- 1 -rsb- optimizes a dictionary made up of the union of orthonor-mal bases . in this paper , we propose a probabilistic interpretation of sezer 's algorithm and suggest a novel optimization procedure based on the em algorithm . comparisons of the performance in terms of missed detection rate show a clear superiority of the proposed probabilistic interpretation of sezer 's algorithm .
we present a new active contour model in spatio-velocity space which is based o n t h e p r obability data association lter pdaf approach . active contour model employs a directional-based m e asurement model of image potential and of optical-ow along the contour points . the proposed d i r ectional approach of measurements is the basis for the velocity-based discrimination between measurements of the object to that of image clutter . the active contour model chooses the appropriate measurements which are most consistent with previous estimation of motion in the sense of the pdaf approach . in order to obtain reliable measurements of image motion and of gradient-based image potential , we propose a directional smoothing operator which is the basis for discriminating the objects measurements from that of image clutter . the directional smoothing operator was applied t o r eal world tracking problems such as a walking leg and a waving hand .
we propose to shift the goal of recognition from naming to describing . doing so allows us not only to name familiar objects , but also : to report unusual aspects of a familiar object -lrb- '' spotty dog '' , not just '' dog '' -rrb- ; to say something about unfamiliar objects -lrb- '' hairy and four-legged '' , not just '' unknown '' -rrb- ; and to learn how to recognize new objects with few or no visual examples . rather than focusing on identity assignment , we make inferring attributes the core problem of recognition . these attributes can be semantic -lrb- '' spotty '' -rrb- or discriminative -lrb- '' dogs have it but sheep do not '' -rrb- . learning attributes presents a major new challenge : generalization across object categories , not just across instances within a category . in this paper , we also introduce a novel feature selection method for learning attributes that generalize well across categories . we support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework .
demand response -lrb- dr -rrb- allows utilities to curtail electricity consumption during peak demand periods . real time automated dr can offer utilities a scalable solution for fine grained control of curtail-ment over small intervals for the duration of the entire dr event . in this work , we demonstrate a system for a real time automated dynamic dr -lrb- d 2 r -rrb- . our system has already been integrated with the electrical infrastructure of the university of southern california , which offers a unique environment to study the impact of automated dr in a complex social and cultural environment including 170 buildings in a '' city-within-a-city '' scenario . our large scale information processing system coupled with accurate forecasting models for sparse data and fast polynomial time optimization algorithms for curtailment maximization provide the ability to adapt and respond to changing curtailment requirements in near real-time . our d 2 r algorithms automatically and dynamically select customers for load curtailment to guarantee the achievement of a curtailment target over a given dr interval .
grouping cues can affect the performance of segmenta-tion greatly . in this paper , we show that superpixels -lrb- image segments -rrb- can provide powerful grouping cues to guide segmentation , where superpixels can be collected easily by -lrb- over -rrb- - segmenting the image using any reasonable existing segmentation algorithms . generated by different algorithms with varying parameters , superpixels can capture diverse and multi-scale visual patterns of a natural image . successful integration of the cues from a large multitude of su-perpixels presents a promising yet not fully explored direction . in this paper , we propose a novel segmentation framework based on bipartite graph partitioning , which is able to aggregate multi-layer superpixels in a principled and very effective manner . computationally , segmentation framework is tailored to unbalanced bipartite graph structure and leads to a highly efficient , linear-time spectral algorithm . our segmentation framework achieves significantly better performance on the berkeley segmenta-tion database compared to state-of-the-art techniques .
prior research has investigated development of virtual auditory displays using low-dimensional models of head related transfer functions -lrb- hrtfs -rrb- as a function of a finite number of principal components -lrb- pcs -rrb- and associated weights -lrb- virtual auditory displays -rrb- . this paper investigates the effect of virtual auditory displays on horizontal plane hrtfs derived from a database of hrirs through analytical optimization experiments . the experiments investigate whether average hrtfs can be tuned to match individual hrtfs . results provide insight on the effect of tuning pcws on spectral features of the virtual auditory displays . a reduced order modeling technique is used to compactly represent each virtual auditory displays . subject testing results are provided , showing that a human can conduct the tuning procedure and reduce localization errors .
we discuss the problem of ranking instances . in our framework each instance is associated with a rank or a rating , which is an integer from 1 to k . our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance 's true rank . we describe a simple and efficient online algorithm , analyze its performance in the mistake bound model , and prove its correctness . we describe two sets of experiments , with synthetic data and with the eachmovie dataset for collaborative filtering . in the experiments we performed , our algorithm outper-forms online algorithms for regression and classification applied to ranking .
in this paper , we consider the class of adaptive lters with error nonlinearities . in particular , we derive an expression for the optimum nonlinearity that minimizes the steady-state error and attains the limit mandated by the cramer-rao bound of the underlying estimation process .
recently , adaptive subband coders based on wavelet packet decomposition and psychoacoustic modelling have been proposed to achieve transparent quality compression of audio signals -lsb- 1 -rsb- , -lsb- 2 -rsb- . while these adaptive subband coders perform well for stationary signals , there is no special mechanism in the coder to prevent the pre-echo artifact when transient signals are encoded . in this paper , we propose a switched dpcm/subband structure to remove the pre-echo problem . this is achieved through a novel temporally varying bit allocation scheme which is based on the temporal masking properties of the human auditory system . the proposed coder/decoder output is found to be free from the pre-echo artifact even at a lower bitrate than the adaptive subband coder .
in this paper , we present the application of the type ii local discriminant basis -lrb- ldb -rrb- technique to feature extraction for land use classification in synthetic aperture radar images . our classification algorithm incorporates spatial information into the decision process by classifying small image blocks , instead of single pixels . a feature vector composed of all the values in the image blocks is large for even small image blocks and , therefore , degrades the performance of many classi-fiers . the classification algorithm greatly compresses the dimensionality of the feature vector , by indicating the most discriminant coordinates within the wavelet packet decomposition of an image block .
this position paper argues for an interactive approach to text understanding . the proposed interactive approach extends an existing semantics-based text authoring system by using the input text as a source of information to assist the user in re-authoring its content . the interactive approach permits a reliable deep semantic analysis by combining automatic information extraction with a minimal amount of human intervention .
driven by the multi-level structure of human intracranial electroencephalogram -lrb- ieeg -rrb- recordings of epileptic seizures , we introduce a new variant of a hierarchical dirich-let process -- the multi-level clustering hierarchical dirichlet process -- that simultaneously clusters datasets on multiple levels . our hierarchical dirich-let process contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient . the mlc-hdp 's clustering clusters over channels-types , seizure-types , and patient-types simultaneously . we describe this hierarchical dirich-let process and its implementation in detail . we also present the results of a simulation study comparing the mlc-hdp to a similar hierarchical dirich-let process , the hierarchical dirich-let process and finally demonstrate the mlc-hdp 's use in modeling seizures across multiple patients . we find the mlc-hdp 's clustering to be comparable to independent human physician clusterings . to our knowledge , the mlc-hdp 's clustering is the first in the epilepsy literature capable of clustering seizures within and between patients .
neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits . an important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics . here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity . this nonlinear receptive field model captures temporal dynamics and correlations due to shared stimulus drive as well as common noise . moreover , because the nonlinear stimulus inputs are mixed by the ongoing dynamics , the nonlinear receptive field model can account for a multiple idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional dynamical model . we introduce a fast estimation method using online expectation maximization with laplace approximations , for which inference scales linearly in both population size and recording duration . we test this nonlinear receptive field model to multi-channel recordings from primary visual cortex and show that fast estimation method accounts for neural tuning properties as well as cross-neural correlations .
speaker clustering is an important task in many applications such as speaker diarization as well as speech recognition . speaker clustering can be done within a single multi-speaker recording -lrb- diarization -rrb- or for a set of different recordings . in this work we are interested by the former case and we propose a simple iterative mean shift algorithm to deal with this problem . traditionally , iterative mean shift algorithm is based on euclidean distance . we propose to use the cosine distance in order to build a new version of iterative mean shift algorithm . we report results as measured by speaker and cluster impurities on nist sre 2008 datasets .
in this paper , we propose a new combined harmonic - wavelet representation for audio where a harmonic analysis-synthesis scheme is used , rst , to approximate each audio frame as a sum of several sinusoids . then , the dier-ence between the original signal and the reconstructed harmonic signal is analyzed using a wavelet ltering scheme . after each step -lrb- harmonic analysis & wavelet ltering -rrb- , parameters are quantized and encoded . compared to previously proposed methods , our audio coder uses dierent harmonic analysis-synthesis and wavelet ltering schemes . we use the total least squares - prony algorithm for the harmonic analysis-scheme , and an m-band wavelet transform for analyzing the residual . altogether , our proposed audio coder is capable of delivering excellent audio signal quality at encoder bitrates of 60-70 kb/s .
the speaker recognition task falls under the general problem of pattern classification . speaker recognition as a pattern classification problem , its ultimate objective is design of a system that classifies the vector of features in different classes by partitioning the feature space into optimal speaker discriminative space . linear discriminant analysis is a feature extraction method that provides a linear transformation of n-dimensional feature vectors -lrb- or samples -rrb- into m-dimensional space -lrb- m < n -rrb- , so that samples belonging to the same class are close together but samples from different classes are far apart from each other . in this paper we discuss the issue of the application of linear discriminant analysis to our gaussian mixture model -lrb- gmm -rrb- based speaker identification task . applying linear discriminant analysis improved the identification performance .
this study aims to determine whether the production of the lexical variants created by the phonological processes of liaison and schwa deletion in french are conditioned by factors linked to lexical recognition . we hypothesise that the realisation of these variants would be favoured for words which are lexically '' salient '' in term of frequency and in their lexical neighbourhoods . this claim was tested by examining a speech corpus for the effects of lexical frequency , neighbourhood density and neighbourhood frequency on the production of liaison -lrb- both in linking and linked words and their co-occurrence -rrb- and elision . overall the results do not support our hypothesis : lexical frequency and competition do not appear to influence strongly whether liaison and elision are realised or not .
the interpreted system model offers a computationally grounded model , in terms of the states of computer processes , to s5 epistemic logics . this paper extends the interpreted system model , and provides a computationally grounded one , called the interpreted perception system model , to those epis-temic logics other than s5 . it is usually assumed , in the interpreted system model , that those parts of the environment that are visible to an agent are correctly perceived by the agent as a whole . the essential idea of the interpreted perception system model is that an agent may have incorrect perception or observations to the visible parts of the environment and the agent may not be aware of this . the notion of knowledge can be defined so that an agent knows a statement iff the statement holds in those states that the agent can not distinguish -lrb- from the current state -rrb- by using only her correct observations . we establish a logic of knowledge and certainty , called kc logic , with a sound and complete proof system . the knowledge modality in this computationally grounded model is s4 valid . it becomes s5 if we assume an agent always has correct observations ; and more interestingly , it can be s4 .2 or s4 .3 under other natural constraints on agents and their sensors to the environment .
the use of dialogue-state dependent language models in automatic inquiry systems can improve speech recognition and understanding if a reasonable prediction of the dialogue state is feasible . in this paper , the dialogue state is defined as the set of parameters which are contained in the automatic inquiry systems prompt . for each dialogue state a separate language model is constructed . in order to obtain robust language models despite the small amount of training data we propose to interpolate all of the dialogue-state dependent language models linearly for each dialogue state and to train the large number of resulting interpolation weights with the em-algorithm in combination with leaving-one-out . we present experimental results on a small dutch corpus which has been recorded in the netherlands with a train timetable information system and show that the perplexity and the word error rate can be reduced significantly .
we present a method to label an audiovisual database and to setup a system for audiovisual speech recognition based on a hybrid artificial neural network/hidden markov model -lrb- ann/hmm -rrb- approach . the multi-stage labeling process is presented on a new audiovisual database recorded at the institute de la communication parlée . the audiovisual database was generated via transposition of the audio database numbers95 . for the labeling first a large subset of numbers95 is used to achieve a bootstrap training of an institute de la communication parlée , which can then be employed to label the audio part of the audiovisual database . this initial labeling is further improved via readapting the institute de la communication parlée to the new audiovisual database and reperforming the labeling . from the audio labeling then the video labeling is derived . tests at different signal to noise ratios -lrb- snr -rrb- are performed to demonstrate the efficiency of the labeling process . furthermore ways to incorporate information from a large audio database into the final audiovisual recognition system were investigated .
this paper investigates the use of microphone arrays to acquire and recognise speech in meetings . meetings pose several interesting problems for speech processing , as they consist of multiple competing speakers within a small space , typically around a table . due to their ability to provide hands-free acquisition and directional discrimination , microphone arrays present a potential alternative to close-talking microphones in such an application . we first propose an appropriate microphone array geometry and improved processing technique for this microphone arrays , paying particular attention to speaker separation during possible overlap segments . data collection of a small vocabulary speech recognition corpus -lrb- numbers -rrb- was performed in a real meeting room for a single speaker , and several overlapping speech scenarios . in speech recognition experiments on the acquired database , the performance of the microphone array system is compared to that of a close-talking lapel microphone , and a single table-top microphone .
projector-camera systems use computer vision to analyze their surroundings and display feedback directly onto real world objects , as embodied by spatial augmented reality . to be effective , the display must remain aligned even when the target object moves , but the added illumination causes problems for traditional algorithms . current solutions consider the displayed content as interference and largely depend on channels orthogonal to visible light . they can not directly align projector images with real world surfaces , even though this may be the actual goal . we propose instead to model the light emitted by projectors and reflected into cameras , and to consider the displayed content as additional information useful for direct alignment . we implemented in software an algorithm that successfully executes on planar surfaces of diffuse reflectance properties at almost two frames per second with subpixel accuracy . although slow , our work proves the viability of the concept , paving the way for future optimization and generalization .
gabor-based region covariance matrix -lrb- gabor-based region covariance matrix -rrb- is an emerging face feature descriptor , which has been shown promising for face recognition . the gabor-based region covariance matrix lies on tensor manifold is inherently non-euclidean , hence a disconnect exists between grcm descriptor and vector-based classifiers , such as collaborative representation-based classifier . collaborative representation-based classifier is a strong alternative to sparse representation-based classifier yet enjoys high efficiency . in this paper , we bridge gabor-based region covariance matrix and collaborative representation-based classifier with kernel learning method . we investigate several geodesic distances on tensor manifold that satisfy the mercer 's condition for kernel crc construction as well as for speedy computation . apart from that , we also devise two strategies to jointly combine the regionalized grcms with tensor kernel crc . extensive experiments on the orl and feret datasets are conducted to verify the efficacy of the proposed method .
deduction , induction , and analogy pervade all our thinking . in contrast with deduction , understanding logical aspects of induction and analogy is still an important and challenging issue of artificial intelligence . this paper describes a logical formalization , called production , of common conjectural reasoning of both induction and analogy . by introduction of preduction , logical formalization is refined into `` preduction + deduction '' and -lrb- empirical -rrb- inductive reasoning is refined into `` preduction + mathematical induction '' . we examine generality of preduction through applications to various examples on induction and analogy .
we describe an analog-vlsi neural network for face recognition based on subspace methods . the analog-vlsi neural network uses a dimensionality-reduction network whose coefficients can be either programmed or learned on-chip to perform pca , or programmed to perform lda . a second analog-vlsi neural network with user-programmed coefficients performs classification with manhattan distances . the analog-vlsi neural network uses on-chip compensation techniques to reduce the effects of device mismatch . using the orl database with 12x12-pixel images , our analog-vlsi neural network achieves up to 85 % classification performance -lrb- 98 % of an equivalent software implementation -rrb- .
automated essay scoring is one of the most important educational applications of natural language processing . recently , researchers have begun exploring methods of scoring essays with respect to particular dimensions of quality such as coherence , technical errors , and relevance to prompt , but there is relatively little work on modeling organization . we present a new annotated corpus and propose heuristic-based and learning-based approaches to scoring essays along the organization dimension , utilizing techniques that involve sequence alignment , alignment kernels , and string kernels .
in this paper we present an approach to enhance existing maps with fine grained segmentation categories such as parking spots and sidewalk , as well as the number and location of road lanes . towards this goal , we propose an efficient approach that is able to estimate these fine grained categories by doing joint inference over both , monocular aerial imagery , as well as ground images taken from a stereo camera pair mounted on top of a car . important to this is reasoning about the alignment between the two types of imagery , as even when the measurements are taken with sophisticated gps+imu systems , this alignment is not sufficiently accurate . we demonstrate the effectiveness of our approach on a new dataset which enhances kitti -lsb- 8 -rsb- with aerial images taken with a camera mounted on an airplane and flying around the city of karlsruhe , germany .
we present and implement a weighted partial maxsat solver based on successive calls to a sat solver . we prove the cor-rectness of our weighted partial maxsat solver and compare our weighted partial maxsat solver with other weighted partial maxsat solvers .
the geometric median is a classic robust estimator of centrality for data in euclidean spaces . in this paper we formulate the geometric median of data on a riemannian manifold as the minimizer of the sum of geodesic distances to the data points . we prove existence and uniqueness of the geometric median on manifolds with non-positive sectional curvature and give sufficient conditions for uniqueness on positively curved manifolds . generalizing the weiszfeld procedure for finding the geometric median of euclidean data , we present an algorithm for computing the geometric median on an arbitrary manifold . we show that this algorithm converges to the unique solution when it exists . this method produces a robust central point for data lying on a manifold , and should have use in a variety of vision applications involving manifolds . we give examples of the geometric median computation and demonstrate its robustness for three types of manifold data : the 3d rotation group , tensor manifolds , and shape spaces .
the problem of elimination of impulsive disturbances from archive audio signals is considered and its new solution , called predictive matched filtering , is proposed . the new approach is based on the observation that a large percentage of noise pulses corrupting archive audio recordings have highly repetitive shapes that match several typical '' patterns '' , called click templates . to localize noise pulses , click templates can be correlated with the sequence of multi-step-ahead prediction errors yielded by the model-based signal predictor . it is shown that predictive matched filtering is an efficient and com-putationally affordable disturbance localization technique -- when combined with the classical detection method based on autoregres-sive modeling , it can significantly improve restoration results .
in this paper we describe our large vocabulary continuous speech recognition system for the german language , the development of which was partly carried out within the context of the european lre project 62-058 sqale . the large vocabulary continuous speech recognition system is the limsi recognizerr1 -rsb- originally developed for french and american english , which has been adapted to german . speciicities of german , as relevant to the large vocabulary continuous speech recognition system , are presented . these speciicities have been accounted for during the recognizer 's adaptation process . we present experimental results on a rst test set ger-dev95 to measure progress in system development . results are given with the large vocabulary continuous speech recognition system using diierent acoustic model sets on two test sets ger-dev95 and ger-eval95 . this large vocabulary continuous speech recognition system achieved a word error rate of 17.3 % -lrb- oocial word error rate of 16.1 % after sqale adjudication process -rrb- on the ger-eval95 test set .
many computer vision problems have an asymmetric distribution of information between training and test time . in this work , we study the case where we are given additional information about the training data , which however will not be available at test time . this situation is called learning using privileged information . we introduce two maximum-margin techniques that are able to make use of this additional source of information , and we show that the framework is applicable to several scenarios that have been studied in computer vision before . experiments with attributes , bounding boxes , image tags and rationales as additional information in object classification show promising results .
this paper presents a new approach to feature analysis in automatic speech recognition based on locality preserving projections . automatic speech recognition is a manifold based dimensionality reduction algorithm which can be trained and applied as a linear projection to asr features . conventional manifold based dimensionality reduction algorithms are generally restricted to batch mode implementation and automatic speech recognition is difficult in practice to apply them to unseen data . it is argued that automatic speech recognition can model feature vectors that are assumed to lie on a nonlin-ear embedding subspace by preserving local relations among input features , so automatic speech recognition has a potential advantage over conventional linear di-mensionality reduction algorithms like principal components analysis and linear discriminant analysis . experimental results obtained on the resource management data set showed that when lpp based dimensionality reduction was applied in the context of mel frequency cepstrum coefficient -lrb- mfcc -rrb- based feature analysis , a significant reduction of word error rate was obtained with respect to standard mfcc features .
the euclidean algorithm is a frequently used tool in the analysis of one-dimensional multirate systems . this tool is however not available for multidimensional multirate systems . in this paper we discus how groebner basis techniques can ll this gap . after presenting the relevant facts about groebner bases , we will show in a few examples how this technique can contribute to md multirate systems theory .
the exemplar-based approaches , which model signals as a sparse linear combination of exemplars of signals , are proved to have state-of-the-art performance in noise robust asr , especially on low snrs . however , since both the speech exem-plars and noise exemplars are built from training data and are fixed throughout the process of enhancing speech features , the conventional approach is especially weak for unknown types of noise . therefore , in this paper , we propose a semi-supervised approach which automatically adapt noise exem-plars to the target noise , while keeping the speech exemplars fixed . continuous digits recognition experiments show that this approach is much more robust for unknown noise . the recognition errors are reduced by 36.2 % .
this paper examines the role of biological constraints in the human auditory localization process . a psychophysical and neural system modeling approach was undertaken in which performance comparisons between competing models and a human subject explore the relevant biologically plausible `` realism constraints '' . the directional acoustical cues , upon which human auditory localization process is based , were derived from the human subject 's head-related transfer functions -lrb- hrtfs -rrb- . sound stimuli were generated by convolving bandpass noise with the hrtfs and were presented to both the subject and the psychophysical and neural system modeling approach . the input stimuli to the psychophysical and neural system modeling approach was processed using the auditory image model of cochlear processing . the auditory image model of cochlear processing was then analyzed by a time-delay neural network which integrated temporal and spectral information to determine the spatial location of the sound source . the combined psychophysical and neural system modeling approach and neural network provided a system model of the human auditory localization process . human-like localization performance was qualitatively achieved for broadband and bandpass stimuli when the system model incorporated frequency division -lrb- or tonotopicity -rrb- , and was trained using variable bandwidth and center-frequency sounds .
this paper describes a method of improving the quality of the waveform interpolation speech coder by adjustment of the phase information . in waveform interpolation speech coder , a slowly-evolving waveform and a rapidly-evolving waveform represent the periodic and the non-periodic part of the signal . the phase of the synthesized signal is determined by the slowly-evolving waveform and rew , and thus the correct quantiza-tion of these parameters are important to producing natural speech quality . a method is described , whereby the phase of the synthesized signal is adjusted by modifying the quantized rew spectrum as a function of the fundamental frequency . this essentialy attempts to correct the discrepancies in phase that arise due to variation in pitch and also accounts for the difference in noise sensitivity between female and male speech -lsb- 5 -rsb- . the overall effect would be the same if multiple codebooks -lrb- depending on pitch -rrb- were used to code the rew spectrum . experimental results confirm that the new method results in significantly improved performance .
two methods to overcome the problems with large vector quantization codebooks are lattice vq -lrb- l v q -rrb- and product codes . the approach described in this paper takes advantage of both methods by applying residual vq with lattice vq -lrb- l v q -rrb- at all stages . using lattice vq -lrb- l v q -rrb- in conjunction with entropy coding is strongly motivated by the fact that entropy constrained but structurally unconstrained vq design leads to more equally sized vq cells . the entropy code of the rst lvq stage should aim at exploiting the statistical properties of the source . the renement l v q stages quantize the residuals . simulations show that there exist certain scales of the renement lattices yielding extraordinary performance . we focus on the search of these scales .
we present an algorithm that exploits the complimentary benefits of best-first search and depth-first search by performing limited dfs lookaheads from the frontier of bfs . we show that this continuum requires significantly less memory than bfs . in addition , a time speedup is also achieved when choosing the lookahead depth correctly . we demonstrate this idea for breadth-first search and for a * . additionally , we show that when using inconsistent heuristics , bidi-rectional pathmax , can be implemented very easily on top of the lookahead phase . experimental results on several domains demonstrate the benefits of all our ideas .
a new voice conversion method that improves the quality of the voice conversion output at higher sampling rates is proposed . speaker transformation algorithm using segmental codebooks is modified to process source and target speech spectra in different subbands . the new voice conversion method ensures better conversion at sampling rates above 16khz . discrete wavelet transform is employed for subband decomposition to estimate the speech spectrum better with higher resolution . faster voice conversion is achieved since the computational complexity decreases at a lower sampling rate . a voice conversion system is implemented using the proposed voice conversion method with necessary tools . the performance of the proposed voice conversion method is demonstrated by both subjective listening tests and applications to film dubbing and looping . in abx listening tests , the listeners preferred the subband based output by 92.1 % as compared to the full-band based output .
we present a new method to robustly and efficiently analyze foreground when we detect background model for a fixed camera view by using mixture of gaussians models and multiple cues . the background model is modeled by three gaussian mixtures as in the work of stauffer and grimson -lsb- 11 -rsb- . then the intensity and texture information are integrated to remove shadows and to enable the algorithm working for quick lighting changes . for foreground analysis , the same gaussian mixture model is employed to detect the static foreground regions without using any tracking or motion information . then the whole static regions are pushed back to the background model to avoid a common problem in background subtraction -- fragmentation -lrb- one object becomes multiple parts -rrb- . the method was tested on our real time video surveillance system . it is robust and run about 130 fps for color images and 150 fps for grayscale images at size 160x120 on a 2gb pentium iv machine with mmx optimization .
this paper presents a new countermeasure for the protection of automatic speaker verification systems from spoofed , converted voice signals . the new countermeasure is based on the analysis of a sequence of acoustic feature vectors using local binary patterns . compared to existing approaches the new countermeasure is less reliant on prior knowledge and affords robust protection from not only voice conversion , for which it is optimised , but also spoofing attacks from speech synthesis and artificial signals , all of which otherwise provoke significant increases in false acceptance . the work highlights the difficulty in detecting converted voice and also discusses the need for formal evaluations to develop new countermeasures which are less reliant on prior knowledge and thus more reflective of practical use cases .
this study revisits the face-to-tongue articulatory inversion problem in speech . we compare the multi linear regression method with two more sophisticated methods based on hidden markov models and gaussian mixture models , using the same french corpus of articulatory data acquired by electromagnetography . gaussian mixture models give overall results better than hmms , but gaussian mixture models does poorly . gaussian mixture models and hmms maintain the original phonetic class distribution , though with some centralisation effects , effects still much stronger with gaussian mixture models . a detailed analysis shows that , if the jaw / lips / tongue tip synergy helps recovering front high vowels and coronal consonants , the velars are not recovered at all . it is therefore not possible to recover reliably tongue from face .
this work presents an unsupervised learning based approach to the ubiquitous computer vision problem of image matching . we start from the insight that the problem of frame-interpolation implicitly solves for inter-frame correspondences . this permits the application of analysis-by-synthesis : we firstly train and apply a convolutional neural network for frame-interpolation , then obtain correspondences by inverting the learned cnn . the key benefit behind this unsupervised learning based approach is that the cnn for frame-interpolation can be trained in an unsupervised manner by exploiting the temporal coherency that is naturally contained in real-world video sequences . the present unsupervised learning based approach therefore learns image matching by simply '' watching videos '' . besides a promise to be more generally applicable , the presented unsupervised learning based approach achieves surprising performance comparable to traditional empirically designed methods .
a voice activity detector plays a vital role in robust speaker verification , where energy vad is most commonly used . voice activity detector works well in noise-free conditions but deteriorates in noisy conditions . one way to tackle this is to introduce speech enhancement preprocessing . we study an alternative , likelihood ratio based vad that trains speech and nonspeech models on an utterance-by-utterance basis from mel-frequency cepstral coefficients . the training labels are obtained from enhanced energy vad . as the speech and nonspeech models are retrained for each utterance , minimum assumptions of the background noise are made . according to both vad error analysis and speaker verification results utilizing state-of-the-art i-vector system , the proposed likelihood ratio based vad outperforms energy vad variants by a wide margin . we provide open-source implementation of the likelihood ratio based vad .
this paper presents a new methodology for evaluation and design of variable step size adaptive algorithms . the new methodology is based on a learning plane , which combines the evolutions of both the step size and the mean square error . it includes both transient and steady-state behaviors and can be used to compare performances of different algorithms against an optimum trajectory in the learning plane . the new technique can also be used for algorithm optimization in system identification applications .
because classical fast vector quantization -lrb- crvq-cs -lrb- constrained range vector quantization -rrb- algorithms ca n't be used in the lsf vector quantizers that use varying weighted euclidean distance , a novel crvq-cs -lrb- constrained range vector quantization -- crvq-cs -lrb- constrained range vector quantization based on component searching -rrb- is presented in this paper . the crvq-cs -lrb- constrained range vector quantization works well with the varying weighted euclidean distance and yields the same result as full search vq with reduced computational complexity does . although the crvq-cs -lrb- constrained range vector quantization is proposed for crvq-cs -lrb- constrained range vector quantization using varying weighted euclidean distance measure , crvq-cs -lrb- constrained range vector quantization is also suitable for crvq-cs -lrb- constrained range vector quantization using simple euclidean distance measure .
conditional random fields -lrb- conditional random fields -lrb- crfs -rrb- have been popular for contextual pattern classification . this paper presents two variational inference methods for direct approximation of a conditional probability instead of indirect calculation through viterbi approximation of a marginal probability . the conditional random fields -lrb- crfs with the factorized variational inference and the structured variational inference are proposed and investigated for human motion recognition . in general , structured variational inference assumes a factorization of variational distributions of individual states for representation of conditional probability while structured variational inference preserves the state structure in the variational distribution . in the experiments on using idiap human motion database , we found that conditional random fields -lrb- crfs using variation inference methods performed better than baseline crfs using viterbi approximation . conditional random fields -lrb- crfs with structured variational inference obtained higher classification accuracy than those with structured variational inference .
the purpose of this paper is to address the problem of maintaining coherent perceptual information in a mobile robotic system working over extended periods of time , interacting with a user and using multiple sensing modalities to gather information about the environment and specific objects . we present a mobile robotic system which is able to use spatial and olfactory sensors to patrol a corridor and execute user requested tasks . to cope with perceptual maintenance we present an extension of the anchoring framework capable of maintaining the correspondence between sensor data and the symbolic descriptions referring to objects . mobile robotic system is also capable of tracking and acquiring information from observations derived from sensor-data as well as information from a priori symbolic concepts . the general mobile robotic system is described and an experimental validation on a mobile robotic system is presented .
numerous recent approaches attempt to remove image blur due to camera shake , either with one or multiple input images , by explicitly solving an inverse and inherently ill-posed deconvolution problem . if the photographer takes a burst of images , a modality available in virtually all modern digital cameras , we show that it is possible to combine them to get a clean sharp version . this is done without explicitly solving any blur estimation and subsequent inverse problem . the proposed algorithm is strikingly simple : it performs a weighted average in the fourier domain , with weights depending on the fourier spectrum magnitude . the method 's rationale is that camera shake has a random nature and therefore each image in the burst is generally blurred differently . experiments with real camera data show that the proposed fourier burst accumulation algorithm achieves state-of-the-art results an order of magnitude faster , with simplicity for on-board implementation on camera phones .
we explore the applicability of machine translation evaluation methods to a very different problem : answer ranking in community question answering . in particular , we adopt a pairwise neural network architecture , which incorporates mte features , as well as rich syntactic and semantic embeddings , and which efficiently models complex non-linear interactions . the evaluation results show state-of-the-art performance , with sizeable contribution from both the mte features and from the pairwise nn architecture .
this paper presents an interactive modeling system that constructs 3d models from a collection of panoramic image mosaics . a panoramic mosaic consists of a set of images taken around the same viewpoint , and a transformation matrix associated with each input image . our interactive modeling system first recovers the camera pose for each mosaic from known line directions and points , and then constructs the 3d models using all available geometrical constraints . we partition constraints into soft and hard linear constraints so that the interactive modeling system can be formulated as a linearly-constrained least-squares problem , which can be solved efficiently using qr factorization . the results of extracting wire frame and texture-mapped 3d models from single and multiple panoramas are presented .
mobile multimedia applications , the focus of many forthcoming wireless services , increasingly demand low-power techniques implementing content protection and customer privacy . in this paper a low-complexity , perception-based partial encryption scheme for telephone-bandwidth speech is presented . speech compressed by a widely-used speech coding algorithm , itu-t g. 729 cs-acelp at 8 kb/s , is partitioned in two classes , one , the most perceptually relevant , to be encrypted , the other , to be left unprotected . encryp-tion of about 45 % of the cs-acelp achieves content protection equivalent to full encryption of the cs-acelp , as verified by both objective measures and formal listening tests . low-power , portable devices can , therefore , implement very high levels of speech-content protection at a fraction of the computational load of current techniques , freeing resources for other tasks and enabling longer battery life .
we present a convolutional network capable of inferring a 3d representation of a previously unseen object given a single image of this object . concretely , the convolutional network can predict an rgb image and a depth map of the object as seen from an arbitrary view . several of these depth maps fused together give a full point cloud of the object . the point cloud can in turn be transformed into a surface mesh . the convolutional network is trained on renderings of synthetic 3d models of cars and chairs . convolutional network successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars .
recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria , which characterize outcomes resulting from no-regret learning dynamics , have near-optimal welfare . this work provides two main technical results that lift this conclusion to games of incomplete information , a.k.a. , bayesian games . first , near-optimal welfare in bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public . second , no-regret learning dynamics converge to bayesian coarse correlated equilibrium in these incomplete information games . these results are enabled by interpretation of a bayesian game as a stochastic game of complete information .
joint uncertainty decoding has recently achieved promising results by integrating the front-end uncertainty into the back-end in a mathematically consistent framework . in this paper , joint uncertainty decoding is compared with the widely used vector taylor series . we show that the two methods are identical except that joint uncertainty decoding applies the taylor expansion on each regression class whereas vector taylor series applies taylor expansion to each hmm mixture . the relatively rougher expansion points used in joint uncertainty decoding make taylor expansion computationally cheaper than vector taylor series but inevitably worse on recognition accuracy . to overcome this drawback , this paper proposes an improved joint uncertainty decoding algorithm which employs second-order taylor expansion on each regression class in order to reduce the expansion errors . special considerations are further given to limit the overall computational cost by adopting different number of regression classes for different orders in the taylor expansion . experiments on the aurora 2 database show that the proposed joint uncertainty decoding algorithm is able to beat vector taylor series on recognition accuracy and computational cost with relative improvement up to 6 % and 60 % , respectively .
given a multidimensional data set and a model of its density , we consider how to define the optimal interpolation between two points . this is done by assigning a cost to each path through space , based on two competing goals-one to interpolate through regions of high density , the other to minimize arc length . from this path functional , we derive the euler-lagrange equations for extremal motionj given two points , the desired interpolation is found by solving a boundary value problem . we show that this interpolation can be done efficiently , in high dimensions , for gaussian , dirichlet , and mixture models .
we present a new method for computing semantic relatedness of concepts . the method relies solely on the structure of a conceptual network and eliminates the need for performing additional corpus analysis . the conceptual network is employed to generate artificial conceptual glosses . they replace textual definitions proper written by humans and are processed by a dictionary based metric of semantic relatedness -lsb- 1 -rsb- . we implemented the metric on the basis of germanet , the german counterpart of wordnet , and evaluated the results on a german dataset of 57 word pairs rated by human subjects for their semantic re-latedness . our approach can be easily applied to compute semantic relatedness based on alternative conceptual networks , e.g. in the domain of life sciences .
following an idea from -lsb- 1 -rsb- , based on the gaussian properties of eigenimages , this paper presents a new technique for texture classification using multiresolution eigenimages . the input image , composed of two textures from the brodatz album , is subdivided into n sub-images of fixed size δµδ , which are blurred with a gaussian and normalized . the application of hotelling transform decomposes each sub-image into δ 2 eigenimages . the r largest resulting coefficients can be used for classification of the texture present in the sub-images . classification is done using the fuzzy c-means algorithm and the performance is measured with an appropriate quality factor . we discuss the successful application of this technique , as well as the influence of the different parameters of the classification process on several pairs of textures . moreover , combination of hotelling coefficients obtained with different values of δ is shown to improve the performance , based on the idea of analyzing the texture at different levels of resolution .
we present unsupervised approaches to the problem of modeling dialog acts in asynchronous conversations ; i.e. , conversations where participants collaborate with each other at different times . in particular , we investigate a graph-theoretic deter-ministic framework and two probabilistic conversation models -lrb- i.e. , hmm and hmm+m ix -rrb- for mod-eling dialog acts in emails and forums . we train and test our conversation models on -lrb- a -rrb- temporal order and -lrb- b -rrb- graph-structural order of the datasets . empirical evaluation suggests -lrb- i -rrb- the graph-theoretic deter-ministic framework that relies on lexical and structural similarity metrics is not the right model for this task , -lrb- ii -rrb- conversation models perform better on the graph-structural order than the temporal order of the datasets and -lrb- iii -rrb- hmm+m ix is a better conversation models than the simple hmm model .
grounded models -lrb- siena 2001b -rrb- differ from axiomatic theories in establishing explicit connections between language and reality that are learned through language games -lrb- wittgen-stein 1953 -rrb- . this paper describes how grounded models are constructed by autonomous agents as a side effect of their activity playing different types of language games -lrb- steels 1999 -rrb- , and explains how they can be used for intuitive reasoning . it proposes a particular language game which can be used for simulating the generation of logical categories -lrb- such as negation , conjunction , disjunction , implication or equivalence -rrb- , and describes some experiments in which a couple of visually grounded agents construct a grounded model that can be used for spatial reasoning .
this paper presents an innovative framework for user authentication based on images . common user authentication based on passwords has the main drawback of the human difficulty in recalling them . images are instead easier to remember than passwords . moreover , modern compression and transmission techniques make image exchange between different devices -lrb- e.g. mobile phones , personal digital assistants , laptops , and workstations -rrb- in heterogeneous networks practically feasible . in the proposed approach , images are coded using the emerging jpeg2000 standard and taking advantage of many of its features -lrb- e.g. image scalability , embedded bitstream , image tiling , and interactivity protocol -rrb- . the described image based authentication is more secure than the common approach based on password .
automatic keyphrase extraction aims to pick out a set of terms as a representation of a document without manual assignment efforts . supervised and unsupervised graph-based ranking methods have been studied for this task . however , previous methods usually computed importance scores of words under the assumption of single relation between words . in this work , we propose wordtopic-multirank as a new method for keyphrase extraction , based on the idea that words relate with each other via multiple relations . first we treat various latent topics in documents as heterogeneous relations between words and construct a multi-relational word network . then , a novel ranking algorithm , named wordtopic-multirank , is applied to score the importance of words and topics simultaneously , as words and topics are considered to have mutual influence on each other . experimental results on two different data sets show the outstanding performance and robustness of our proposed approach in automatic keyphrase extraction task .
the growing availability of very high resolution -lrb- < 1 m/pixel -rrb- satellite and aerial images has opened up unprecedented opportunities to monitor and analyze the evolution of land-cover and land-use across the world . to do so , images of the same geographical areas acquired at different times and , potentially , with different sensors must be efficiently parsed to update maps and detect land-cover changes . however , a na ¨ ıve transfer of ground truth labels from one location in the source image to the corresponding location in the target image is generally not feasible , as these images are often only loosely registered -lrb- with up to ± 50m of non-uniform errors -rrb- . furthermore , land-cover changes in an area over time must be taken into account for an accurate ground truth transfer . to tackle these challenges , we propose a mid-level sensor-invariant representation that encodes image regions in terms of the spatial distribution of their spectral neighbors . we incorporate this mid-level sensor-invariant representation in a markov random field to simultaneously account for nonlinear mis-registrations and enforce locality priors to find matches between multi-sensor images . we show how our approach can be used to assist in several mul-timodal land-cover update and change detection problems .
exact inference in densely connected bayesian networks is computation-ally intractable , and so there is considerable interest in developing effective approximation schemes . one approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution . while this leads to a tractable algorithm , the mean field distribution is assumed to be factorial and hence unimodal . in this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions . we derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks . our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased .
the discrete fourier transform of signals constructed through mul-tiplicative and additive iterative procedures is determined and its specific features are considered . it is shown that -- in spite of the rather different structure of multiplicative and additive signals -- the fourier transforms of both types of signals exhibit the property of self-affinity . the power spectra of additive signals produced by different generating vectors have similar forms and can be divided into similar branches . the number of branches depends on the generation level and the symmetry of the power spectrum of the generating vector .
in this paper we study the limitations of current verification strategies in object recognition and suggest how verification strategies may be enhanced . on the whole object topology is exploited little during verification . in practice , understanding the con-nectivity relationships between features in the image , or on the object , can lead to significantly more accurate evaluations of recognition hypotheses . we study how topology reasoning allows us to hypothesize the presence of occlusions in the image . analysis of these hypotheses provides information which turns out to be crucial to the quality of our overall verification results .
this paper presents a novel probabilistic approach to integrating multiple cues in visual tracking . we perform visual tracking in different cues by interacting processes . each process is represented by a hidden markov model , and these parallel processes are arranged in a chain topol-ogy . the resulting linked hidden markov models naturally allow the use of particle filters and belief propagation in a unified framework . in particular , a target is tracked in each cue by a particle filter , and the particle filters in different cues interact via a message passing scheme . the general framework of our probabilistic approach allows a customized combination of different cues in different situations , which is desirable from the implementation point of view . our examples selectively integrate four visual cues including color , edges , motion and contours . we demonstrate empirically that the ordering of the cues is nearly inconsequential , and that our probabilistic approach is superior to other approaches such as independent integration and hierarchical integration in terms of flexibility and robustness .
we develop new stochastic optimization methods that are applicable to a wide range of structured regularizations . basically our stochastic optimization methods are combinations of basic stochastic optimization techniques and alternating direction multiplier method . alternating direction multiplier method is a general framework for optimizing a composite function , and has a wide range of applications . we propose two types of online variants of alternating direction multiplier method , which correspond to on-line proximal gradient descent and regular-ized dual averaging respectively . the proposed stochastic optimization methods are computationally efficient and easy to implement . our stochastic optimization methods yield o -lrb- 1 / √ t -rrb- convergence of the expected risk . moreover , the online proximal gradient descent type method yields o -lrb- log -lrb- t -rrb- / t -rrb- convergence for a strongly convex loss . numerical experiments show effectiveness of our stochastic optimization methods in learning tasks with structured sparsity such as overlapped group lasso .
distributed constraint optimization -lrb- dcop -rrb- is rapidly emerging as a prominent technique for multiagent coordination . however , despite agent privacy being a key motivation for applying distributed constraint optimization in many applications , rigorous quantitative evaluations of privacy loss in distributed constraint optimization have been lacking . recently , -lsb- maheswaran et al. 2005 -rsb- introduced a framework for quantitative evaluations of privacy in distributed constraint optimization , showing that some distributed constraint optimization lose more privacy than purely centralized approaches and questioning the motivation for applying distributed constraint optimization . this paper addresses the question of whether state-of-the art distributed constraint optimization suffer from a similar shortcoming by investigating several of the most efficient distributed constraint optimization , including both dpop and adopt . furthermore , while previous work investigated the impact on efficiency of distributed contraint reasoning design decisions -lrb- e.g. constraint-graph topology , asynchrony , message-contents -rrb- , this paper examines the privacy aspect of such distributed contraint reasoning design decisions , providing an improved understanding of privacy-efficiency tradeoffs .
we present a method for constructing ensembles from libraries of thousands of models . model libraries are generated using different learning algorithms and parameter settings . forward stepwise selection is used to add to the ensemble the models that maximize its performance . ensemble selection allows ensembles to be optimized to performance metric such as accuracy , cross entropy , mean precision , or roc area . experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection .
matching pedestrians across multiple camera views known as human re-identification is a challenging problem in visual surveillance . in the existing works concentrating on feature extraction , representations are formed locally and independent of other regions . we present a novel siamese long short-term memory architecture that can process image regions sequentially and enhance the discriminative capability of local feature representation by leverag-ing contextual information . the feedback connections and internal gating mechanism of the siamese long short-term memory architecture enable our siamese long short-term memory architecture to memorize the spatial dependencies and selectively propagate relevant contextual information through the network . we demonstrate improved performance compared to the baseline algorithm with no lstm units and promising results compared to state-of-the-art methods on market-1501 , cuhk03 and viper datasets . visualization of the internal mechanism of siamese long short-term memory architecture shows meaningful patterns can be learned by our siamese long short-term memory architecture .
the formulation of trace quotient is shared by many computer vision problems ; however , it was conventionally approximated by an essentially different formulation of quotient trace , which can be solved with the generalized eigenvalue decomposition approach . in this paper , we present a direct solution to the former formulation . first , considering that the feasible solutions are constrained on a grassmann manifold , we present a necessary condition for the optimal solution of the trace quotient problem , which then naturally elicits an iterative procedure for pursuing the optimal solution . the proposed algorithm , referred to as optimal projection pursuing , has the following characteristics : 1 -rrb- optimal projection pursuing directly optimizes the trace quotient , and is theoretically optimal ; 2 -rrb- optimal projection pursuing does not suffer from the solution uncertainty issue existing in the quotient trace formulation that the objective function value is invariant under any non-singular linear transformation , and optimal projection pursuing is invariant only under orthogonal transformations , which does not affect final distance measurement ; and 3 -rrb- optimal projection pursuing reveals the underlying equivalence between the trace quotient problem and the corresponding trace difference problem . extensive experiments on face recognition validate the superiority of optimal projection pursuing over the solution of the corresponding quotient trace problem in both objective function value and classification capability .
we analyze the sub-optimality of traditional greedy active learning based relevance feedback methods in image retrieval , and propose a novel active learning approach to query labels of multiple images together , which minimize the needed round of feedbacks and achieve satisfactory result in a near optimal manner . our experiments on real image retrieval demonstrate that our active learning approach can yield comparable precession/recall rate by significantly less relevance feedbacks .
the aim of this paper is fine-grained categorization without human interaction . different from prior work , which relies on detectors for specific object parts , we propose to localize distinctive details by roughly aligning the objects using just the overall shape , since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes . the alignments are then used to transfer part annotations from training images to test images -lrb- supervised alignment -rrb- , or to blindly yet consistently segment the object in a number of regions -lrb- unsupervised alignment -rrb- . we furthermore argue that in the distinction of fine-grained sub-categories , classification-oriented encodings like fisher vectors are better suited for describing localized information than popular matching oriented features like hog . we evaluate the method on the cu-2011 birds and stanford dogs fine-grained datasets , outperforming the state-of-the-art .
we present minimum bayes-risk system combination , a method that integrates consensus decoding and system combination into a unified multi-system minimum bayes-risk technique . unlike other mbr methods that re-rank translations of a single smt system , minimum bayes-risk system combination uses the mbr decision rule and a linear combination of the component systems ' probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary . we introduce expected bleu , an approximation to the bleu score that allows to efficiently apply bleu in these conditions . minimum bayes-risk system combination is a general method that is independent of specific smt models , enabling us to combine systems with heterogeneous structure . experiments show that our approach bring significant improvements to single-system-based mbr decoding and achieves comparable results to different state-of-the-art system combination methods .
the acoustic pressure microphone has served as the primary instrument for collecting speech data for automatic speech recognition systems . the acoustic pressure microphone suffers from limitations , such as sensitivity to background noise and relatively far proximity to speech production organs . alternative speech collection sensors may serve to enhance the effectiveness of automatic speech recognition systems . in this study , we first consider an experimental evaluation of the teo-cb-autoenv feature in an actual law enforcement training scenario . we consider feature relation to stress level assessment over time . next , we explore the use of the physiological microphone , a gel-based device placed next to the vocal folds on the outside of the throat used to measure vibrations of the vocal tract and minimize background noise , as we investigate the effectiveness of a teo-cb-autoenv-based automatic stress recognition system . we employ both acoustic and physiological sensors as stand-alone speech data collection devices as well as consider both sensors concurrently . for the latter , we devise a weighted composite decision scheme using both the acoustic and physiological microphone data that yields relative average error rate reductions of 32 % and 6 % versus sole employment of acoustic and physiological microphone data , respectively , in a realistic stressful environment .
the problem of finding a minimum vertex cover in a graph is a well known np-hard problem with important applications . there has been much interest in developing heuristic algorithms for finding a '' good '' vertex cover in graphs . in practice , most heuristic algorithms for minimum vertex cover are based on the local search method . previously , local search algorithms for minimum vertex cover have focused on solving academic benchmarks where the graphs are of relatively small size , and they are not suitable for solving massive graphs as they usually have high-complexity heuristics . in this paper , we propose a simple and fast local search algorithm called minimum vertex cover for solving minimum vertex cover in massive graphs , which is based on two low-complexity heuristics . experimental results on a broad range of real world massive graphs show that minimum vertex cover finds much better vertex covers -lrb- and thus also independent sets -rrb- than state of the art local search algorithms for minimum vertex cover .
in this paper , a new approach to model syllable pitch contour for mandarin speech is proposed . it takes the mean and shape of syllable pitch contour as two basic modeling units and considers several affecting factors that contribute to their variations . parameters of the two models are automatically estimated by the em algorithm . experimental results showed that rmses of 0.551 ms and 0.614 ms in the reconstructed pitch were obtained for the closed and open tests , respectively . all inferred values of those affecting factors agreed well with our prior linguistic knowledge . besides , the prosodic states automatically labeled by the pitch mean model provided useful cues to determine the prosodic phrase boundaries occurred at inter-syllable locations without punctuation marks . so it is a promising pitch modeling approach .
a study of the durational characteristics of hindi stop consonants in spoken sentences was carried out . an annotated and time-aligned hindi speech database was used in the experiment . the influences of aspiration , voicing and gemination on the durations of closure and post-release segments of plosives as well as the duration of the preceding vowel were studied . it was observed that the post-release duration of a plosive changes systematically with manner of articulation . however , due to its large variation in continuous speech , the post-release duration alone is not sufficient to identify the manner of articulation of hindi stops as hypothesised in earlier studies . a low value of the ratio of the duration of a vowel to the closure duration of the following plosive is a reliable indicator of gemination in hindi stop consonants in continuous speech .
we present a novel weight design for average consensus that improves its transient and stead-state performance . the idea is to blend metropolis-hastings weights and convex-optimization weights via signal-adaptive morphing coefficients . the resulting weight design is shown to be particularly useful in dynamic scenarios where the measurements feature abrupt changes or unknown noise levels .
face recognition -lrb- face recognition -rrb- with a single training sample per person -lrb- face recognition -rrb- is a very challenging problem due to the lack of information to predict the variations in the query sample . sparse representation based classification has shown interesting results in robust face recognition ; however , its performance will deteriorate much for face recognition with face recognition . to address this issue , in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by face recognition . instead of learning from the generic training set independently w.r.t. the gallery set , the proposed sparse variation dictionary learning method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set . the learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images , including illumination , expression , occlusion , pose , etc. , can be better handled . experiments on the large-scale cmu multi-pie , frgc and lfw databases demonstrate the promising performance of face recognition on face recognition with face recognition .
this paper presents a deep neural network to extract articulatory information from the speech signal and explores different ways to use such information in a continuous speech recognition task . the deep neural network was trained to estimate articulatory trajectories from input speech , where the training data is a corpus of synthetic english words generated by the haskins laboratories ' task-dynamic model of speech production . speech parameterized as cepstral features were used to train the deep neural network , where we explored different cepstral features to observe their role in the accuracy of articulatory trajectory estimation . the best feature was used to train the final deep neural network , where the deep neural network was used to predict articulatory trajectories for the training and testing set of aurora-4 , the noisy wall street journal corpus . this study also explored the use of hidden variables in the deep neural network as a potential acoustic feature candidate for speech recognition and the results were encouraging . word recognition results from aurora-4 indicate that the articulatory features from the deep neural network provide improvement in speech recognition performance when fused with other standard cepstral features ; however when tried by themselves , they failed to match the baseline performance . index terms -- automatic speech recognition , articulatory trajectories , vocal tract variables , deep neural networks .
in this paper the approximation of a complex-valued spec-ication by the frequency response of a 2-d iir separable denominator -lrb- sd -rrb- digital lter is considered . the approximation problem is transformed into an equivalent one , where a real-valued 2-d iir sd digital lter with some additional characteristics has to be determined that approximates a given real-valued 2-d fir digital lter . a theorem is presented that helps to reduce the number of parameters in the fir-to-iir approximation problem and a procedure to solve the problem numerically is given .
we propose an optimum channel shortening method for discrete multitone dmt transceivers . the proposed optimum channel shortening method shortens a given channel to a desired length while maximizing the number of bits transmitted on a dmt symbol . the key to the optimum solution is the deenition of the snr in a subchannel using the equivalent signal , noise , and isi paths in the system . our simulation results show that the proposed optimum channel shortening method outperforms the best existing optimum channel shortening method with a 18 increase in the bit rate . we show that the maximum shortening snr method is a special case of the proposed optimum channel shortening method and both methods are nearly equivalent when the input energy distribution is constant o v er all subchannels .
† jacobian adaptation of the acoustic models is an efficient adaptation technique for robust speech recognition . several improvements for the ja have been proposed in the last years , either to generalize the jacobian linear transformation for the case of large noise mismatch between training and testing or to extend the adaptation to other degrading factors , like channel distortion and vocal tract length . however , the ja technique has only been used so far with the conventional mel-frequency cepstral coefficients . in this paper , the ja technique is applied to an alternative type of features , the frequency-filtered spectral energies , resulting in a more computationally efficient ja technique . furthermore , in experimental tests with the database aurora1 , this new ja technique has shown an improved recognition performance with respect to the jacobian adaptation with mel-frequency cepstral coefficients .
we present a new algorithm to jointly track multiple objects in multi-view images . while this has been typically addressed separately in the past , we tackle the problem as a single global optimization . we formulate this assignment problem as a min-cost problem by defining a graph structure that captures both temporal correlations between objects as well as spatial correlations enforced by the configuration of the cameras . this leads to a complex combinato-rial optimization problem that we solve using dantzig-wolfe decomposition and branching . our formulation allows us to solve the problem of reconstruction and tracking in a single step by taking all available evidence into account . in several experiments on multiple people tracking and 3d human pose tracking , we show our method outperforms state-of-the-art approaches .
this paper presents a method for obtaining class membership probability estimates for multiclass classification problems by coupling the probability estimates produced by binary classifiers . this is an extension for arbitrary code matrices of a method due to hastie and tibshirani for pairwise coupling of probability estimates . experimental results with boosted naive bayes show that our method produces calibrated class membership probability estimates , while having similar classification accuracy as loss-based decoding , a method for obtaining the most likely class that does not generate probability estimates .
a major hindrance to rendering spoken dialog systems capable of ongoing , continuous listening without requiring a push-to-talk device is the spoken dialog systems of distinguishing speech which is intended for the system from that which is overheard . we present a decision-theoretic approach to this spoken dialog systems that exploits bayesian models of spoken dialog at four levels of analysis within a domain-independent , multi-modal computational architecture called quartet . we applied quartet to the task of navigating powerpoint slide shows during a spoken presentation in a prototype system called presenter . we describe the runtime behavior of presenter as well as the results of an experimental study comparing the performance of presenter to human subjects in discriminating arbitrarily formed spoken requests for powerpoint slide shows during a recorded lecture .
sinusoidal modeling of audio at low-bit rates involves selecting a limited number of parameters according to a quantitative or perceptual criterion . most perceptual sinusoidal component selection strategies are computationally intensive and not suitable for real-time applications . in this paper , a computationally efficient sinusoidal selection algorithm based on a novel hybrid loudness estimation scheme is presented . the hybrid loudness estimation scheme first estimates efficiently the loudness of a multi-tone signal from the loudness patterns of its constituent sinusoidal components . then it refines this estimate by performing a full evaluation of loudness but only in select critical bands . experimental results show that the proposed technique maintains a low perceptual sinusoidal synthesis error at a much lower computational complexity .
in 1990 , thomas schwartz proposed the conjecture that every nonempty tournament has a unique minimal τ-retentive set -lrb- τ stands for tournament equilibrium set -rrb- . a weak variant of schwartz 's conjecture was recently proposed by felix brandt . however , both conjectures were disproved very recently by two counterexamples . in this paper , we prove sufficient conditions for infinite classes of tournaments that satisfy schwartz 's conjecture and brandt 's conjecture . moreover , we prove that τ can be calculated in polynomial time in several infinite classes of tournaments . furthermore , our results reveal some structures that are forbidden in every counterexample to schwartz 's conjecture .
the goal of this paper is to perform 3d object detection from a single monocular image in the domain of autonomous driving . our method first aims to generate a set of candidate class-specific object proposals , which are then run through a standard cnn pipeline to obtain high-quality object detections . the focus of this paper is on proposal generation . in particular , we propose an object proposal generation approach that places object candidates in 3d using the fact that objects should be on the ground-plane . we then score each candidate box projected to the image plane via several intuitive potentials encoding semantic seg-mentation , contextual information , size and location priors and typical object shape . our experimental evaluation demonstrates that our object proposal generation approach significantly outperforms all monocular approaches , and achieves the best detection performance on the challenging kitti benchmark , among published monocular competitors .
this paper presents pipeline iteration , an approach that uses output from later stages of a pipeline iteration to constrain earlier stages of the same pipeline iteration . we demonstrate significant improvements in a state-of-the-art pcfg parsing pipeline using base-phrase constraints , derived either from later stages of the parsing pipeline or from a finite-state shallow parser . the best performance is achieved by reranking the union of un-constrained parses and relatively heavily-constrained parses .
the paper is concerning an approach for understanding speech using a new form of probabilistic models to represent syntactic and semantic knowledge of a restricted domain . one important feature of our grammar is that the parse tree directly represents the semantic content of the utterance . since we determine that semantic content by an integrated search , we avoid consistency problems at the interface between the recognizer and the language understanding part of the speech understanding system . we succeeded in designing such an incremental algorithm , which integrates semantic , syntactic , and acoustic-phonetic knowledge in a seamless , consistent way . high efficiency is achieved by using a chart-parsing technique with structure-sharing and a strict top-down strategy for opening new word hypotheses in the pronunciation layer .
this work explores a statistical basis for a process often described in computer vision : image segmentation by region merging following a particular order in the choice of regions . we exhibit a particular blend of algorithmics and statistics whose error is , as we formally show , close to the best possible . this approach can be approximated in a very fast segmentation algorithm for processing images described using most common numerical feature spaces . simple modifications of the algorithm allow to cope with occlu-sions and/or hard noise levels . experiments on grey-level and color images , obtained with a short c-code , display the quality of the segmentations obtained .
we introduce a new light-field dataset of materials , and take advantage of the recent success of deep learning to perform material recognition on the 4d light-field . our light-field dataset of materials contains 12 material categories , each with 100 images taken with a lytro illum , from which we extract about 30,000 patches in total . to the best of our knowledge , light-field dataset of materials is the first mid-size dataset for light-field images . our main goal is to investigate whether the additional information in a light-field -lrb- such as multiple sub-aperture views and view-dependent reflectance effects -rrb- can aid material recognition . since recognition networks have not been trained on 4d images before , we propose and compare several novel cnn architectures to train on light-field images . in our experiments , the best performing cnn architectures achieves a 7 % boost compared with 2d image classification -lrb- 70 % → 77 % -rrb- . these results constitute important baselines that can spur further research in the use of cnn architectures for light-field applications . upon publication , our light-field dataset of materials also enables other novel applications of light-fields , including object detection , image segmentation and view interpolation .
eric cosatto we developed a system for finding address blocks on mail pieces that can process four images per second . besides locating the address blocks , our system also determines the writing style , handwritten or machine printed , and moreover , it measures the skew angle of the text lines and cleans noisy images . a layout analysis of all the elements present in the image is performed in order to distinguish drawings and dirt from text and to separate text of advertisement from that of the destination address . a speed of more than four images per second is obtained on a modular hardware platform , containing a board with two of the net32k neural net chips , a sp arc2 processor board , and a board with 2 digital signal processors . the system has been tested with more than 100,000 images . its performance depends on the quality of the images , and lies between 85 % correct location in very noisy images to over 98 % in cleaner images .
the paper addresses the problem of learning to parse sentences to logical representations of their underlying meaning , by inducing a syntactic-semantic grammar . the approach uses a class of grammars which has been proven to be learnable from representative examples . in this paper , we introduce tractable learning algorithms for learning this class of grammars , comparing them in terms of a-priori knowledge needed by the learner , hypothesis space and algorithm complexity . we present experimental results on learning tense , aspect , modality and negation of verbal constructions .
we propose a new method to incorporate statistical priors on the solution of the nonnegative matrix factorization for single-channel source separation applications . the gaussian mixture model -lrb- gmm -rrb- is used as a log-normalized gain prior model for the nmf solution . the normalization makes the prior log-normalized gain prior model energy independent . in nmf based scss , nmf is used to decompose the spectra of the observed mixed signal as a weighted linear combination of a set of trained basis vectors . in this work , the nmf decomposition weights are enforced to consider statistical prior information on the weight combination patterns that the trained basis vectors can jointly receive for each source in the observed mixed signal . the nmf solutions for the weights are encouraged to increase the log-likelihood with the trained gain prior gmms while reducing the nmf reconstruction error at the same time .
fully observed large binary matrices appear in a wide variety of contexts . to model them , proba-bilistic matrix factorization methods are an attractive solution . however , current batch algorithms for proba-bilistic matrix factorization methods can be inefficient because batch algorithms need to analyze the entire data matrix before producing any parameter updates . we derive an efficient stochastic inference algorithm for proba-bilistic matrix factorization methods of fully observed binary matrices . our stochastic inference algorithm exhibits faster convergence rates than more expensive batch algorithms and has better predictive performance than scalable alternatives . the proposed stochastic inference algorithm includes new data subsampling strategies which produce large gains over standard uniform subsampling . we also address the task of automatically selecting the size of the minibatches of data used by our stochastic inference algorithm . for this , we derive an stochastic inference algorithm that adjusts this hyper-parameter online .
in this paper , we investigate the use of a switched split vector quantiser for coding linear predictive coding parameters . the switched split vector quantiser is applied to quantise the lpc parameters in terms of line spectral frequencies from the timit database and its performance is compared with the split vector quantiser . experimental results show that the switched split vector quantiser provides a better trade-off between bit-rate and distortion performance than the split vq . in addition , the switched split vector quantiser has a lower computational complexity than the split vq , though this is attained at the expense of an increase in memory requirements . in order to achieve a spectral distortion of 1 db , the switched split vector quantiser with an 8 directional switch requires 23 bits/frame , 4.41 kflops/frame of computations and 8272 floats of memory , while the corresponding values for a traditional three-part split vq are 25 bits/frame , 13.3 kflops/frame and 3328 floats , respectively .
this paper presents a novel approach to sign language recognition that provides extremely high classification rates on minimal training data . key to this approach is a 2 stage classification procedure where an initial classification stage extracts a high level description of hand shape and motion . this high level description is based upon sign linguistics and describes actions at a conceptual level easily understood by humans . moreover , such a description broadly generalises temporal activities naturally overcoming variability of people and environments . a second stage of classification is then used to model the temporal transitions of individual signs using a classifier bank of markov chains combined with independent component analysis . we demonstrate classification rates as high as 97.67 % for a lexicon of 43 words using only single instance training outperforming previous approaches where thousands of training examples are required .
commonsense reasoning at scale is a core problem for cognitive systems . in this paper , we discuss two ways in which heuristic graph traversal methods can be used to generate plausible inference chains . first , we discuss how cyc 's predicate-type hierarchy can be used to get reasonable answers to queries . second , we explain how connection graph-based techniques can be used to identify script-like structures . finally , we demonstrate through experiments that these heuristic graph traversal methods lead to significant improvement in accuracy for both q/a and script construction .
prior knowledge in the form of multiple polyhedral sets , each belonging to one of two categories , is introduced into a reformulation of a linear support vector machine classifier . the resulting formulation leads to a linear program that can be solved efficiently . real world examples , from dna sequencing and breast cancer prognosis , demonstrate the effectiveness of the proposed method . numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary , data-based linear support vector machine classifiers . one experiment also shows that a linear classifier , based solely on prior knowledge , far outperforms the direct application of prior knowledge rules to classify data .
in this study the problem of modeling a family of curves is addressed . the need of such modeling appears frequently in many aspects of image processing where many linear structures keep spatial relationships during their evolution . we come up with a modeling tool well suited to the spatial modeling of a family of curves , and which can be very useful for motion tracking and curve evolution as well . the family of curves is represented as the line paths -lrb- orbits -rrb- of a '' spline vector field '' , i.e. a vector field interpolating data using a framework similar to the theory of spline curves . the modeling tool is exemplified with oceanic satellite data . its usefullness for curve evolution modeling is also presented .
word and n-gram posterior probabilities estimated on n-best hypotheses have been used to improve the performance of statistical machine translation in a rescoring framework . in this paper , we extend the idea to estimate the posterior probabilities on n-best hypotheses for translation phrase-pairs , target language n-grams , and source word re-orderings . the statistical machine translation is self-enhanced with the posterior knowledge learned from n-best hypotheses in a re-decoding framework . experiments on nist chinese-to-english task show performance improvements for all the strategies . moreover , the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 bleu score on nist-2003 set , and 0.64 on nist-2005 set , respectively .
in previous work we showed that state-of-the-art end-of-utterance detection -lrb- as used , for example , in dialog systems -rrb- can be improved significantly by making use of prosodic and/or language models that predict utterance endpoints , based on word and alignment output from a speech recognizer . however , using a recog-nizer in endpointing might not be practical in certain applications . in this paper we demonstrate that the improvements due to the prosodic knowledge can be realized largely without alignment information , i.e. , without requiring a speech recognizer . a prosodic end-of-utterance detector using only speech/nonspeech detection output is still considerably more accurate and has lower latency than a baseline system based on pause-length thresholding .
random problem distributions have played a key role in the study and design of algorithms for constraint satisfaction and boolean satisfiability , as well as in our understanding of problem hardness , beyond standard worst-case complexity . we consider random problem distributions from a highly structured problem domain that generalizes the quasigroup completion problem and quasigroup with holes -lrb- qwh -rrb- , a widely used domain that captures the structure underlying a range of real-world applications . our problem domain is also a generalization of the well-known sudoku puzzle : we consider sudoku instances of arbitrary order , with the additional generalization that the block regions can have rectangular shape , in addition to the standard square shape . we evaluate the computational hardness of generalized sudoku instances , for different parameter settings . our experimental hardness results show that we can generate instances that are considerably harder than qcp/qwh instances of the same size . more interestingly , we show the impact of different balancing strategies on problem hardness . we also provide insights into backbone variables in generalized sudoku instances and how they correlate to problem hardness .
multidimensional -lrb- md -rrb- physical systems are usually given in terms of partial differential equations . similar to one-dimensional systems , they can also be described by transfer function models . in addition to including initial and boundary conditions as well as excitation functions exactly , the transfer function models can also be discretized in a simple way . this leads to suitable implementations for digital signal processors . therefore it is possible to implement physics based digital sound synthesis algorithms derived from transfer function models in real-time . this paper extends the recently presented solution for vibrating strings with one spatial dimension to two-dimensional drum models .
in this paper , we propose a new acoustic measure for detecting aspiration noise in vowels . the acoustic measure is an index of synchronization between frequency bands around the first and third formants . the acoustic measure is based on the principle that the vocal tract responses to the glottal excitation are synchronized between these frequency bands when aspiration noise is absent , and uncorrelated otherwise . evaluation results show that the proposed acoustic measure can be used together with spectral slope measures for automatic detection of aspiration noise .
jpeg2000 part 2 allows the application of arbitrary wavelet decomposition structures -lrb- wavelet packet bases -rrb- . efficient anisotropic wavelet packet basis selection for the coding framework of jpeg2000 has been developed and evaluated . previous work focused on isotropic wavelet packet basis selection algorithms for jpeg2000 , which serves as basis for the performance of the assessment of anisotropic wavelet basis selection . several cost functions are applied in a top-down anisotropic wavelet packet selection scheme . our evaluations employ state-of-the-art quality assessment tools supplementary to psnr evaluations .
in this paper we propose a fast and memory efficient encoding strategy for text image compression with the jbig2 standard . the encoder splits up the input image into horizontal stripes and encodes one stripe at a time . construction of the current dictionary is based on updating dictionaries from previous stripes . we describe separate updating processes for the singleton exclusion dictionary and for the modified-class dictionary . experiments show that , for both dictionaries , splitting the page into two stripes can save 30 % of encoding time and 40 % of physical memory with a small loss of about 1.5 % in compression . further gains can be obtained by using more stripes but with diminishing returns . the same updating processes are also applied to compressing multi-page document images and shown to improve compression by 8-10 % over coding a multi-page document as a collection of single-page documents .
in spite of their superior performance , neural probabilistic language models remain far less widely used than n-gram models due to their notoriously long training times , which are measured in weeks even for moderately-sized datasets . training nplms is computationally expensive because training nplms are explicitly normalized , which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients . we propose a fast and simple algorithm for training neural probabilistic language models based on noise-contrastive estimation , a newly introduced procedure for estimating unnormalized continuous distributions . we investigate the behaviour of the algorithm on the penn treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models . the algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well . we demonstrate the scalability of the proposed approach by training several neural language models on a 47m-word corpus with a 80k-word vocabulary , obtaining state-of-the-art results on the microsoft research sentence completion challenge dataset .
we propose a scheme for recycling gaussian random vectors into structured matrices to approximate various kernel functions in sublin-ear time via random embeddings . our framework includes the fastfood construction of le et al. -lrb- 2013 -rrb- as a special case , but also extends to circulant , toeplitz and hankel matrices , and the broader family of structured matrices that are characterized by the concept of low-displacement rank . we introduce notions of coherence and graph-theoretic structural constants that control the approximation quality , and prove unbiasedness and low-variance properties of random feature maps that arise within our framework . for the case of low-displacement matrices , we show how the degree of structure and randomness can be controlled to reduce statistical variance at the cost of increased computation and storage requirements . empirical results strongly support our theory and justify the use of a broader family of structured matrices for scaling up kernel methods using random features .
for interaction with its environment , a robot is required to learn models of objects and to perceive these models in the livestreams from its sensors . in this paper , we propose a novel approach to model learning and real-time tracking . we extract multi-resolution 3d shape and texture representations from rgb-d images at high frame-rates . an efficient variant of the iterative closest points algorithm allows for registering maps in real-time on a cpu . our approach learns full-view models of objects in a probabilistic optimization framework in which we find the best alignment between multiple views . finally , we track the pose of the camera with respect to the learned model by registering the current sensor view to the model . we evaluate our approach on rgb-d benchmarks and demonstrate its accuracy , efficiency , and robustness in model learning and real-time tracking . we also report on the successful public demonstration of our approach in a mobile manipulation task .
active learning refers to algorithmic frameworks aimed at selecting training data points in order to reduce the number of required training data points and/or improve the generalization performance of a learning method . in this paper , we present an asymptotic analysis of active learning for generalized linear models . our analysis holds under the common practical situation of model misspecifica-tion , and is based on realistic assumptions regarding the nature of the sampling distributions , which are usually neither independent nor identical . we derive un-biased estimators of generalization performance , as well as estimators of expected reduction in generalization error after adding a new training data point , that allow us to optimize its sampling distributions through a convex optimization problem . our analysis naturally leads to an algorithm for sequential active learning which is applicable for all tasks supported by generalized linear models -lrb- e.g. , binary classification , multi-class classification , regression -rrb- and can be applied in non-linear settings through the use of mercer kernels .
winder et al. -lsb- 15 , 14 -rsb- have recently shown the superiority of the daisy descriptor -lsb- 12 -rsb- in comparison to other widely extended descriptors such as sift -lsb- 8 -rsb- and surf -lsb- 1 -rsb- . motivated by those results , we present a novel algorithm that extracts viewpoint and illumination invariant keypoints and describes them with a particular implementation of a daisy-like layout . we demonstrate how to efficiently compute the scale-space and re-use this information for the descriptor . comparison to similar approaches such as sift and surf show higher precision vs recall performance of the proposed method . moreover , we dramatically reduce the computational cost by a factor of 6x and 3x , respectively . we also prove the use of the proposed method for computer vision applications .
a method to establish correspondences between regions belonging to independent segmentations of multiple views of a scene is presented . the trade-off between color similarity and projective similarity of the matching regions is formulated in terms of a constrained optimization , analogous to a rate-distortion budget-constrained allocation problem , and solved using lagrangian optimization techniques .
this paper presents the results of a pitch accent categorisation simulation which attempts to classify l * h and h * l pitch accents using a psychologically motivated exemplar-theoretic model of categorisation . pitch accents are represented in terms of six linguistically meaningful parameters describing their shape . no additional information is employed in the categorisation process . the results indicate that these pitch accents can be successfully categorised , via exemplar-based comparison , using a limited number of purely tonal features .
we present a data-driven approach to learn user-adaptive referring expression generation policies for spoken dialogue systems . referring expressions can be difficult to understand in technical domains where users may not know the technical ` jargon ' names of the domain entities . in such cases , data-driven approach must be able to model the user 's domain knowledge and use appropriate referring expressions . we present a reinforcement learning framework in which the data-driven approach learns reg policies which can adapt to unknown users online . furthermore , unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on , we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction , by using a rl framework and a statistical user simulation . we show that in comparison to adaptive hand-coded baseline policies , the learned policy performs significantly better , with an 18.6 % average increase in adaptation accuracy . the best learned policy also takes less dialogue time -lrb- average 1.07 min less -rrb- than the best hand-coded policy . this is because the learned policies can adapt online to changing evidence about the user 's domain expertise .
recent research suggests that it is more appropriate to model pronunciation variation with syllable-length acoustic models than with context-dependent phones . due to the large number of factors contributing to pronunciation variation at the syllable level , the creation of multi-path model topologies appears necessary . in this paper , we propose a novel approach for constructing multi-path models for frequent syllables . the suggested approach uses phonetic knowledge for the initialisation of the parallel paths , and a data-driven solution for their re-estimation . when applied to 94 frequent syllables in a 37-hour corpus of dutch read speech , it leads to improved recognition performance when compared with a triphone recogniser of similar complexity .
in-service , non-intrusive measurement devices -lrb- inmd -rrb- estimate the perceived quality of the telephone link by extracting quality-defining criteria like echo attenuation , echo delay , active speech level , noise level , frame losses and transient failures from a telephone call . in addition , the quality depends on the used digital transmission systems -lrb- codec systems -rrb- . this paper proposes a method to distinguish between two codec classes . with the help of features determined from the speech signal , a classi-fier decides about the class affiliation of the signal . the recognition rate for signals with 16 seconds of active speech is about 97 % .
an acoustic conndence measure for acceptanceerejection of recognition hypotheses for continuous speech utterances is proposed . this acoustic conndence measure is useful for rejecting utterances that are out of domain , or contain out-of-vocabulary words or speech dissuencies . a phone-based approach is implemented so that a single global threshold can be applied to hypothesis rejection for any w ord sequence . phone conn-dence is computed for each frame of speech as the posterior phone probability g i v en the acoustic observation . word sequence conndence is evaluated as the average phone conn-dence , either by w eighting all frames equally or by normalizing by phone duration . the acoustic conndence measure is tested on a database of spoken company names . when normalized by phone duration , acoustic conndence measure achieves , in some cases with less computational expense , rejection performance comparable to a baseline system implementing a common ller-model approach . when all frames are equally weighted , performance is substantially poorer .
multimedia multimedia interfaces are r apidly evolving to facilitate hu-man/machine communication . most of the technologies on which they are b ased a r e , as yet , imperfect . but , the multimedia interfaces do begin to allow information exchange in ways familiar and comfortable to the human | principally through natural actions in the sensory dimensions of sight , sound and touch . further , as digital networking becomes ubiquitous , the opportunity grows for collaborative work through confer-enced c omputing . in this context the machine takes on the role of mediator in human/machine/human communication | the ideal being to extend the intellectual abilities of humans through access to distributed information resources and collective decision making . the challenge is how to design machine mediation so that machine mediation extends , not impedes , human abilities . this report describes evolving work to incorporate multimodal interfaces into a networked system for collabo-rative distributed c omputing . machine mediation also addresses strategies for quantifying the synergies that may be gained .
web-scale data has been used in a diverse range of language research . most of this research has used web counts for only short , fixed spans of context . we present a unified view of using web counts for lexical disambiguation . unlike previous approaches , our supervised and unsupervised systems combine information from multiple and overlapping segments of context . on the tasks of preposition selection and context-sensitive spelling correction , the supervised and unsupervised systems reduces disambiguation error by 20-24 % over the current state-of-the-art .
syntactic structures have been good features for opinion analysis , but it is not easy to use them . to find these features by supervised learning methods , correct syntactic labels are indispensible . two possible sources to acquire syntactic structures are parsing trees and dependency trees . for the annotation processing , parsing trees are more readable for annotators , while dependency trees are easier to use by programs . to use syntactic structures as features , this paper tried to annotate on human friendly materials and transform these annotations to the corresponding machine friendly materials . we annotated the gold answers of opinion syntactic structures on the parsing tree from chinese treebank , and then proposed methods to find their corresponding dependency relations on the dependency trees generated from the same sentence . with these relations , we could train a model to annotate opinion dependency relations automatically to provide an opinion dependency parser , which is language independent if language resources are incorporated . experiment results show that the annotated syntactic structures and their corresponding dependency relations improve at least 8 % of the performance of opinion analysis .
we address the problem of minimizing human effort in interactive tracking by learning sequence-specific model parameters . determining the optimal model parameters for each sequence is a critical problem in interactive tracking . we demonstrate that by using the optimal model parameters for each sequence we can achieve high precision interactive tracking results with significantly less effort . we leverage the sequential nature of interactive tracking to formulate an efficient method for learning model parameters through a maximum margin framework . by using our method we are able to save ∼ 60 − 90 % of human effort to achieve high precision on two datasets : the virat dataset and an infant-mother interaction dataset .
most previous visual recognition systems simply assume ideal inputs without real-world degradations , such as low resolution , motion blur and out-of-focus blur . in presence of such unknown degradations , the conventional approach first resorts to blind image restoration and then feeds the restored image into a classifier . treating restoration and recognition separately , such a straightforward approach , however , suffers greatly from the defective output of the ill-posed blind image restoration . in this paper , we present a joint blind image restoration and recognition method based on the sparse representation prior to handle the challenging problem of face recognition from low-quality images , where the degradation model is realistic and totally unknown . the sparse representation prior states that the degraded input image , if correctly restored , will have a good sparse representation in terms of the training set , which indicates the identity of the test image . the proposed joint blind image restoration and recognition method achieves simultaneous restoration and recognition by iteratively solving the blind image restoration in pursuit of the sparest representation for face recognition . based on such a sparse representation prior , we demonstrate that the image restoration task and the recognition task can benefit greatly from each other . extensive experiments on face datasets under various degradations are carried out and the results of our joint blind image restoration and recognition method shows significant improvements over conventional methods of treating the two tasks independently .
this paper describes an enhanced pruning technique aimed at a further reduction of the active search space in large vocabulary speech recognition , to speed-up decoding while maintaining the accuracy . the pruning technique is based on anticipating both the linguistic and acoustic contribution of a phonetic arc , before expanding that arc in the search . the decoder is based on a time-synchronous beam search and a lexical tree . crossword hmms and m-gram language models are integrated in a single decoding pass . the new pruning technique has been evaluated for one-pass trigram decoding of broadcast news . with respect to the baseline , the search eeort can be halved at almost no degradation . when pruning more aggressively to get a speed-up of 10 , real-time decoding is achieved on hub4 evaluation , however , with an increase of the base error rate by one third .
in this paper we address the problem of identifying a broad range of term variations in japa-nese web search queries , where these variations pose a particularly thorny problem due to the multiple character types employed in its writing system . our method extends the techniques proposed for english spelling correction of web queries to handle a wider range of term variants including spelling mistakes , valid alternative spellings using multiple character types , transliterations and abbreviations . the core of our method is a statistical model built on the mart algorithm -lrb- friedman , 2001 -rrb- . we show that both string and semantic similarity features contribute to identifying term variation in web search queries ; specifically , the semantic similarity features used in our statistical model are learned by mining user session and click-through logs , and are useful not only as model features but also in generating term variation candidates efficiently . the proposed method achieves 70 % precision on the term variation identification task with the recall slightly higher than 60 % , reducing the error rate of a naïve baseline by 38 % .
in this paper we present a new method for pitch estimation using a system based on phase-locked-loop devices . three main blocks define our system . the aim of the first one is to make an harmonic decomposition of the speech signal . this stage is implemented using a band-pass filter bank and phase-locked-loops cascaded to the output of each filter . a second block enhances the harmonic corresponding to the fundamental frequency and attenuates all other harmonics . finally a third stage re-synthesizes a new signal with high energy at the fundamental frequency and extracts pitch contour from that signal using another phase locked-loop . performance is evaluated over two databases of laryngograph-labeled speech and compared to various well known pitch estimation algorithms .
current supervised speech technology relies heavily on transcribed speech and pronunciation dictionaries . in settings where unlabelled speech data alone is available , unsupervised methods are required to discover categorical linguistic structure directly from the audio . we present a novel bayesian model which segments unlabelled input speech into word-like units , resulting in a complete unsupervised transcription of the speech in terms of discovered word types . in our bayesian model , a potential word segment -lrb- of arbitrary length -rrb- is embedded in a fixed-dimensional space ; the model -lrb- implemented as a gibbs sampler -rrb- then builds a whole-word acoustic model in this space while jointly doing seg-mentation . we report word error rates in a connected digit recognition task by mapping the unsupervised output to ground truth transcriptions . our model outperforms a previously developed hmm-based system , even when the model is not constrained to discover only the 11 word types present in the data .
extracting discriminant , transformation-invariant features from raw audio signals remains a serious challenge for speech recognition . the issue of speaker variability is central to this problem , as changes in accent , dialect , gender , and age alter the sound waveform of speech units at multiple levels -lrb- phonemes , words , or phrases -rrb- . approaches for dealing with this variability have typically focused on analyzing the spectral properties of speech at the level of frames , on par with frame-level acoustic modeling usually applied to speech recognition systems . in this paper , we propose a framework for representing speech at the word level and extracting features from the acoustic , temporal domain , without the need for spectral encoding or preprocess-ing . leveraging recent work on unsupervised learning of invariant sensory representations , we extract a signature for a word by first projecting its raw waveform onto a set of templates and their transformations , and then forming empirical estimates of the resulting one-dimensional distributions via histograms . the representation and relevant parameters are evaluated for word classification on a series of datasets with increasing speaker-mismatch difficulty , and the results are compared to those of an mfcc-based representation .
this paper describes our work on audio event detection , one of our tasks in the european project vidivideo . preliminary experiments with a small corpus of sound effects have shown the potential of this type of corpus for training purposes . this paper describes our experiments with svm classifiers , and different features , using a 290-hour corpus of sound effects , which allowed us to build detectors for almost 50 semantic concepts . although the performance of these detectors on the development set is quite good -lrb- achieving an average f-measure of 0.87 -rrb- , preliminary experiments on documentaries and films showed that the task is much harder in real-life videos , which so often include overlapping audio events .
artificial intelligence methods may be used to model human intelligence or to build intelligent computer systems . al has already reached the stage of human simulation where it can model such `` ineffable '' phenomena as intuition , insight and inspiration . this paper reviews the empirical evidence for these capabilities .
we describe an interactive system which supports the exploration of arguments generated from bayesian networks . in particular , we consider key features which support interactive behaviour : -lrb- 1 -rrb- an attentional mechanism which updates the activation of concepts as the interaction progresses ; -lrb- 2 -rrb- a set of exploratory responses ; and -lrb- 3 -rrb- a set of probabilistic patterns and an argument grammar which support the generation of natural language arguments from bayesian networks . a preliminary evaluation assesses the effect of our exploratory responses on users ' beliefs .
visual search is the task of nding a target in an image against a background of distractors . unique features of targets enable them to pop out against the background , while targets deened by lacks of features or conjunctions of features are more diicult to spot . it is known that the ease of target detection can change when the roles of gure and ground are switched . the mechanisms underlying the ease of pop out and asymmetry in visual search have been elusive . this paper shows that a model of segmentation in v1 based on intracortical interactions can explain many of the qualitative aspects of visual search .
this paper presents a novel , ranking-style word segmentation approach , called rsvm-seg , which is well tailored to chinese information retrieval . this ranking-style word segmentation approach makes seg-mentation decision based on the ranking of the internal associative strength between each pair of adjacent characters of the sentence . on the training corpus composed of query items , a ranking model is learned by a widely-used tool ranking svm , with some useful statistical features , such as mutual information , difference of t-test , frequency and dictionary information . experimental results show that , this ranking-style word segmentation approach is able to eliminate overlapping ambiguity much more effectively , compared to the current word segmentation methods . furthermore , as this ranking-style word segmentation approach naturally generates segmentation results with different granular-ity , the performance of chinese information retrieval is improved and achieves the state of the art .
we present temporal milestones for hierarchical task networks to enable the complex synchronization of tasks . a temporal milestone of a task is an intermediate event that occurs during the execution of a complex task , e.g. , the start time , the end time or a milestone of any of its subtasks . unlike landmark variables , introduced in existing work , temporal milestones respect the task abstraction boundaries and preserve structural properties enabling much more efficient reasoning . furthermore , temporal milestones are as expressive as landmark variables . we provide analytical and empirical evidence to support these claims .
we present and evaluate a machine learning approach to constructing patient-specific classifiers that detect the onset of an epileptic seizure through analysis of the scalp eeg , a non-invasive measure of the brain 's electrical activity . this problem is challenging because the brain 's electrical activity is composed of numerous classes with overlapping characteristics . the key steps involved in realizing a high performance machine learning approach included shaping the problem into an appropriate machine learning framework , and identifying the features critical to separating seizure from other types of brain activity . when trained on 2 or more seizures per patient and tested on 916 hours of continuous eeg from 24 patients , our machine learning approach detected 96 % of 173 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period . we also provide information about how to download the chb-mit database , which contains the data used in this study .
our goal is to learn a mahalanobis distance by minimizing a loss defined on the weighted sum of the precision at different ranks . our core motivation is that minimizing a weighted rank loss is a natural criterion for many problems in computer vision such as person re-identification . we propose a novel metric learning formulation called weighted approximate rank component analysis . we then derive a scalable stochastic gradient descent algorithm for the resulting computer vision . we also derive an efficient non-linear extension of warca by using the kernel trick . kernel space embedding decouples the training and prediction costs from the data dimension and enables us to plug inarbitrary distance measures which are more natural for the features . we also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently . we validate this new method on nine standard person re-identification datasets including two large scale market-1501 and cuhk03 datasets and show that we improve upon the current state-of-the-art methods on all of them .
the goal of this paper is to propose the optimal statistical test based on the modeling of discrete cosine transform coefficients with a quantified laplacian distribution . this paper focuses on the detection of hidden information embedded in bits of the dct coefficients of a jpeg image . this detection of hidden information is difficult , in terms of statistical decision , for two main reasons : first , the number of dct coefficients used to conceal the hidden bits is random ; second , the jpeg image compression induces a strong quantization of dct coefficients . the proposed test explicitly takes into account the randomness of the number of dct coefficients used . it maximizes the probability of hidden information detection by ensuring a prescribed level of false alarm .
image denoising is an under-determined problem , and hence it is important to define appropriate image priors for regularization . one recent popular prior is the graph laplacian regularizer , where a given pixel patch is assumed to be smooth in the graph-signal domain . the strength and direction of the resulting image priors are computed from the graph 's edge weights . in this paper , we derive the optimal edge weights for local graph-based filtering using gradient estimates from non-local pixel patches that are self-similar . to analyze the effects of the gradient estimates on the graph laplacian reg-ularizer , we first show theoretically that , given graph-signal h d is a set of discrete samples on continuous function h -lrb- x , y -rrb- in a closed region ω , graph laplacian regularizer -lrb- h d -rrb- t lh d converges to a continuous functional sω integrating gradient norm of h in metric space g -- i.e. , -lrb- ∇ h -rrb- t g − 1 -lrb- ∇ h -rrb- -- over ω . we then derive the optimal metric space g : one that leads to a graph laplacian regu-larizer that is discriminant when the gradient estimates are accurate , and robust when the gradient estimates are noisy . finally , having derived g we compute the corresponding edge weights to define the laplacian l used for filtering . experimental results show that our image denoising algorithm using the per-patch optimal metric space g outperforms non-local means by up to 1.5 db in non-local means .
we propose neural responding machine , a neural network-based response generator for short-text conversation . neural responding machine takes the general encoder-decoder framework : neural responding machine formalizes the generation of response as a decoding process based on the latent representation of the input text , while both encoding and decoding are realized with recurrent neural networks . the neural responding machine is trained with a large amount of one-round conversation data collected from a microblogging service . empirical study shows that neural responding machine can generate grammatically correct and content-wise appropriate responses to over 75 % of the input text , outperforming state-of-the-arts in the same setting , including retrieval-based and smt-based models .
we propose robirank , a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification . robirank shows competitive performance on standard benchmark datasets against a number of other representative algorithms in the literature . we also discuss extensions of robirank to large scale problems where explicit feature vectors and scores are not given . we show that robirank can be efficiently parallelized across a large number of machines ; for a task that requires 386 , 133 × 49 , 824 , 519 pairwise interactions between items to be ranked , robi-rank finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm , given the same amount of wall-clock time for computation .
in recent years , planning and scheduling research has paid increasing attention to problems that involve resource over-subscription , where cumulative demand for resources out-strips their availability and some subset of goals or tasks must be excluded . two basic classes of techniques to solve oversubscribed scheduling problems have emerged : searching directly in the space of possible schedules and searching in an alternative space of task permutations -lrb- by relying on a schedule builder to provide a mapping to schedule space -rrb- . in some problem contexts , permutation-based search methods have been shown to outperform schedule-space search methods , while in others the opposite has been shown to be the case . we consider two techniques for which this behavior has been observed : taskswap , a schedule-space repair search procedure , and squeaky wheel optimization , a permutation-space scheduling procedure . we analyze the circumstances under which one can be expected to dominate the other . starting from a real-world scheduling problem where squeaky wheel optimization has been shown to outperform taskswap , we construct a series of problem instances that increasingly incorporate characteristics of a second real-world scheduling problem , where taskswap has been found to outperform squeaky wheel optimization . experimental results provide insights into when schedule-space methods and permutation-based methods may be most appropriate .
this paper describes our efforts towards cross-domain acoustic training for large vocabulary continuous speech recognition -lrb- lvcsr -rrb- systems . we used weighted multi-style training by pooling insufficient telephony landline and cellular data with down sampled wide band clean data to develop better hybrid acoustic models . we explored the effects on decision tree size to accuracy by approximately 10 % . the results show that by fixing number of parameters , system with smaller number of context dependent hmm states yields better accuracy . it leads to a smaller phone set design . we then investigated the performance degradation on two reduced phone sets for spanish . based on these studies , we are able to develop a hybrid system for 8khz closing talking microphone , telephony landline and cellular phone environments . the hybrid system is evaluated on both flat grammars , digit and name at department , and language model tasks , atis and general dictation , using the ibm viavoice product engine .
for many election systems , bribery -lrb- and related -rrb- attacks have been shown np-hard using constructions on combinatorially rich structures such as partitions and covers . this paper shows that for voters who follow the most central political-science model of electorates -- single-peaked preferences -- those hardness protections vanish . by using single-peaked preferences to simplify combinatorial covering challenges , we for the first time show that np-hard bribery problems -- including those for kemeny and llull elections -- fall to polynomial time for single-peaked electorates . by using single-peaked preferences to simplify combinatorial covering challenges , we for the first time show that np-hard bribery problems fall to polynomial time for single-peaked electorates . we show that for single-peaked electorates , the winner problems for dodgson and kemeny elections , though θ p 2-complete in the general case , fall to polynomial time . and we completely classify the complexity of weighted coalition manipulation for scoring protocols in single-peaked electorates .
this paper proposes and evaluates a new spectral smoothing technique whose performance is comparable with lsp interpolation in terms of euclidean spectral distance measurements but whose interpolated formant trajectories are more reasonable from a phonetic point of view . the spectral smoothing technique firstly estimates derivative logarithmic magnitude spectra from both the source and the target frame represented by autoregressive filter coefficients . then , dynamic programming yields the best alignment between these two spectral representations . smoothed frequency responses are achieved by weighted linear interpolation between the corresponding source and target spectral lines whose alignment was found by dp backtracking . finally , the spectrum is converted to autoregressive filter coefficients with the intermediate stage of autocorrelation coefficients .
this paper proposes a new approach to learning a discrimi-native model of object classes , incorporating appearance , shape and context information efficiently . the learned model is used for automatic visual recognition and semantic segmentation of photographs . our dis-criminative model exploits novel features , based on textons , which jointly model shape and texture . unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes . accurate image segmentation is achieved by incorporating these classifier in a conditional random field . efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods . high classification and segmentation accuracy are demonstrated on three different databases : i -rrb- our own 21-object class database of photographs of real objects viewed under general lighting conditions , poses and viewpoints , ii -rrb- the 7-class corel subset and iii -rrb- the 7-class sowerby database used in -lsb- 1 -rsb- . the proposed algorithm gives competitive results both for highly textured -lrb- e.g.
ultimately being motivated by facilitating space-variant blind deconvolution , we present a class of linear transformations , that are expressive enough for space-variant filters , but at the same time especially designed for efficient matrix-vector-multiplications . successful results on astronomical imaging through atmospheric turbulences and on noisy magnetic resonance images of constantly moving objects demonstrate the practical significance of our approach .
in this paper , the speech production process state is defined by a number of categorical articulatory features . we describe a detector that outputs a stream -lrb- sequence of classes -rrb- for each articulatory feature given the mel frequency cepstral coefficient representation of the input speech . the detector consists of a bank of recurrent neural network classifiers , a variable depth lattice generator and viterbi decoder . a bank of classifiers has been previously used for articulatory feature detection by many researchers . however , we extend their work first by creating variable depth lattices for each feature and then by combining variable depth lattices into product lattices for rescoring using the viterbi algorithm . during the rescor-ing we incorporate language and duration constraints along with the posterior probabilities of classes provided by the mel frequency cepstral coefficient representation . we present our results for place and manner features using timit data , and compare the results to a baseline system . we report performance improvements both at the frame and segment levels .
this paper proposes a new spatio-temporal resolution enhancement method of video sequences based on super-resolution reconstruction . the proposed method derives a new observation model based on feature point correspondence between successive frames . the observation model is defined by including the motion estimation function in computing the warping matrix . also , a new constraint is introduced to the optimization formula for estimating the parameters of the observation model , in order to achieve effective resolution-enhancement . by the newly obtained matrix and the new constraint introduction make accurate high-resolution and high frame rate video sequences . simulation results are shown to confirm the high performance of the proposed method .
the ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning . in real-world applications , using explicit user feedback as the reward signal is often unreliable and costly to collect . this problem can be mitigated if the user 's intent is known in advance or data is available to pre-train a task success predictor off-line . in practice neither of these apply for most real world applications . here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a gaussian process model . this on-line learning framework operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder . the experimental results demonstrate that the proposed on-line learning framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning .
in the search for adaptive representation of speech signals , the wavelet packet decomposition has been proved to be a eecient tool because of its frequency adaptation skills through the best basis search algorithm . the en-tropic minimization of this algorithm is bounded by two artifacts : the dyadic structure of the decomposition and the lack of temporal segmentation . we propose here a low cost extended tree in the wavelet packet decomposition which improves the best basis search by reducing the entropy of the base and which is still compatible with the classical wpd . the decomposition also allows perfect reconstruction . the entropic test is updated to take into account the new basis . the preliminary use of a temporal segmentation , based on the local entropic criterion highly improves the entropic gain of the global analysis . the results are shown on experimental speech signals comparing the gain of our scheme versus a usual wavelet packet decomposition .
speech utterance classification has been widely applied to a variety of spoken language understanding tasks , including call routing , dialog systems , and command and control . most speech utterance classification systems adopt a data-driven statistical learning approach , which requires manually transcribed and annotated training data . in this paper we introduce a novel classification model training approach based on unsupervised language model adaptation . classification model training approach only requires wave files of the training speech utterances and their corresponding classification destinations for modeling training . no manual transcription of the utterances is necessary . experimental results show that this classification model training approach , which is much cheaper to implement , has achieved classification accuracy at the same level as the model trained with manual transcriptions .
we present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resource-rich language . we train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy reg-ularization . our method can be used as a purely monolingual dependency parser , requiring no human translations for the test data , thus making probabilistic parsing models applicable to a wide range of resource-poor languages . we perform experiments on three data sets -- version 1.0 and version 2.0 of google universal dependency treebanks and treebanks from conll shared-tasks , across ten languages . we obtain state-of-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems .
emotions are an important part of human communication and are expressed both verbally and non-verbally . common non-verbal vocalizations such as laughter , cries and sighs carry important emotional content in conversations . sighs often are associated with negative emotion . in this work , we show that emotional sighs exist along both ends of the valence axis -lrb- positive-emotion vs. negative-emotion sighs -rrb- in spontaneous affective dialogs and that they have certain distinct multimodal characteristics . classification results show that it is possible to differentiate between the two types of emotionally valenced sighs , using a combination of acoustic and gestural features with an overall unweighted accuracy of 58.26 % .
in this paper we are presenting a novel multivariate analysis method . our multivariate analysis method is based on a novel kernel orthonormalized partial least squares -lrb- pls -rrb- variant for feature extraction , imposing sparsity constrains in the solution to improve scalabil-ity . the multivariate analysis method is tested on a benchmark of uci data sets , and on the analysis of integrated short-time music features for genre prediction . the upshot is that the multivariate analysis method has strong expressive power even with rather few features , is clearly outperforming the ordinary kernel pls , and therefore is an appealing multivariate analysis method for feature extraction of labelled data .
multiple measurement vector -lrb- mmv -rrb- is a newly emerged problem in sparse representation in an over-complete dictionary ; it poses new challenges . efficient methods have been designed to search for sparse representations -lsb- 1 , 2 -rsb- ; however , we have not seen substantial development in the theoretical analysis , considering what has been done in a simpler case -- single measurement vector -- in which many theoretical results are known , e.g. , -lsb- 9 , 3 , 4 , 5 , 6 -rsb- . this paper extends the known results of multiple measurement vector to mmv . our theoretical results show the fundamental limitation on when a sparse representation is unique . moreover , the relation between the solutions of 0-norm minimization and the solutions of 1-norm minimization indicates a compu-tationally efficient approach to find a sparse representation . interestingly , simulations show that the predictions made by these theorems tend to be conservative .
this paper presents an investigation into the use of adapted gaussian mixture models in the context of open-set , text-independent speaker identification . the study includes a scheme for using the fast-scoring method which has been proposed for speaker verification . furthermore , it provides an evaluation of various score normalisation methods in the proposed osti-si framework . the dataset used for the experimental investigation is based on nist sre2003 1-speaker detection task . it is shown that significant improvements can be achieved if only a single mixture is used in the fast-scoring technique . furthermore , it is experimentally observed that comparable performance is obtained using unconstrained co-hort normalisation , t-norm and tz-norm . the paper provides a detailed description of the experimental set up , and presents an analysis of the results obtained .
we performed automated feature selection for multi-stream -lrb- i.e. , ensemble -rrb- automatic speech recognition , using a hill-climbing algorithm that changes one feature at a time if the change improves a performance score . for both clean and noisy data sets -lrb- using the ogi numbers corpus -rrb- , hc scripts usually improved performance on held out data compared to the initial system it started with , even for noise types that were not seen during the hc process . overall , we found that using opitz 's scoring formula , which blends single-classifier word recognition accuracy and ensemble diversity , worked better than ensemble accuracy as a performance score for guiding hc scripts in cases of extreme mismatch between the snr of training and test sets . our noisy version of the ogi numbers corpus , our multi-layer-perceptron-based numbers asr system , and our hc scripts are available online .
robocup challenge oers a set of challenges for intelligent agent researchers using a friendly competition in a dynamic , real-time , multi-agent domain . while robocup challenge oers in general envisions longer range challenges over the next few decades , robocup challenge oers presents three specic challenges for the next two years : -lrb- i -rrb- learning of individual agents and teams ; -lrb- ii -rrb- multi-agent team planning and plan-execution in service of teamwork ; and -lrb- iii -rrb- opponent mod-eling . robocup challenge oers provides a novel opportunity for machine learning , planning , and multi-agent researchers | robocup challenge oers not only supplies a concrete domain to evalute their techniques , but also challenges researchers to evolve these techniques to face key constraints fundamental to this domain : real-time , uncertainty , and teamwork .
the paper presents the speaker normalization technique we implemented in a teaching and training system for hearing handicapped children with the goal to reduce inter-speaker variability in time-frequency speech representation . in an effort to reduce variance caused by variation in vocal tract shape among speakers , a formant based nonlinear frequency warping approach to vocal tract normalization is investigated . the proposed speaker normalization technique can be efficiently realized in an analysis by synthesis framework . after the speech decomposition into the vocal tract envelope and excitation model , the vocal tract envelope is warped by the estimated frequency warping function , while the excitation characteristics are mapped to the reference speaker excitation . the results have shown significant spectral distance decrease for correctly pronounced words between test and the reference speaker after the normalization has been applied , while for poor pronunciation by the test speaker the spectral distance remains relatively high .
point-based algorithms and rtdp-bel are approximate methods for solving rtdp-bel that replace the full updates of parallel value iteration by faster and more effective updates at selected beliefs . an important difference between the two methods is that the former adopt sondik 's representation of the value function , while the latter uses a tabular representation and a discretization function . the algorithms , however , have not been compared up to now , because they target different rtdp-bel : discounted pomdps on the one hand , and goal pomdps on the other . in this paper , we bridge this representational gap , showing how to transform discounted pomdps into goal pomdps , and use the transformation to compare rtdp-bel with point-based algorithms over the existing discounted pomdps . the results appear to contradict the conventional wisdom in the area showing that rtdp-bel is competitive , and sometimes superior to point-based algorithms in both quality and time .
this paper presents the chinese word segmentation system developed by nokia research center -lrb- nrc -rrb- , which was evaluated in the fourth international chinese language processing bakeoff and the first cips chinese language processing evaluation organized by sighan . in our chinese word segmentation system , a preprocessing module was used to discover the out-of-vocabulary words which occur repeatedly in the text , then an improved n-gram model was used for segmentation and some post processing strategies are adopted in chinese word segmentation system to recognize the organization names and new words . we took part in three tracks , which are called the open and closed track on corpora state language commission of p.r.c. , and closed track on corpora shanxi university -lrb- sxu -rrb- . our chinese word segmentation system achieved good performance , especially in the open track on state language commission of p.r.c. , our chinese word segmentation system ranks 1 st among 11 systems .
recent local stereo matching algorithms based on an adaptive-weight strategy achieve accuracy similar to global approaches . one of the major problems of these local stereo matching algorithms is that they are computationally expensive and this complexity increases proportionally to the window size . this paper proposes a novel cost aggregation step with complexity independent of the window size -lrb- i.e. o -lrb- 1 -rrb- -rrb- that outperforms state-of-the-art o methods . moreover , compared to other o approaches , our cost aggregation step does not rely on integral his-tograms enabling aggregation using colour images instead of grayscale ones . finally , to improve the results of the proposed cost aggregation step a disparity refinement pipeline is also proposed . the overall cost aggregation step produces results comparable to those of state-of-the-art stereo matching algorithms .
semantic frames are a rich linguistic resource . there has been much work on semantic frame parsers , but less that applies them to general nlp problems . we address a task to predict change in stock price from financial news . semantic frames help to generalize from specific sentences to scenarios , and to detect the -lrb- positive or negative -rrb- roles of specific companies . we introduce a novel tree representation , and use tree representation to train predic-tive models with tree kernels using support vector machines . our experiments test multiple text representations on two binary classification tasks , change of price and polarity . experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task .
nonnegative matrix factorization -lrb- nmf -rrb- is a powerful tool for decomposing mixtures of audio signals in the time-frequency domain . in applications such as source separation , the phase recovery for each extracted component is a major issue since it often leads to audible artifacts . in this paper , we present a methodology for evaluating various nmf-based source separation techniques involving phase recovery . for each model considered , a comparison between two approaches -lrb- blind separation without prior information and oracle separation with supervised model learning -rrb- is performed , in order to inquire about the room for improvement for the estimation methods . experimental results show that the high resolution nmf -lrb- hrnmf -rrb- model is particularly promising , because it is able to take phases and correlations over time into account with a great expressive power .
in this paper , we present a node dependent wavelet threshold-ing approach in order to remove strongly coloured noises from speech signals . the noise power in each node is first estimated using a recursive method . given the voiced or unvoiced nature of the frame , the signal is expanded onto a predefined best basis . then a infinitely smooth soft threshold is applied depending on each node of the decomposition tree . finally the estimated clean signal is reconstructed . experimental results on a japanese database , for various coloured noises , demonstrate the effectiveness of the proposed node dependent wavelet threshold-ing approach , even at low snr . compared with the common level dependent method , this node dependent wavelet threshold-ing approach provides better denoising results .
we propose a new ngca -lrb- non-gaussian component analysis -rrb- for dimension reduction to identify non-gaussian components in high dimensional data . our ngca -lrb- non-gaussian component analysis -rrb- , ngca -lrb- non-gaussian component analysis -rrb- , uses a very general semi-parametric framework . in contrast to existing projection methods we define what is uninteresting -lrb- gaussian -rrb- : by projecting out uninterestingness , we can estimate the relevant non-gaussian subspace . we show that the estimation error of finding the non-gaussian components tends to zero at a paramet-ric rate . once ngca components are identified and extracted , various tasks can be applied in the data analysis process , like data visualization , clustering , denoising or classification . a numerical study demonstrates the usefulness of our ngca -lrb- non-gaussian component analysis -rrb- .
tone mapping operators -lrb- tone mapping operators -rrb- that convert high dynamic range -lrb- hdr -rrb- images to standard low dynamic range -lrb- ldr -rrb- images are highly desirable for the visualization of these images on standard displays . although many existing tone mapping operators produce visually appealing images , it is until recently validated objective measures that can assess their quality have been proposed . without such objective measures , the design of traditional tone mapping operators can only be based on intuitive ideas , lacking clear goals for further improvement . in this paper , we propose a substantially different tone mapping approach , where instead of explicitly designing a new computational structure for tone mapping operators , we search in the space of images to find better quality images in terms of a recent objective measure that can assess the structural fidelity between two images of different dynamic ranges . specifically , starting from any initial image , the proposed algorithm moves the image along the gradient ascent direction and stops until it converges to a maximal point . our experiments show that the proposed algorithm reliably produces better quality images upon a number of state-of-the-art tone mapping operators .
when automatically translating from a weakly inflected source language like english to a target language with richer grammatical features such as gender and dual number , the output commonly contains morpho-syntactic agreement errors . to address this issue , we present a target-side , class-based agreement model . agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis . for english-to-arabic translation , our target-side , class-based agreement model yields a +1.04 bleu average improvement over a state-of-the-art baseline . the target-side , class-based agreement model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders .
articulated objects represent an important class of objects in our everyday environment . automatic detection of the type of articulated or otherwise restricted motion and extraction of the corresponding motion parameters are therefore of high value , e.g. in order to augment an otherwise static 3d reconstruction with dynamic semantics , such as rotation axes and allowable translation directions for certain rigid parts or objects . hence , in this paper , a novel theory to analyse relative transformations between two motion-restricted parts will be presented . the analysis is based on linear subspaces spanned by relative transformations . moreover , a signature for relative transformations will be introduced which uniquely specifies the type of restricted motion encoded in these relative transformations . this theoretic framework enables the derivation of novel algebraic constraints , such as low-rank constraints for subsequent rotations around two fixed axes for example . lastly , given the type of restricted motion as predicted by the signature , the paper shows how to extract all the motion parameters with matrix manipulations from linear algebra . our theory is verified on several real data sets , such as a rotating blackboard or a wheel rolling on the floor amongst others .
in formating temporal sequences of notes played by the same instrument -lrb- referred to as music streams -rrb- , timbre of musical instruments may be a predominant feature . in polyphonic music , the performance of timber extraction based on power-related features deteriorates , because such features are blurred when two or more frequency components are superimposed in the same frequency . to cope with this problem , we integrated timbre similarity and direction proximity with success , but left using other features as future work . in this paper , we investigate four features , timbre similarity , direction proximity , pitch transition and pitch relation consistency to clarify the precedence among them in music stream formation . experimental results with quartet music show that direction proximity is the most dominant feature , and pitch transition is the secondary . in addition , the performance of music stream formation was improved from 63.3 % by only timbre similarity to 84.9 % by integrating four features .
we describe and evaluate a simple method to extract parallel sentences from comparable corpora . the approach , termed stacc , is based on expanded lexical sets and the jaccard similarity coefficient . we evaluate our stacc against state-of-the-art methods on a large range of datasets in different domains , for ten language pairs , showing that stacc either matches or outper-forms current methods across the board and gives significantly better results on the noisiest datasets . stacc is a portable method , requiring no particular adaptation for new domains or language pairs , thus enabling the efficient mining of parallel sentences in comparable corpora .
in this paper , we survey the current state-of-art models for structured learning problems , including hidden markov model , conditional random fields , averaged perceptron , structured svms -lrb- <i> svm <sup> struct </sup> -rrb- </i> , max margin markov networks -lrb- m <sup> 3 </sup> n -rrb- , and an integration of search and learning algorithm -lrb- searn -rrb- . with all due tuning efforts of various parameters of each state-of-art models , on the data sets we have applied the state-of-art models to , we found that svm <i> <sup> struct </sup> </i> enjoys better performance compared with the others . in addition , we also propose a new method which we call the structured learning ensemble -lrb- sle -rrb- to combine these structured learning models . empirical results show that our sle algorithm provides more accurate solutions compared with the best results of the individual state-of-art models .
we introduce dramatis , a computational model of suspense based on a reformulation of a psychological definition of the suspense phenomenon . in this reformulation , suspense is correlated with the audience 's ability to generate a plan for the protagonist to avoid an impending negative outcome . dramatis measures the suspense level by generating such a plan and determining its perceived likelihood of success . we report on three evaluations of dramatis , including a comparison of dramatis output to the suspense reported by human readers , as well as ablative tests of dramatis components . in these studies , we found that dramatis output corresponded to the suspense ratings given by human readers for stories in three separate domains .
we study stochastic optimization problems when the data is sparse , which is in a sense dual to current perspectives on high-dimensional statistical learning and optimization . we highlight both the difficulties -- in terms of increased sample complexity that sparse data necessitates -- and the potential benefits , in terms of allowing parallelism and asynchrony in the design of algorithms . concretely , we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data , and we exhibit algorithms achieving these rates . we also show how leveraging sparsity leads to -lrb- still minimax optimal -rrb- parallel and asynchronous algorithms , providing experimental evidence complementing our theoretical results on several medium to large-scale learning tasks .
multitask learning algorithms are typically designed assuming some fixed , a priori known latent structure shared by all the tasks . however , flexible , nonparametric bayesian model is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem . ideally , the '' right '' latent task structure should be learned in a data-driven manner . we present a flexible , nonparametric bayesian model that posits a mixture of factor analyzers structure on the tasks . the nonparametric aspect makes the flexible , nonparametric bayesian model expressive enough to subsume many existing flexible , nonparametric bayesian model of latent task structures -lrb- e.g , mean-regularized tasks , clustered tasks , low-rank or linear/non-linear subspace assumption on tasks , etc. -rrb- . moreover , flexible , nonparametric bayesian model can also learn more general task structures , addressing the shortcomings of such flexible , nonparametric bayesian model . we present a vari-ational inference algorithm for our flexible , nonparametric bayesian model . experimental results on synthetic and real-world datasets , on both regression and classification problems , demonstrate the effectiveness of the proposed vari-ational inference algorithm .
the performance of wiener filters in restoring the quality and intelligibility of noisy speech depends on : -lrb- i -rrb- the accuracy of the estimates of the power spectra or the correlation values of the noise and the speech processes , and -lrb- ii -rrb- on the wiener filter structure . in this paper a bayesian method is proposed where model combination and model decomposition are employed for the estimation of parameters required to implement subband wiener filters . the use of subband wiener filters provides advantages in terms of improved parameter estimates and also in restoring the temporal-spectral composition of speech . the bayesian method is evaluated , and compared with the parallel model combination , using the timit continuous speech database with bmw and volvo car noise databases .
talkers adopt different speech styles in response to factors such as the perceived needs of the interlocutor , environmental noise and explicit instruction . some styles have been shown to be beneficial for listeners but many aspects of the relationship between speech modifications and intelligibility remain unclear , particularly for prosodic changes . the current study measures the relative intelligibility in noise of speech spoken in 5 speech styles -- plain , infant - , computer-and foreigner-directed , and shouted -- and relates listener scores to acous-tic/prosodic parameters and quantitative estimates of energetic masking . intelligibility changes over plain speech correlated well with durational modifications , which included elongations of all segments as well as increases in the number of unfilled pauses . both mean fundamental frequency and its range displayed great variation across styles but with no clear intelligibility benefits . energetic masking per unit time was similar in each style but the total amount of speech which escaped masking was a good predictor of word identification rate . these findings suggest that much of the prosody-related intelligibility gain is derived from durational increases .
achieving computer vision on micro-scale devices is a challenge . on these computer vision , the power and mass constraints are severe enough for even the most common computations -lrb- matrix manipulations , convolution , etc. -rrb- to be difficult . this paper proposes and analyzes a class of miniature vision sensors that can help overcome these constraints . these miniature vision sensors reduce power requirements through template-based optical convolution , and miniature vision sensors enable a wide field-of-view within a small form through a novel optical design . we describe the trade-offs between the field of view , volume , and mass of these miniature vision sensors and we provide analytic tools to navigate the design space . we also demonstrate milli-scale prototypes for computer vision tasks such as locating edges , tracking targets , and detecting faces .
we develop an intuitive geometric interpretation of the standard support vector machine for classification of both linearly separable and inseparable data and provide a rigorous derivation of the concepts behind the geometry . for the separable case finding the maximum margin between the two sets is equivalent to finding the closest points in the smallest convex sets that contain each class -lrb- the convex hulls -rrb- . we now extend this argument to the inseparable case by using a reduced convex hull formulation reduced away from out-liers . we prove that solving the reduced convex hull formulation is exactly equivalent to solving the standard inseparable svm for appropriate choices of parameters . some additional advantages of the new formulation are that the effect of the choice of parameters becomes geometrically clear and that the formulation may be solved by fast nearest point algorithms . by changing norms these arguments hold for both the standard 2-norm and 1-norm svm .
for domain-specific speech recognition tasks , it is best if the statistical language model component is trained with text data that is content-wise and style-wise similar to the targeted domain for which the application is built . for state-of-the-art language modeling techniques that can be used in real-time within speech recognition engines during first-pass decoding -lrb- e.g. , n-gram models -rrb- , the above constraints have to be fulfilled in the training data . however collecting such data , even through crowd sourcing , is expensive and time consuming , and can still be not representative of how a much larger user population would interact with the recognition system . in this paper , we address this problem by employing several semantic web sources that already contain the domain-specific knowledge , such as query click logs and knowledge graphs . we build statistical language models that meet the requirements listed above for domain-specific speech recognition tasks where natural language is used and the user queries are about name entities in a specific domain . as a case study , in the movies domain where users ' voice queries are movie related , compared to a generic web language model , a language model trained with the above resources not only yields significant perplexity and word-error-rate improvements , but also presents an approach where such language model can be rapidly developed for other domains .
in this paper we study large-scale optimization problems in multi-view geometry , in particular the bundle adjustment problem . in its conventional formulation , the complexity of existing solvers scale poorly with problem size , hence this component of the structure-from-motion pipeline can quickly become a bottleneck . here we present a novel formulation for solving bundle adjustment in a truly distributed manner using consensus based optimization methods . our algorithm is presented with a concise derivation based on proximal splitting , along with a theoretical proof of convergence and brief discussions on complexity and implementation . experiments on a number of real image datasets convincingly demonstrates the potential of the proposed method by outperforming the conventional bundle adjustment formulation by orders of magnitude .
this paper addresses the well-established problem of un-supervised object discovery with a novel method inspired by weakly-supervised approaches . in particular , the ability of an object patch to predict the rest of the object -lrb- its context -rrb- is used as supervisory signal to help discover visually consistent object clusters . the main contributions of this work are : 1 -rrb- framing unsupervised clustering as a leave-one-out context prediction task ; 2 -rrb- evaluating the quality of context prediction by statistical hypothesis testing between thing and stuff appearance models ; and 3 -rrb- an iterative region prediction and context alignment approach that gradually discovers a visual object cluster together with a segmentation mask and fine-grained correspondences . the proposed method outperforms previous unsupervised as well as weakly-supervised object discovery approaches , and is shown to provide correspondences detailed enough to transfer keypoint annotations .
with long-span neural network language models , considerable improvements have been obtained in speech recognition . however , it is difficult to apply these models if the underlying search space is large . in this paper , we combine previous work on lattice decoding with long short-term memory -lrb- lstm -rrb- neural network language models . by adding refined pruning techniques , we are able to reduce the search effort by a factor of three . furthermore , we introduce two novel approximations for full lattice rescoring , which opens the potential of lattice-based speech recognition techniques . compared to 1000-best lists , we find that we can increase the word error rate improvements obtained with lstms from 8.2 % to 10.7 % relative over a state-of-the-art baseline , while the resulting lattices are even considerably smaller . in addition , we investigate the use of lstms for babel assamese keyword search , obtaining significant improvements of 2.5 % relative .
we present a representational format for observed movements the representation has a temporal structure relating components of a single complex movement . we also present oxbow , an unsupervised learning system , which constructs classes of these movements . empirical results indicate that the unsupervised learning system builds abstract movement concepts with appropriate component structure allowing unsupervised learning system to predict the latter portions of a partially observed movement .
in this paper we extend the classical notion of strong and weak backdoor sets by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes ; the union of the base classes forms a heterogeneous base class . backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones , hence they are much more desirable but possibly harder to find . we draw a detailed complexity landscape for the problem of detecting strong and weak backdoor sets into heterogeneous base classes for sat and csp .
we introduce a new methodology for radiometric reconstruction from multiple images . it opens new possibilities because it allows simultaneous recovery of varying unknown illuminants -lrb- one per image -rrb- , surface albedoes , and cameras ' radiometric responses . designed to complement geometric reconstruction techniques , it only requires as input the geometry of the scene and of the cameras . unlike photometric stereo approaches , it is not restricted to images taken from a single viewpoint . linear and non-linear implementations in the lambertian case are proposed ; simulation results are discussed and compared to related work to demonstrate the gain in stability ; and results on real images are shown .
the purpose of room impulse response shortening is to improve the intelligibility of the received signal by pre-filtering the source signal before it is played with a loudspeaker in a closed room . in this paper , we propose to use the infinity-norm as optimization criterion for the design of shortening filters of room impulse response shortening . similar to the equiripple filter design method , design errors will be uniformly distributed over the unwanted temporal range of the shortened global impulse response . the d50 measure is exploited during the design of the shortening filter , which makes it possible to significantly reduce the length of the prefilter without affecting the perceived performance .
many classification problems have the property that the only costly part of obtaining examples is the class label . this paper suggests a simple method for using distribution information contained in unlabeled examples to augment labeled examples in a supervised training framework . empirical tests show that the technique described in this paper can significantly improve the accuracy of a supervised learner when the learner is well below its asymptotic accuracy level .
we show that a classifier based on gaussian mixture models can be trained dis-criminatively to improve accuracy . we describe a training procedure based on the extended baum-welch algorithm used in speech recognition . we also compare the accuracy and degree of sparsity of the new discrimi-native gmm classifier with those of genera-tive gmm classifiers , and of kernel classifiers , such as support vector machines and relevance vector machines .
this paper introduces a novel form of parametric synthesis that uses context embeddings produced by the bottleneck layer of a deep neural network to guide the selection of models in a rich-context hmm-based synthesiser . rich-context synthesis -- in which gaussian distributions estimated from single linguistic contexts seen in the training data are used for synthesis , rather than more conventional decision tree-tied models -- was originally proposed to address over-smoothing due to averaging across contexts . our previous investigations have confirmed experimentally that averaging across different contexts is indeed one of the largest factors contributing to the limited quality of statistical parametric speech synthesis . however , a possible weakness of the rich context approach as previously formulated is that a conventional tied model is still used to guide selection of gaussians at synthesis time . our proposed tied model replaces parametric synthesis with context embeddings derived from a neural network .
given a u-invariant sampling scheme on an arbitrary hilbert space h . this paper characterizes atomic subspaces a of h such that every signal x ∈ a can be reconstructed from its samples acquired with this u-invariant sampling scheme . if signal recovery is possible a linear filter is derived which reconstructs the signal from the samples .
we present ongoing doctoral work on automatically understanding the positions of politicians with respect to those of the party they belong to . to this end , we use textual data , namely transcriptions of political speeches from meetings of the ger-man bundestag , and party manifestos , in order to automatically acquire the positions of political actors and parties , respectively . we discuss a variety of possible supervised and unsupervised approaches to determine the topics of interest and compare positions , and propose to explore an approach based on topic modeling techniques for these tasks .
spoken language understanding -lrb- spoken language understanding -rrb- addresses the problem of extracting semantic meaning conveyed in an utterance . the traditional knowledge-based approach to this problem is very expensive-it requires joint expertise in natural language processing and speech recognition , and best practices in language engineering for every new domain . on the other hand , a statistical learning approach needs a large amount of annotated data for model training , which is seldom available in practical applications outside of large research labs . a generative hmm/cfg composite model , which integrates easy-to-obtain domain knowledge into a data-driven statistical learning framework , has previously been introduced to reduce data requirement . the major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework . we also study and compare conditional random fields with perceptron learning for spoken language understanding . experimental results show that the conditional model framework achieve more than 20 % relative reduction in slot error rate over the hmm/cfg model , which had already achieved an slu accuracy at the same level as the best results reported on the atis data .
the miniaturization required for interfacing with the brain demands new methods of transforming neuron responses -lrb- spikes -rrb- into digital representations . the sparse nature of neu-ral recordings is evident when represented in a shift invariant basis . although a compressive sensing framework may seem suitable in reducing the data rates , we show that the time varying sparsity in the signals makes adaptive sampling scheme difficult to apply . furthermore , we present an adaptive sampling scheme which takes advantage of the local characteristics of the neural spike trains and electrocardiograms -lrb- ecg -rrb- . in contrast to the global constraints imposed in cs our adaptive sampling scheme is sensitive to the local time structure of the input . the simplicity in the design of the integrate-and-fire make adaptive sampling scheme a viable adaptive sampling scheme in current brain machine interfaces and ambulatory cardiac monitoring .
cooperative intelligent transportation systems -lrb- cooperative intelligent transportation systems -rrb- are complex systems well-suited to a multi-agent mod-eling . we propose a multi-agent based modeling of a cooperative intelligent transportation systems , that couples 3 dynamics -lrb- physical , informational and control dynamics -rrb- in order to ensure a smooth cooperation between non cooperative and cooperative vehicles , that communicate with each other -lrb- v2v communication -rrb- and the infrastructure -lrb- i2v and v2i communication -rrb- . we present our multi-agent based modeling , tested through simulations using real traffic data and integrated into our extension of the multi-model open-source vehicular-traffic simulator .
most algorithms dedicated to the generation of referential descriptions widely suffer from a fundamental problem : they make too strong assumptions about adjacent processing components , resulting in a limited coordination with their perceptive and linguistics data , that is , the provider for object descriptors and the lexical expression by which the chosen descriptors is ultimately realized . motivated by this deficit , we present a new algorithm that -lrb- 1 -rrb- allows for a widely unconstrained , incremental , and goal-driven selection of descriptors , -lrb- 2 -rrb- integrates linguistic constraints to ensure the expressibility of the chosen descriptors , and -lrb- 3 -rrb- provides means to control the appearance of the created referring expression . hence , the main achievement of our approach lies in providing a core algorithm that makes few assumptions about other processing components and improves the flow of control between modules .
by reducing optimization to a sequence of small subproblems , working set methods achieve fast convergence times for many challenging problems . despite excellent performance , theoretical understanding of working sets is limited , and implementations often resort to heuristics to determine subproblem size , makeup , and stopping criteria . we propose blitz , a fast working set algorithm accompanied by useful guarantees . making no assumptions on data , our theory relates subproblem size to progress toward convergence . this result motivates methods for optimizing algorithmic parameters and discarding irrelevant variables as iterations progress . applied to 1-regularized learning , blitz convincingly outperforms existing solvers in sequential , limited-memory , and distributed settings . blitz is not specific to 1-regularized learning , making the algorithm relevant to many applications involving sparsity or constraints .
a human can easily find his or her way in an unfamiliar building , by walking around and reading the floor-plan . we try to mimic and automate this human thinking process . more precisely , we introduce a new and useful task of locating an user in the floor-plan , by using only a camera and a floor-plan without any other prior information . we address the problem with a novel matching-localization algorithm that is inspired by human logic . we demonstrate through experiments that our method outperforms state-of-the-art floor-plan-based localization methods by a large margin , while also being highly efficient for real-time applications .
cl research began experimenting with massive xml tagging of texts to answer questions in trec 2002 . in duc 2003 , the experiments were extended into text summarization . based on these experiments , the knowledge management system was developed to combine these two capabilities and to serve as a unified basis for other types of document exploration . knowledge management system has been extended to include web question answering , both general and topic-based summarization , information extraction , and document exploration . the document exploration functionality includes identification of semantically similar concepts and dynamic ontology creation . as development of knowledge management system has continued , user modeling has become a key research issue : how will different users want to use the information they identify .
music recommendation is an important aspect of many streaming services and multi-media systems , however , music recommendation is typically based on so-called collaborative filtering methods . in this paper we consider the recommendation task from a personalized viewpoint and examine to which degree music preference can be elicited and predicted using simple and robust queries such as pairwise comparisons . we propose to model-and in turn predict-the pairwise music preference using a very flexible model based on gaussian process priors for which we describe the required inference . we further propose a specific covariance function and evaluate the pre-dictive performance on a novel dataset . in a recommendation setting we obtain a leave-one-out accuracy of 76 % compared to 50 % with random predictions , showing potential for further refinement and evaluation .
pose variation remains one of the major factors adversely affect the accuracy of real-world face recognition systems . inspired by the recently proposed probabilistic elastic part model and the success of the deep hierarchical architecture in a number of visual tasks , we propose the probabilistic elastic part model to approach the uncon-strained face recognition problem . we apply the probabilistic elastic part model hierarchically to decompose a face image into face parts at different levels of details to build pose-invariant part-based face representations . following the hierarchy from bottom-up , we stack the face part representations at each layer , discriminatively reduce its dimensionality , and hence aggregate the face part representations layer-by-layer to build a compact and invariant face representation . the probabilistic elastic part model exploits the fine-grained structures of the face parts at different levels of details to address the pose variations . probabilistic elastic part model is also guided by supervised information in constructing the face part/face representations . we empirically verify the probabilistic elastic part model on two public benchmarks -lrb- i.e. , the lfw and youtube faces -rrb- and a face recognition challenge -lrb- i.e. , the pasc grand challenge -rrb- for image-based and video-based face verification . the state-of-the-art performance demonstrates the potential of our probabilistic elastic part model .
we investigate methods for planning in a markov decision process where the cost function is chosen by an adversary after we fix our policy . as a running example , we consider a robot path planning problem where costs are influenced by sensors that an adversary places in the environment . we formulate the robot path planning problem as a zero-sum matrix game where rows correspond to deterministic policies for the planning player and columns correspond to cost vectors the adversary can select . for a fixed cost vector , fast algorithms -lrb- such as value iteration -rrb- are available for solving mdps . we develop efficient algorithms for matrix games where such best response oracles exist . we show that for our robot path planning problem these algorithms are at least an order of magnitude faster than direct solution of the linear programming formulation .
in this paper , we propose a novel approach to automatic generation of aspect-oriented summaries from multiple documents . we first develop an event-aspect lda model to cluster sentences into aspects . we then use extended lexrank algorithm to rank the sentences in each cluster . we use integer linear programming for sentence selection . key features of our method include automatic grouping of semantically related sentences and sentence ranking based on extension of random walk model . also , we implement a new sentence compression algorithm which use dependency tree instead of parser tree . we compare our method with four baseline methods . quantitative evaluation based on rouge metric demonstrates the effectiveness and advantages of our method .
rainbows are a natural cue for calibrating outdoor imagery . while ephemeral , they provide unique calibration cues because they are centered exactly opposite the sun and have an outer radius of 42 degrees . in this work , we define the geometry of a rainbow and describe minimal sets of constraints that are sufficient for estimating camera calibration . we present both semi-automatic and fully automatic methods to calibrate a camera using an image of a rainbow . to demonstrate our semi-automatic and fully automatic methods , we have collected a large database of rainbow images and use these to evaluate calibration accuracy and to create an empirical semi-automatic and fully automatic methods of rainbow appearance . we show how this semi-automatic and fully automatic methods can be used to edit rainbow appearance in natural images and how rainbow geometry , in conjunction with a horizon line and capture time , provides an estimate of camera location . while we focus on rainbows , many of the geometric properties and algorithms we present also apply to other solar-refractive phenomena , such as parhelion , often called sun dogs , and the 22 degree solar halo .
features based on a hierarchy of neural networks with com-pressive layers -- stacked bottleneck features -- were recently shown to provide excellent performance in lvcsr systems . this paper summarizes several techniques investigated in our work towards babel 2014 evaluations : -lrb- 1 -rrb- using several versions of fundamental frequency estimates , -lrb- 2 -rrb- semi-supervised training on un-transcribed data and mainly -lrb- 3 -rrb- adapting the nn structure at different levels . they are tested on three 2014 babel languages with full gmm-and dnn-based systems . separately and in combination , they are shown to out-perform the baselines and confirm the usefulness of bottleneck features in current lvcsr systems .
standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages -lrb- which require large lexical tagset inventories -rrb- . for this reason , a number of alternative methods have been proposed over the years . one of the most successful methods used for this task , fdoohg7lhuhg7djjlqj7xil , 1999 -rrb- , exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions . a second phase is aimed at recovering the full set of morpho-syntactic features . in this paper we present an alternative method to tiered tagging , based on local optimizations with neural networks and we show how , by properly encoding the input sequence in a general neural network architecture , we achieve results similar to the tiered tagging methodology , significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method .
in this paper we present an approach to introducing more pho-netically motivated information into automatic speech recognition in the form of a phonetic ` expert ' . to avoid the curse of dimensionality problem , the expert information is introduced at the level of the acoustic model . two types of experts are used , each providing discriminative information regarding groups of phonetically related phonemes . the phonetic expert is implemented using an mlp . experiments on a numbers recognition task show that , when using the expert in conjunction with both a fullband and a multi-band system speech recognition performances are increased .
this paper addresses the recovery of structure and motion from two uncalibrated images of a scene under full perspective or under aane projection . epipolar geometry , projective reconstruction , and aane reconstruction are elaborated in a way such that everyone having knowledge of linear algebra can understand the discussion without diiculty . a general expression of the fundamental matrix is derived which is valid for any projection model without lens distortion -lrb- including full perspective and aane camera -rrb- . a new technique for aane reconstruction from two aane images is developed , which consists in rst estimating the aane epipolar geometry and then performing a triangulation for each point match with respect to an implicit common aane basis . this technique is very ef-cient .
it is known that american english / r / can be produced as a retroflex or bunched / r / , but it can be challenging to teach students how to articulate both . we already developed a physical model for retroflex / r / and demonstrated that the physical model produces the / r / sound . however , almost no studies have reported a physical model for bunched / r / . we developed a new physical model using sliding blocks for the lips and tongue to help teach students how to produce bunched / r / . we recorded several sets of sounds produced by the physical model , analyzed the output signals , and used physical model for perceptual experiments . acoustic analysis and perceptual experiments confirmed that the retroflex and bunched / r / models produced clear american / r / sounds , and that the narrow constriction placed between 5-7 cm from the lips seems to be the key in producing these sounds . furthermore , bunched / r / with lip rounding produced the most clear / r / sound . both physical model are helpful for practicing pronunciation because learners can readily see there are two ways to produce / r / , they can see and alter the tongue position manually , and they can hear the output sounds .
the noise predictive structure of dfe is attractive for the equalization of the coded modulation signals . in this paper , a blind predictive constant modulus -lrb- cm -rrb- decision feedback equalizer -lrb- pcm-dfe -rrb- is presented and analyzed . the pcm-dfe employs the cm linear equalizer as its forward lter and a feedback lter that optimizes the cm cost of the decision variable . it is shown that for any xed forward lter with reasonable small residue intersymbol interference , the cm cost function for the feedback lter is approximately convex and its global minimum can be approximated in closed form . we demonstrate that the convergence rate of the feedback lter is similar to the least mean square algorithm used in the nonblind design . we show that the pcm-dfe performs better than the nonblind linear mmse equalizer in simulations .
deep space missions are characterized by severely constrained communication links . to meet the needs of future missions and increase their scientific return , future space systems will require an increased level of autonomy on-board . in this work , we propose a comprehensive approach to on-board autonomy relying on model-based reasoning , and encompassing many important reasoning capabilities such as plan generation , validation , execution and monitoring , fdir , and run-time diagnosis . the controlled platform is represented symbolically , and the reasoning capabilities are seen as symbolic manipulation of such formal model . we have developed a prototype of our framework , implemented within an on-board autonomous reasoning engine . we have evaluated our approach on two case-studies inspired by real-world , ongoing projects , and characterized it in terms of reliability , availability and performance .
probabilistic logic programming is a powerful technique to represent and reason with imprecise probabilistic knowledge . a probabilistic logic program is a knowledge base which contains a set of conditional events with probability intervals . in this paper , we investigate the issue of revising such a probabilistic logic program in light of receiving new information . we propose postulates for revising probabilistic logic program when a new piece of evidence is also a probabilistic conditional event . our postulates lead to jeffrey 's rule and bayesian conditioning when the original probabilistic logic program defines a single probability distribution . furthermore , we prove that our postulates are extensions to darwiche and pearl -lrb- dp -rrb- postulates when new evidence is a propositional formula . we also give the representation theorem for the postulates and provide an instantiation of revision operators satisfying the proposed postulates .
we contribute to the theoretical understanding of evolutionary algorithms and carry out a parameterized analysis of evolutionary algorithms for the euclidean traveling salesperson problem -lrb- euclidean tsp -rrb- . we exploit structural properties related to the optimization process of evolutionary algorithms for this problem and use structural properties to bound the runtime of evolutionary algorithms . our analysis studies the runtime in dependence of the number of inner points k and shows that simple evolutionary algorithms solve the euclidean tsp in expected time o -lrb- n 4k -lrb- 2k − 1 -rrb- ! -rrb- . moreover , we show that , under reasonable geometric constraints , a locally optimal 2-opt tour can be found by randomized local search in expected time o -lrb- n 2k k ! -rrb- .
in this paper , we consider multipath exploitation and sparse reconstruction in a network of distributed multistatic radar units for stationary target localization behind walls . multipath exploitation leverages prior information of the indoor scattering environment to eliminate ghosts targets . however , uncertainties in interior wall positions severely impair the effectiveness of multipath exploitation . we develop a multipath signal model for the distributed radar network configuration , which parameterizes the wall locations , and perform joint optimization for simultaneously recovering the target and wall positions . supporting simulation results are provided , which validate the effectiveness of the proposed multipath signal model .
search results clustering helps users to browse the search results and locate what they are looking for . in the search result clustering , the label selection which annotates a meaningful phrase for each cluster becomes the most fundamental issue . in this paper , we present a new method of using the language modeling approach over dmoz for label selection , namely label language model . experimental results show that our method is helpful to obtain meaningful clustering labels of search results .
this paper presents a combined packet loss compensation system for distributed speech recognition . compensation is applied at three stages within the distributed speech recognition beginning with interleaving on the terminal device to reduce burst lengths in the received feature vector stream . on the receiver side estimation of missing vectors is applied to reconstruct the feature vector stream prior to recognition . finally , the decoding process of the recogniser is modified to take into account the varying reliability of these estimated feature vectors . experiments performed on both the aurora connected digits task and the wsjcam0 large vocabulary task show substantial gains in recognition accuracy across a range of packet loss conditions .
measures of scatter are used in statistical pattern recognition to identify and select important features , computed as linear combinations of the given features . examples include principal components and linear discriminants . the classic computational procedures require eigenvector decomposition of large matrices , and in the case of images they are only practical for identifying a low dimensional feature sub-space . we investigate the case in which the selected features are required to be a subset of the given features . it is shown that the same scatter measures used in the general case can also be used in this discrete selection case , but the computational procedures no longer involves matrix eigenvector decomposition . instead , the selection of pixels that optimize scatter measures can be accomplished by a very simple and efficient discrete optimization technique that runs in linear time regardless of the subspace size . applications to clustering and content based indexing are discussed .
the phase retrieval problem arises when a signal must be reconstructed from only the magnitude of its fourier transform ; if the phase information were also available , the signal could simply be synthesized using the inverse fourier transform . in continuous phase retrieval , most previous solutions rely on discretizing the problem and then employing an iterative algorithm . we a void this approximation by using wavelet expansions to transform this uncountably innnite problem into a linear system of equations . the wavelet bases permit a solution by incorporating a priori signal information and they provide a structured system of equations which results in a fast algorithm . our solutions obviate the stagnation problems associated with iterative algorithms , they are computationally simpler and more stable than previous non-iterative algorithms , and they can accommodate noisy fourier magnitude information . this paper develops our 1-d continuous , non-minimum phase retrieval algorithm and illustrates its eeectiveness with numerical examples .
body-worn solid-state audio recorders can easily and cheaply capture the bearer 's entire acoustic environment throughout the day ; we refer to such recordings as '' personal audio '' . extracting useful information , and providing access and navigation tools for this data is a challenge ; in this paper we investigate the use of an audio fingerprinting technique , originally developed for identifying music recordings corrupted by noise , as a tool to rapidly identify recurrent sound events within long -lrb- multi-day -rrb- recordings . the audio fingerprinting technique is based on energy peaks in time-frequency , largely removing framing issues and making audio fingerprinting technique intrinsically robust to background noise levels . we show that the audio fingerprinting technique is very effective at identifying exact repetitions of structured sound -lrb- such as jingles and electronic telephone rings -rrb- but is unable to find repeats of more ` organic ' sound events such as garage door openings .
in this paper , we propose a method based on the modification of the channel noise variance at the equaliser input in order to improve the performance of a turbo-equaliser . we will show a relation between the modified channel noise variance and the a priori information statistics . the simulation are done for 2 types of turbo-equalisers .
global illumination effects such as inter-reflections , diffusion and sub-surface scattering severely degrade the performance of structured light-based 3d scanning . in this paper , we analyze the errors caused by global illumination in structured light-based shape recovery . based on this analysis , we design structured light patterns that are resilient to individual global illumination effects using simple logical operations and tools from combinatorial mathematics . scenes exhibiting multiple phenomena are handled by combining results from a small ensemble of such structured light patterns . this combination also allows us to detect any residual errors that are corrected by acquiring a few additional images . our techniques do not require explicit separation of the direct and global components of scene radiance and hence work even in scenarios where the separation fails or the direct component is too low . our methods can be readily incorporated into existing scanning systems without significant overhead in terms of capture time or hardware . we show results on a variety of scenes with complex shape and material properties and challenging global illumination effects .
human readings of structured documents exhibit a much richer intonation than that observed in read isolated sentences . it is a challenge to capture this richness in an automatic way using data-driven techniques . in this paper , we extend our previous research on intonation modelling for isolated sentences in different respects : -lrb- i -rrb- the rnn -lrb- recurrent neural network -rrb- intonation model is now trained and evaluated on read documents , -lrb- ii -rrb- the rnn -lrb- recurrent neural network -rrb- intonation model is evaluated as part of the overall prosody model , -lrb- iii -rrb- the feature selection process is completely automated , and -lrb- iv -rrb- the importance of text-level features such as text type , text structure and type-setting are investigated . it is demonstrated that acceptable intonation models can be constructed starting from a database that does not contain any explicit hand labelling of the intonation contours . it also appears that text type and text structure are important features whereas type-setting is not .
motivated by increasing interest in energy efficient modulations , we investigate a blind algorithm for biorthogonal signaling . while this modulation has historically been considered only for use in nar-rowband systems without intersymbol interference , recent attention has been given to its use in intersymbol interference . due to the fact that biorthogonal modulation results in a source that is not i.i.d. , however , classical adaptive equalization techniques can not be directly applied to equalization of bom signals . we review the mmse and lms-based equalizers , and then identify some peculiarities that arise in blind equalization of bom signals when compared to more traditional modulations like bpsk . next , we present a novel blind algorithm , called mmse , for the adaptive equalization of bom signals . we discuss the convergence properties of this blind algorithm , and demonstrate its performance with numerical simulations .
this paper introduces our research aimed at building '' active music listening interfaces '' . this research approach is intended to enrich end-users ' music listening experiences by applying music-understanding technologies based on signal processing . active music listening is a way of listening to music through active interactions . we have developed seven interfaces for active music listening , such as interfaces for skipping sections of no interest within a musical piece while viewing a graphical overview of the entire song structure , for displaying virtual dancers or song lyrics synchronized with the music , for changing the timbre of instrument sounds in compact-disc recordings , and for browsing a large music collection to encounter interesting musical pieces or artists . these interfaces demonstrate the importance of music-understanding technologies and the bene ¿ t they offer to end users . our hope is that this work will help change music listening into a more active , immersive experience .
this paper presents a novel framework for simultaneously learning representation and control in continuous markov decision processes . our approach builds on the framework of proto-value functions , in which the underlying representation or basis functions are automatically derived from a spectral analysis of the state space manifold . the proto-value functions correspond to the eigenfunctions of the graph laplacian . we describe an approach to extend the eigenfunctions to novel states using the nyström extension . a least-squares policy iteration method is used to learn the control policy , where the underlying subspace for approximating the value function is spanned by the learned proto-value functions . a detailed set of experiments is presented using classic benchmark tasks , including the inverted pendulum and the mountain car , showing the sensitivity in performance to various parameters , and including comparisons with a parametric radial basis function method .
recently we proposed a cubic-spline-based variable-parameter hidden markov model whose mean and variance parameters vary according to some cubic spline functions of additional environment-dependent parameters . we have shown good properties of the cubic-spline-based variable-parameter hidden markov model and demonstrated on the aurora-3 corpus that cubic-spline-based variable-parameter hidden markov model greatly outperforms the mce-trained conventional hmm at the cost of increased total number of model parameters . in this paper , we propose to share spline functions across different gaussian mixture components to reduce the total number of model parameters and develop a clustering algorithm to do so . we demonstrate the effectiveness of our parameter clustering and sharing algorithm for the cubic-spline-based variable-parameter hidden markov model on aurora-3 corpus and show that proper parameter sharing can reduce the number of parameters from 4 times of that used in the conventional hmm to 1.13 times and still get 18 % relative wer reduction over the mce trained conventional hmm under the well-matched condition . effective parameter sharing makes the cubic-spline-based variable-parameter hidden markov model an attractive model for noise robustness .
in this paper , we present a neural network architecture that belongs to the multi-layer perceptron family , associated with two different algorithms : the ordinary gradient and the natural gradient , we compare performances of those algorithms . the identification of a non-normalized power amplifier yielded to the introduction of an additional weight in the classical multi-layer perceptron structure . the application of this neural network architecture is space telecommunications : identification of satellite communication channels , and especially the down link . this link is made up with two elements . the first one is a high power amplifier -lrb- non-linearity -rrb- . the second one is a filter -lrb- memory -rrb- .
we propose an efficient dialogue management for an information navigation system based on a document knowledge base . it is expected that incorporation of appropriate n-best candidates of asr and contextual information will improve the information navigation system performance . the information navigation system also has several choices in generating responses or confirmations . in this paper , this selection is optimized as minimization of bayes risk based on reward for correct information presentation and penalty for redundant turns . we have evaluated this minimization of bayes risk with our spoken dialogue information navigation system '' dialogue navigator for kyoto city '' , which also has question-answering capability . effectiveness of the proposed minimization of bayes risk was confirmed in the success rate of retrieval and the average number of turns for information access .
a joint source-channel coding scheme for scalable video is developed in this paper . an snr scalable video coder is used and unequal error protection is allowed for each scalable layer . our problem is to allocate the available bit rate across scalable layers and , within each layer , between source and channel coding , while minimizing the end-to-end distortion of the received video sequence . the resulting optimization algorithm we propose utilizes universal rate-distortion characteristic plots . these plots show the contribution of each layer to the total distortion as a function of the source rate of the layer and the residual bit error rate -lrb- the error rate that remains after the use of channel coding -rrb- . models for these plots are proposed in order to reduce the computational complexity of the snr scalable video coder . experimental results demonstrate the effectiveness of the proposed joint source-channel coding scheme .
quantum learning holds great promise for the field of machine intelligence . the most studied quantum learning algorithm is the quantum neural network . many such quantum learning algorithm have been proposed , yet none has become a standard . in addition , these quantum learning algorithm usually leave out many details , often excluding how they intend to train their networks . this paper discusses one approach to the problem and what advantages it would have over classical networks .
to be effective , an agent that collaborates with humans needs to be able to learn new tasks from humans they work with . this paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration , explanation and dialogue . to accomplish this , the system integrates a range of ai technologies : deep natural language understanding , knowledge representation and reasoning , dialogue systems , plan-ning/agent-based systems and machine learning . a formal evaluation shows the approach has great promise .
a method for detecting and segmenting periodic motion is presented . we exploit periodicity as a cue and detect periodic motion in complex scenes where common methods for motion segmentation are likely to fail . we note that periodic motion detection can be seen as an approximate case of sequence alignment where an image sequence is matched to itself over one or more periods of time . to use this observation , we first consider alignment of two video sequences obtained by independently moving cameras . under assumption of constant translation , the fundamental matrices and the homographies are shown to be time-linear matrix functions . these dynamic quantities can be estimated by matching corresponding space-time points with similar local motion and shape . for periodic motion , we match corresponding points across periods and develop a ransac procedure to simultaneously estimate the period and the dynamic geometric transformations between periodic views . using this method , we demonstrate detection and segmentation of human periodic motion in complex scenes with non-rigid backgrounds , moving camera and motion parallax .
in this paper we reconstruct the 3-d mr spectroscopic data acquired using an echo-planar spectroscopic imaging sequence . we propose to compensate for 0 b inhomogeneity artifact by using the 0 b field map and * 2 t decay estimated from a higher resolution , multi-slice proton imaging data scanned with similar shimming to the epsi one . we employ total variation regularization to obtain more uniform spectral peaks and lineshapes , and exploit the sparsity of the spectral data by using the 1 a norm in the reconstruction .
in this paper , we describe active learning methods for reducing the labeling effort in a statistical call classification system . active learning aims to minimize the number of labeled utterances by automatically selecting for labeling the utterances that are likely to be most informative . the first active learning methods , inspired by certainty-based active learning , selects the examples that the classifier is least confident about . the second active learning methods , inspired by committee-based active learning , selects the examples that multiple classifiers do not agree on . we have evaluated these active learning methods using a call classification system used for at&t customer care . our results indicate that it is possible to reduce human labeling effort at least by a factor of two .
deep convolutional neural networks -lrb- cnn -rrb- have shown their promise as a universal representation for recognition . however , global cnn activations lack geometric invariance , which limits their robustness for recognition and matching of highly variable scenes . to improve the invariance of cnn activations without degrading their discriminative power , this paper presents a simple but effective scheme called multi-scale orderless pooling -lrb- mop-cnn -rrb- . this scheme extracts cnn activations for local patches at multiple scale levels , performs orderless vlad pooling of these cnn activations at each level separately , and concatenates the result . the resulting mop-cnn representation can be used as a generic feature for either supervised or unsupervised recognition tasks , from image classification to instance-level retrieval ; it consistently outperforms global cnn activations without requiring any joint training of prediction layers for a particular target dataset . in absolute terms , it achieves state-of-the-art results on the challenging sun397 and mit indoor scenes classification datasets , and competitive results on ilsvrc2012/2013 classification and inria holidays retrieval datasets .
this paper considers the problem of blind channel estimation of multi channel fir lters . this is a problem arising in e.g. mobile communication systems using digital signalling . by using the orthogonality property between the noise subspace and the channel matrix , it has been shown in earlier work that the channel matrix is identiiable up to a multiplicative constant . in this article , the asymptotic properties of a subspace method using this orthogonality property is presented . an asymptotically correct weighting matrix is derived , demonstrating an attainable lower theoretical bound using the subspace estimate .
we present a new method for generating large numbers of accurate point correspondences between two wide baseline images . this is important for structure from motion algorithms , which rely on many correct matches to reduce error in the derived geometric structure . given a small initial correspondence set we iteratively expand the set with nearby points exhibiting strong affine correlation , and then we constrain the set to an epipolar geometry using ransac . a key point to our algorithm is to allow a high error tolerance in the constraint , allowing the correspondence set to expand into many areas of an image before applying a lower error tolerance constraint . we show that this method successfully expands a small set of initial matches , and we demonstrate it on a variety of image pairs .
downstream processing of machine translation -lrb- mt -rrb- output promises to be a solution to improve translation quality , especially when the mt system 's internal decoding process is not accessible . both rule-based and statistical automatic post-editing -lrb- ape -rrb- methods have been proposed over the years , but with contrasting results . a missing aspect in previous evaluations is the assessment of different methods : i -rrb- under comparable conditions , and ii -rrb- on different language pairs featuring variable levels of mt quality . fo-cusing on statistical ape methods -lrb- more portable across languages -rrb- , we propose the first systematic analysis of two approaches . to understand their potential , we compare them in the same conditions over six language pairs having english as source . our results evidence consistent improvements on all language pairs , a relation between the extent of the gain and mt output quality , slight but statistically significant performance differences between the two methods , and their possible complementarity .
surjectivity of linear projections between distribution families with fixed mean and covariance -lrb- regardless of dimension -rrb- is re-derived by a new proof . we further extend this property to distribution families that respect additional constraints , such as symmetry , unimodality and log-concavity . by combining our results with classic univariate inequalities , we provide new worst-case analyses for natural risk criteria arising in classification , optimization , portfolio selection and markov decision processes .
we present a structuring method for discrete constraint satisfaction problems . structuring method takes advantage of interchangeabilities to represent sets of equivalent values by meta-values and thus obtain more compact representations . strongly related variables are clustered into meta-variables to create occurrences of inter-changeabilities . by iterative application , a csp can be transformed into an hierarchy of equivalent discrete constraint satisfaction problems , where each problem is signiicantly simpler than the original one . this structure is particularly advantageous when a large set of possible solutions must be inspected .
a convex and feature-rich discriminative approach to dependency grammar inductionédouard abstract in this paper , we introduce a new method for the problem of unsupervised dependency parsing . most current approaches are based on generative models . learning the parameters of such models relies on solving a non-convex optimization problem , thus making them sensitive to initial-ization . we propose a new convex formulation to the task of dependency grammar induction . our approach is discriminative , allowing the use of different kinds of features . we describe an efficient optimization algorithm to learn the parameters of our model , based on the frank-wolfe algorithm . our method can easily be generalized to other unsupervised learning problems . we evaluate our approach on ten languages belonging to four different families , showing that our method is competitive with other state-of-the-art methods .
we propose a new technique to jointly recover cosegmen-tation and dense per-pixel correspondence in two images . our method parameterizes the correspondence field using piecewise similarity transformations and recovers a mapping between the estimated common '' foreground '' regions in the two images allowing them to be precisely aligned . our formulation is based on a hierarchical markov random field model with segmentation and transformation labels . the hierarchical markov random field model uses nested image regions to constrain inference across multiple scales . unlike prior hierarchical methods which assume that the structure is given , our proposed iterative technique dynamically recovers the structure along with the labeling . this joint inference is performed in an energy minimization framework using iterated graph cuts . we evaluate our method on a new dataset of 400 image pairs with manually obtained ground truth , where it outperforms state-of-the-art methods designed specifically for either cosegmentation or correspondence estimation .
a multiuser downlink mimo-ofdm scheme is considered . perfect channel knowledge is assumed at the base station and linear processing is used at both the transmit and receive sides . the objective is to optimize the mean ber of the system while satisfying a transmit power constraint and fulfilling each user 's rate . fdma and sdma are investigated . we show how the multiuser interference induced by sdma can be annihilated and exhibit that this multiple access technique should be preferred to fdma .
hyperspectral image analysis has been subjected to many improvements made in past decade . yet the accurate estimation of dimensionality is still a challenge . since dimension estimation of the hyperspectral data is the first step in analysis of an image , the accuracy of analysis results highly depends on the accuracy of the dimension estimation step . mostly , existing methods isolate the process of dimension estimation and process of denoising which leads to an inaccurate estimation of constituent components in the signal . in this paper , the problem of estimating the dimensionality of hyperspectral data using the concept of '' noiseless code length '' is addressed . in our proposed method , nclm , a set of nested subsets including the hyperspectral data is generated first and then an error comparison approach is utilized by estimating the noiseless data error rather than noisy data error used by the existing methods to find the optimum subset . it has been shown that the estimated noiseless error has a minimum that represents the accurate estimation of the dimensionality of hyperspec-tral data . the comparison of nclm to other methods shows a substantial improvement in estimation of dimensionality in hyperspectral imagery .
in data-driven spoken dialog system development , developers should prepare a dialog corpus with semantic annotation . however , the labeling process is a laborious and time consuming task . to reduce human efforts , we propose an unsupervised approach based on non-parametric bayesian hidden markov model to the problem of modeling user actions . with the non-parametric bayesian hidden markov model , system designers do not need to determine the number and type of user actions . in the experiments , we evaluated the clustering results by comparing them to the human annotation . we also tested a dialog system that used models trained from the automatically annotated corpus with a user simulation .
this paper describes a preparation of the first large czech prosodic database which should be useful both in automatic speech recognition and text-to-speech synthesis . in the area of automatic speech recognition we intend to use automatic speech recognition for an automatic punctuation annotation , in the area of automatic speech recognition for building a prosodic module for the czech high-quality synthesis . the database is based on the czech radio & tv broadcast news corpus -lrb- uwb b02 -rrb- recorded at the university of west bohemia . the configuration of the database includes recorded speech , raw and stylized f0 values , frame level energy values , a word-and phoneme-level time alignment , and a linguistically motivated description of the prosodic data . a technique of prosodic data acquisition and stylization is described . a new tagset for a linguistical annotation of the czech prosody is proposed and used .
since more and more multimedia data associated with spoken documents have been made available to the public , spoken document retrieval has become an important research subject in the past two decades . following the research tendency , many efforts have been devoted towards developing indexing and modeling techniques for representing spoken documents , but only few have been made on improving query formulation for better representing users ' information needs . the i-vector based language modeling framework , stemming from the state-of-the-art i-vector framework for language identification and speaker recognition , has been proposed and formulated to represent documents in sdr with good promise recently . however , a major challenge of using i-vector based language modeling framework for query formulation is that a query usually consists of only a few words ; thus , it is hard to learn a reliable representation accordingly . in this paper , we focus our attention on query reformulation and propose three novel methods on top of i-vector based language modeling framework to more accurately represent users ' information needs . in addition , we also explore the use of multi-levels of index features , including word-and subword-level units , to work in concert with the proposed methods . a series of empirical sdr experiments conducted on the tdt-2 -lrb- topic detection and tracking -rrb- collection demonstrate the good effectiveness of our proposed methods as compared to existing state-of-the-art methods .
we extensively evaluated a data hiding algorithm for stereo audio signals which embeds data using the polarity of the echoes added to the high-frequency channels , which we have previously proposed . its performance was also compared to conventional data hiding using spread spectrum , and those using echoes with different delays . raw embedded data was detected with little or no errors for added noise at 20 db snr and above , or with mp3 coders , although the spread spectrum method showed almost no errors at all . however , sample rate conversion and random bit cropping were shown not to affect the embedded data with the proposed data hiding algorithm , while other methods , including spread spectrum , showed significant amount of errors . the embedded audio quality test using the mushra standard method resulted in little noticeable degradation , far better quality compared to other hiding methods .
the objective of this work is the integration and optimization of an automatic face detection and recognition system for video indexing applications . the system is composed of a face detection stage presented previously which provides good results maintaining a low computational cost . the face detection stage is based on the principal components analysis approach which has been modified to cope with the video indexing applications . after the integration of the two stages , several improvements are proposed which increase the face detection and recognition rate and the overall performance of the system . good results have been obtained using the mpeg-7 video content set used in the mpeg-7 evaluation group .
the additive clustering model is widely used to infer the features of a set of stimuli from their similarities , on the assumption that similarity is a weighted linear function of common features . this paper develops a fully bayesian formulation of the additive clustering model , using methods from nonparametric bayesian statistics to allow the number of features to vary . we use this to explore several approaches to parameter estimation , showing that the fully bayesian formulation provides a straightforward way to obtain estimates of both the number of features used in producing similarity judgments and their importance .
in this paper , we extend graph-based identification methods by allowing background knowledge in the form of non-zero parameter values . such background knowledge could be obtained , for example , from a previously conducted randomized experiment , from substantive understanding of the domain , or even an identification technique . to incorporate such background knowledge systematically , we propose the addition of auxiliary variables to the graph-based identification methods , which are constructed so that certain paths will be conveniently cancelled . this cancellation allows the auxiliary variables to help conventional methods of identification -lrb- e.g. , single-door criterion , instrumental variables , half-trek criterion -rrb- , as well as model testing -lrb- e.g. , d-separation , over-identification -rrb- . moreover , by iteratively alternating steps of identification and adding auxiliary variables , we can improve the power of existing graph-based identification methods via a bootstrapping approach that does not require external knowledge . we operationalize this graph-based identification methods for simple instrumental sets -lrb- a generalization of instrumental variables -rrb- and show that the resulting graph-based identification methods is able to identify at least as many models as the most general identification method for linear systems known to date . we further discuss the application of auxiliary variables to the tasks of model testing and z-identification .
a directed generative model for binary data using a small number of hidden continuous units is investigated . a clipping nonlinear-ity distinguishes the directed generative model from conventional principal components analysis . the relationships between the correlations of the underlying continuous gaussian variables and the binary output variables are utilized to learn the appropriate weights of the directed generative model . the advantages of this directed generative model are illustrated on a translationally invariant binary distribution and on handwritten digit images .
approaches from standard automatic speaker recognition , which rely on cepstral features , suffer the problem of lack of interpretability for forensic applications . but the growing practice of using '' higher-level '' features in automatic systems offers promise in this regard . we provide an overview of automatic higher-level systems and discuss potential advantages , as well as issues , for their use in the forensic context .
we develop the syntactic topic model , a nonparametric bayesian model of parsed documents . the syntactic topic model generates words that are both thematically and syntactically constrained , which combines the semantic insights of topic models with the syntactic information available from parse trees . each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree-specific syntactic transitions . words are assumed to be generated in an order that respects the parse tree . we derive an approximate posterior inference method based on variational methods for hierarchical dirichlet processes , and we report qualitative and quantitative results on both synthetic data and hand-parsed documents .
for a hands-free speech interface , it is important to detect commands in spontaneous utterances . usual voice activity detection systems can only distinguish speech frames from non-speech frames , but they can not discriminate whether the detected speech section is a command for a system or not . in this paper , in order to analyze the difference between system requests and spontaneous utterances , we focus on fluctuations in a long period , such as prosodic articulation , and fluctuations in a short period , such as phoneme articulation . the use of multi-resolution analysis using gabor wavelet on a log-scale mel-frequency filter-bank clarifies the different characteristics of system commands and spontaneous utterances . experiments using our robot dialog corpus show that the accuracy of the proposed method is 92.6 % in f-measure , while the conventional power and prosody-based method is just 66.7 % .
interactive dynamic influence diagrams -lrb- interactive dynamic influence diagrams -rrb- offer a transparent and semantically clear representation for the sequential decision-making problem over multiple time steps in the presence of other interacting agents . solving interactive dynamic influence diagrams exactly involves knowing the solutions of possible models of the other agents , which increase exponentially with the number of time steps . we present a method of solving interactive dynamic influence diagrams approximately by limiting the number of other agents ' candidate models at each time step to a constant . we do this by clustering the models and selecting a representative set from the clusters . we discuss the error bound of the approximation technique and demonstrate its empirical performance .
to achieve more accurate and consistent registration in an image population , a novel hierarchical groupwise registration framework , called atlas building by self-organized registration and bundling -lrb- absorb -rrb- , is proposed in this paper . in this new hierarchical groupwise registration framework , the global structure , i.e. , the relative distribution of subject images is always preserved during the registration process by constraining each subject image to deform only locally with respect to its neighbors within the learned image manifold . to achieve this goal , two novel strategies , i.e. , the self-organized registration by warping one image towards a set of its eligible neighbors and image bundling to cluster similar images , are specially proposed . by using these two strategies , this new hierarchical groupwise registration framework can perform groupwise registration in a hierarchical way . specifically , in the high level , hierarchical groupwise registration framework will perform on a much smaller dataset formed by the representative subject images of all subgroups that are generated in the previous levels of registration . compared to the other groupwise registration methods , our proposed hierarchical groupwise registration framework has several advantages : -lrb- 1 -rrb- hierarchical groupwise registration framework explores the local data distribution and uses the obtained distribution information to guide the registration ; -lrb- 2 -rrb- the possible registration error can be greatly reduced by requiring each individual subject to move only towards its nearby subjects with similar structures ; -lrb- 3 -rrb- hierarchical groupwise registration framework can produce a smoother registration path , in general , from each subject image to the final built atlas than other groupwise registration methods . experimental results on both synthetic and real datasets show that the proposed hierarchical groupwise registration framework can achieve substantial improvements , compared to the other two widely used groupwise registration methods , in terms of both registration accuracy and robustness .
unsupervised relation extraction -lrb- unsupervised relation extraction methods -rrb- methods automatically discover semantic relations in text corpora of unknown content and extract for each discovered relation a set of relation instances . due to the sparsity of the feature space , unsupervised relation extraction methods is vulnerable to ambiguities and underspeci-fication in patterns . in this paper , we propose to increase the discriminative power of patterns in unsupervised relation extraction methods using selectional restrictions . we propose a method that utilizes a web-derived soft clustering of n-grams to model selectional restrictions in the open domain . we comparatively evaluate our method against a baseline without selectional restrictions , a setup in which standard 7-class named entity types are used as selectional restrictions and a setup that models selectional restrictions using a fine-grained entity type system . our results indicate that modeling sr into patterns significantly improves the ability of unsupervised relation extraction methods to discover relations and enables the discovery of more fine-granular relations .
we present an overview of fab-map , an algorithm for place recognition and mapping developed for infrastructure-free mobile robot navigation in large environments . the fab-map allows a robot to identify when it is revisiting a previously seen location , on the basis of imagery captured by the robot 's camera . we outline a complete probabilis-tic framework for the task , which is applicable even in visually repetitive environments where many locations may appear identical . our work introduces a number of technical innovations-notably we demonstrate that place recognition performance can be improved by learning an approximation to the joint distribution over visual elements . we also investigate several principled approaches to making the fab-map robust in visually repetitive environments , and define an efficient bail-out strategy for multi-hypothesis testing to improve system speed . our model has been shown to substantially outperform standard tf-idf ranking on our task of interest . we demonstrate the fab-map performing reliable online appearance mapping and loop closure detection over a 1,000 km trajectory , with mean filter update times of 14 ms.
for the analysis of images , a deeper understanding of their intrinsic structure is required . this has been obtained for 2d images by means of statistical analysis -lsb- 15 , 18 -rsb- . here , we analyze the relation between local image structures -lrb- i.e. , homogeneous , edge-like , corner-like or texture-like structures -rrb- and the underlying local 3d structure , represented in terms of continuous surfaces and different kinds of 3d discontinuities , using 3d range data with the true color information . we find that homogeneous image patches correspond to continuous surfaces , and discontinuities are mainly formed by edge-like or corner-like structures . the results are discussed with regard to existing and potential computer vision applications and the assumptions made by these applications .
usually , speaker recognition systems do not take into account the short -- term dependence between the vocal source and the vocal tract . a feasibility study that retains this dependence is presented here . a model of joint probability functions of the pitch and the feature vectors is proposed . three strategies are designed and compared for all female speakers taken from the spidre corpus . the first operates on all voiced and unvoiced speech segments -lrb- baseline strategy -rrb- . the second strategy considers only the voiced speech segments and the last includes the short -- term pitch information along with the standard mfcc . we use two pattern recognizers : lvq -- slp and gmm . in all cases , we observe an increase in the identification rates and more specifically when using a time duration of 500 ms -lrb- 6 % higher -rrb- .
in this paper , we present an unsupervised methodology for propagating lexical co-occurrence vectors into an ontology such as wordnet . we evaluate the unsupervised methodology on the task of automatically attaching new concepts into the ontology . experimental results show 73.9 % attachment accuracy in the first position and 81.3 % accuracy in the top-5 positions . this unsupervised methodology could potentially serve as a foundation for on-tologizing lexical-semantic resources and assist the development of other large-scale and internally consistent collections of semantic information .
in this paper we describe a software assistant agent that can proactively assist human users situated in a time-constrained environment to perform nor-mative reasoning -- reasoning about prohibitions and obligations -- so that the user can focus on her planning objectives . in order to provide proactive assistance , the software assistant agent must be able to 1 -rrb- recognize the user 's planned activities , 2 -rrb- reason about potential needs of assistance associated with those predicted activities , and 3 -rrb- plan to provide appropriate assistance suitable for newly identified user needs . to address these specific requirements , we develop an software assistant agent that integrates user intention recognition , normative reasoning over a user 's intention , and planning , execution and replanning for assistive actions . this paper presents the software assistant agent and discusses practical applications of this software assistant agent .
affine projection algorithm , which updates the weight vector based on several previous input vectors , is an useful adaptive filter to improve the convergence speed of lms-type filter . however , the computational complexity of affine projection algorithm highly depends on the number of input vectors used for update . in this paper , we propose affine projection algorithm with selective regressors whose purpose is to reduce complexity by selecting a subset of input regressors at every iteration . the optimal selection of input regressors is derived by comparing the cost functions based on the principle of minimum disturbance . the new affine projection algorithm show good convergence performance as attested to by various experimental results .
correlation filters take advantage of specific properties in the fourier domain allowing them to be estimated efficiently : o -lrb- n d log d -rrb- in the frequency domain , versus o -lrb- d 3 + n d 2 -rrb- spatially where d is signal length , and n is the number of signals . recent extensions to correlation filters , such as mosse , have reignited interest of their use in the vision community due to their robustness and attractive computational properties . in this paper we demonstrate , however , that this computational efficiency comes at a cost . specifically , we demonstrate that only 1 d proportion of shifted examples are unaffected by boundary effects which has a dramatic effect on detection/tracking performance . in this paper , we propose a novel approach to correlation filter estimation that : -lrb- i -rrb- takes advantage of inherent computational redundancies in the frequency domain , -lrb- ii -rrb- dramatically reduces boundary effects , and -lrb- iii -rrb- is able to implicitly exploit all possible patches densely extracted from training examples during learning process . impressive object tracking and detection results are presented in terms of both accuracy and computational efficiency .
sampling repeated clinical laboratory tests with appropriate timing is challenging because the latent physio-logic function being sampled is in general nonstation-ary . when ordering repeated tests , clinicians adopt various simple strategies that may or may not be well suited to the behavior of the function . previous research on this topic has been primarily focused on cost-driven assessments of oversampling . but for monitoring physiologic state or for retrospective analysis , undersampling can be much more problematic than oversampling . in this paper we analyze hundreds of observation sequences of four different clinical laboratory tests to provide princi-pled , data-driven estimates of undersampling and over-sampling , and to assess whether the sampling adapts to changing volatility of the latent function . to do this , we developed a new method for fitting a gaussian process to samples of a nonstationary latent function . our method includes an explicit estimate of the latent func-tion 's volatility over time , which is deterministically related to its nonstationarity . we find on average that the degree of undersampling is up to an order of magnitude greater than oversampling , and that only a small minority are sampled with an adaptive strategy .
the accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms . in high dimensional settings the sample covariance is known to perform poorly , hence regularization strategies such as analytic shrinkage of ledoit/wolf are applied . in the standard setting , i.i.d. data is assumed , however , in practice , time series typically exhibit strong autocorrela-tion structure , which introduces a pronounced estimation bias . recent work by sancetta has extended the shrinkage framework beyond i.i.d. data . we contribute in this work by showing that the sancetta estimator , while being consistent in the high-dimensional limit , suffers from a high bias in finite sample sizes . we propose an alternative estimator , which is -lrb- 1 -rrb- unbiased , -lrb- 2 -rrb- less sensitive to hyperparame-ter choice and -lrb- 3 -rrb- yields superior performance in simulations on toy data and on a real world data set from an eeg-based brain-computer-interfacing experiment .
the number of multi-robot systems deployed in field applications has risen dramatically over the years . nevertheless , supervising and operating multiple robots at once is a difficult task for a single operator to execute . in this paper we propose a novel approach for utilizing advising automated agents when assisting an operator to better manage a team of multiple robots in complex environments . we introduce the myopic advice optimization problem and exemplify its implementation using an agent for the search and rescue task . our intelligent advising agent was evaluated through extensive field trials , with 44 non-expert human operators and 10 low-cost mobile robots , in simulation and physical deployment , and showed a significant improvement in both team performance and the operator 's satisfaction .
motivated by a previous study of adaptive multiuser de-modulators for direct-sequence spread-spectrum multiple access , detectors for spread-spectrum signals are investigated . due to the prohibitive complexity of the locally optimum detector for such a stochastic multi-variate signal in impulsive channel noise , moderate complexity , distribution-free detectors are pursued . in particular , the diierential snrs -lrb- processing gain -rrb- of correlator based structures are determined . this performance measure is apt given the relatively low signal strength of spread digital signals . the numerical results -lrb- in the context of the prior investigation of adaptive multiuser demodulators -rrb- impel the development of a hybrid detector which is composed of linear and non-linear structures . the asymptotic normality of the test statistics under study is also examined .
in this paper , we present our latest investigations on pronunciation modeling and its impact on asr . we propose completely automatic methods to detect , remove , and substitute inconsistent or flawed entries in pronunciation dictionaries . the experiments were conducted on different tasks , namely -lrb- 1 -rrb- word-pronunciation pairs from the czech , english , french , german , polish , and spanish wiktionary -lsb- 1 -rsb- , a multilingual wiki-based open content dictionary , -lrb- 2 -rrb- our globalphone hausa pronunciation dictionary -lsb- 2 -rsb- , and -lrb- 3 -rrb- pronunciations to complement our mandarin-english seame code-switch dictionary -lsb- 3 -rsb- . in the final results , we fairly observed on average an improvement of 2.0 % relative in terms of word error rate and even 27.3 % for the case of english wik-tionary word-pronunciation pairs .
the paper presents principal component analysis approach to the reduction of noise contaminating the data . the principal component analysis approach performs the role of lossy compression and de-compression . the compression/decompression provides the means of coding the data and then recovering principal component analysis approach with some losses , dependent on the realized compression ratio . in this process some part of information contained in the data is lost . when the loss tolerance is equal to the noise strength , the noise and the loss tolerance are augmented and the de-compressed signal is deprived of noise . this way of noise ltering has been checked on the examples of 1-dimensional and 2-dimensional data and the results of numerical experiments have been included in the paper .
traditional syntax models typically leverage part-of-speech information by constructing features from hand-tuned templates . we demonstrate that a better approach is to utilize pos tags as a reg-ularizer of learned representations . we propose a simple method for learning a stacked pipeline of syntax models which we call '' stack-propagation '' . we apply this to dependency parsing and tagging , where we use the hidden layer of the tagger network as a representation of the input tokens for the parser . at test time , our parser does not require predicted pos tags . on 19 languages from the universal dependencies , our method is 1.3 % -lrb- absolute -rrb- more accurate than a state-of-the-art graph-based approach and 2.7 % more accurate than the most comparable greedy model .
in this paper , we describe a rote extrac-tor that learns patterns for finding semantic relationships in unrestricted text , with new procedures for pattern generalization and scoring . these include the use of part-of-speech tags to guide the generalization , named entity categories inside the patterns , an edit-distance-based pattern generalization algorithm , and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora . in an evaluation with 14 entities , the rote extrac-tor attains a precision higher than 50 % for half of the relationships considered .
bike sharing systems -lrb- bsss -rrb- experience a significant loss in customer demand due to starvation -lrb- empty base stations precluding bike pickup -rrb- or congestion -lrb- full base stations precluding bike return -rrb- . therefore , bsss operators reposition bikes between stations with the help of carrier vehicles . due to unpredictable and dynamically changing nature of the demand , myopic reasoning typically provides a below par performance . we propose an online and robust repositioning approach to min-imise the loss in customer demand while considering the possible uncertainty in future demand . specifically , we develop a scenario generation approach based on an iterative two player game to compute a strategy of repositioning by assuming that the environment can generate a worse demand scenario -lrb- out of the feasible demand scenarios -rrb- against the current repositioning solution . extensive computational results from a simulation built on real world data set of bike sharing company demonstrate that our online and robust repositioning approach can significantly reduce the expected lost demand over the existing benchmark approaches .
we investigate whether a neural , encoder-decoder translation system learns syntactic information on the source side as a by-product of training . we propose two methods to detect whether the encoder has learned local and global source syntax . a fine-grained analysis of the syntactic structure learned by the encoder reveals which kinds of syntax are learned and which are missing .
we describe a novel approach to optimizing matrix completion problems involving nuclear norm regulariza-tion and apply it to the matrix completion problem . we combine methods from non-smooth and smooth optimization . at each step we use the proximal gradient to select an active sub-space . we then find a smooth , convex relaxation of the smaller subspace problems and solve these using second order methods . we apply our methods to matrix completion problems including netflix dataset , and show that they are more than 6 times faster than state-of-the-art nuclear norm solvers . also , this is the first paper to scale nuclear norm solvers to the yahoo-music dataset , and the first time in the literature that the efficiency of nuclear norm solvers can be compared and even compete with non-convex solvers like alternating least squares .
supply chain formation -lrb- scf -rrb- is the process of determining the participants in a supply chain , who will exchange what with whom , and the terms of the exchanges . decentralized scf appears as a highly intricate task because agents only possess local information and have limited knowledge about the capabilities of other agents . the decentralized scf problem has been recently cast as an optimization problem that can be efficiently approximated using max-sum loopy belief propagation . along this direction , in this paper we propose a novel encoding of the decentralized scf problem into a binary factor graph -lrb- containing only binary variables -rrb- as well as an alternative algorithm . we empirically show that our approach allows to significantly increase scalability , hence allowing to form supply chains in market scenarios with a large number of participants and high competition .
this paper describes a method for reducing sudden noise using noise detection and classification methods , and noise power estimation . sudden noise detection and classification have been dealt with in our previous study . in this paper , gmm-based noise reduction is performed using the detection and classification results . as a result of classification , we can determine the kind of noise we are dealing with , but the power is unknown . in this paper , this gmm-based noise reduction is solved by combining an estimation of noise power with the noise reduction method . in our experiments , the proposed method achieved good performance for recognition of utterances overlapped by sudden noises .
this paper introduces a multi-principal-distribution-model method and hidden markov model for gesture image sequence interpretation . to track the hand-shape , multi-principal-distribution-model method uses the pdm model which is built by learning patterns of variability from a training set of correctly annotated images . for gesture recognition , we need to deal with a large variety of hand-shape . therefore , we divide all the training hand shapes into a number of similar groups , with each group trained for an individual pdm shape model . finally , we use the hidden markov model to determine model transition among these pdm shape model . from the model transition sequence , multi-principal-distribution-model method can identify the continuous gestures denoting one-digit or two-digit numbers .
this paper reports on the final results of the ester 2 evaluation campaign held from 2007 to april 2009 . the aim of this campaign was to evaluate automatic radio broadcasts rich transcription systems for the french language . the evaluation tasks were divided into three main categories : audio event detection and tracking -lrb- e.g. , speech vs. music , speaker tracking -rrb- , ortho-graphic transcription , and information extraction . the paper describes the data provided for the campaign , the task definitions and evaluation protocols as well as the results .
synchronous context-free grammars -lrb- scfgs -rrb- have been successfully exploited as translation models in machine translation applications . when parsing with an synchronous context-free grammars , computational complexity grows exponentially with the length of the rules , in the worst case . in this paper we examine the problem of factorizing each rule of an input synchronous context-free grammars to a generatively equivalent set of rules , each having the smallest possible length . our algorithm works in time o -lrb- n log n -rrb- , for each rule of length n . this improves upon previous results and solves an open problem about recognizing permutations that can be factored .
recently , the hybrid deep neural networks and hidden markov models have achieved dramatic gains over the conventional gmm/hmms method on various large vocabulary continuous speech recognition -lrb- lvcsr -rrb- tasks . in this paper , we propose two new methods to further improve the hidden markov models : i -rrb- use dropout as pre-conditioner -lrb- dap -rrb- to initialize dnn prior to back-propagation for better recognition accuracy ; ii -rrb- employ a shrinking dnn structure with hidden layers decreasing in size from bottom to top for the purpose of reducing model size and expediting computation time . the proposed shrinking dnn structure is evaluated in a 70-hour mandarin transcription task and the 309-hour switchboard task . compared with the traditional greedy layer-wise pre-trained dnn , shrinking dnn structure can achieve about 10 % and 6.8 % relative recognition error reduction for psc and swb tasks respectively . in addition , we also evaluate shrinking dnn structure as well as its combination with dap on the swb task . experimental results show that these methods can reduce model size to 45 % of original size and accelerate training and test time by 55 % , without losing recognition accuracy .
we will demonstrate the gems system for automated development and evaluation of high-quality cancer diagnostic models and biomarker discovery from microarray gene expression data . the development of gems system was informed by the results of an extensive algorithmic evaluation using 11 microarray datasets . the gems system was further evaluated in two cross-dataset applications and using 5 microarray datasets . the performance of models produced by gems system is comparable or better than the results obtained by human analysts , and these models generalize well to independent samples in cross-dataset applications . the gems system is freely available for download from http://www.gems-system.org for non-commercial use .
while discriminative training -lrb- e.g. , crf , structural svm -rrb- holds much promise for machine translation , image segmentation , and clustering , the complex inference these applications require make exact training intractable . this leads to a need for approximate training methods . unfortunately , knowledge about how to perform efficient and effective approximate training is limited . focusing on structural svms , we provide and explore algorithms for two different classes of approximate training algorithms , which we call undergenerating -lrb- e.g. , greedy -rrb- and overgenerating -lrb- e.g. , relaxations -rrb- algorithms . we provide a theoretical and empirical analysis of both types of approximate trained structural svms , focusing on fully connected pairwise markov random fields . we find that models trained with overgenerating methods have theoretic advantages over undergenerating methods , are empirically robust relative to their undergenerating brethren , and relaxed trained models favor non-fractional predictions from relaxed predictors .
we consider the problem of sensor node localization in a randomly deployed sensor network , using a mobile access point . the mobile access point can be used to localize many sensors simultaneously in a broadcast mode , without a pre-established sensor network . we consider a multi-modal approach , combining radio and acoustics . the radio broadcasts timing , location information , and acoustic signal parameters . the acoustic emission may be used at the sensor to measure doppler stretch , time delay , and angle of arrival . these acoustic signal parameters are individually sufficient to localize a sensor node , or they may be advantageously combined . we focus on the cases of doppler and time delay . sensor local-ization algorithms are developed , and performance analysis includes acoustic propagation effects caused by the turbulent atmosphere .
we present a method for tracking a hand while it is interacting with an object . this setting is arguably the one where hand-tracking has most practical relevance , but poses significant additional challenges : strong occlusions by the object as well as self-occlusions are the norm , and classical anatomical constraints need to be softened due to the external forces between hand and object . to achieve robustness to partial occlusions , we use an individual local tracker for each segment of the articulated structure . the segments are connected in a pairwise markov random field , which enforces the anatomical hand structure through soft constraints on the joints between adjacent segments . the most likely hand configuration is found with belief propagation . both range and color data are used as input . experiments are presented for synthetic data with ground truth and for real data of people manipulating objects .
typical user demands of electricity vary throughout the day , which increases the cost to utility companies and decreases the stability of the power system . time-of-use pricing has been proposed as a demand-side management method to influence user demands . in this paper , we describe a new approach of optimal tou pricing strategy based on game theory . we propose models for costs due to the fluctuating user demands to the utility companies , as well as the user satisfaction measurement because of the difference between the demand and actual load . we design utility functions for the company and the user , and obtain the nash equilibrium using backward induction and iterative methods . numerical example shows that our method is effective in lev-eling the user demand by setting optimal tou prices , in potentially increasing the profit of the utility companies and ensuring overall user benefit .
the ability to perform reasoning on inconsistent data is a central problem both for ai and database research . one approach to deal with this situation is consistent query answering , where queries are answered over all possible repairs of the database . in general , the repair may be very distant from the original database . in this work we present a new approach where this distance is bounded and analyze its computational complexity . our results show that in many -lrb- but not all -rrb- cases the complexity drops .
transform/subband representations form a basic building block for many signal processing algorithms and applications . most of the eort has focused on developing representations for innite-length signals , with simple extensions to nite-length 1-d and rectangular support 2-d signals . however , many signals may h a v e arbitrary length or arbitrarily shaped -lrb- as -rrb- regions of support -lrb- ros -rrb- . we present a novel framework for creating critically sampled perfect reconstruction transform/subband representations for as signals . our method selects an appropriate subset of vectors from an -lrb- easily obtained -rrb- basis for a larger -lrb- superset -rrb- signal space , in order to form a basis for the as signal . in particular , we have developed a number of promising wavelet representations for arbitrary-length 1-d signals and as 2-d/m-d signals that provide high performance with low complexity .
in this paper , we study the problem of learning a metric and propose a loss function based metric learning framework , in which the metric is estimated by minimizing an empirical risk over a training set . with mild conditions on the instance distribution and the used loss function , we prove that the empirical risk converges to its expected counterpart at rate of root-n . in addition , with the assumption that the best metric that minimizes the expected risk is bounded , we prove that the learned metric is consistent . two example algorithms are presented by using the proposed loss function based metric learning framework , each of which uses a log loss function and a smoothed hinge loss function , respectively . experimental results suggest the effectiveness of the proposed algorithms .
if we take an existing supervised nlp system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features . we evaluate brown clusters , collobert and weston -lrb- 2008 -rrb- embeddings , and hlbl -lrb- mnih & hinton , 2009 -rrb- embeddings of words on both ner and chunking . we use near state-of-the-art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines . we find further improvements by combining different word representations . you can download our word features , for off-the-shelf use in existing nlp systems , as well as our word features , here : http://metaoptimize . com/projects/wordreprs /
in this paper we address the problem of obtaining structured information about products in the form of attribute-value pairs by leveraging a combination of enterprise internal product descriptions and external data . product descriptions are short text product descriptions used internally within enterprises to describe a product . these product descriptions usually comprise of the brand name , name of the product , and its attributes like size , color , etc. . existing product data quality solutions provide us the capability to standardize and segment these descriptions into their composing attributes using domain specific rulesets . we provide techniques that can leverage the supervision provided by these existing rulesets for extracting missing values from other external text data sources accurately . we use a large real life data collection to demonstrate the effectiveness of our approach .
the ability to comprehend wishes or desires and their fulfillment is important to natural language understanding . this paper introduces the task of identifying if a desire expressed by a subject in a given short piece of text was fulfilled . we propose various unstructured and structured models that capture fulfillment cues such as the subject 's emotional state and actions . our experiments with two different datasets demonstrate the importance of understanding the narrative and discourse structure to address this task .
we 've examined the speaker discriminative power of mel - , antimel-and linear-frequency cepstral coefficients -lrb- a-mfccs , a-mfccs and a-mfccs -rrb- in the nasal , vowel , and non-nasal consonant speech regions . our inspiration came from the work of lu and dang in 2007 , who showed that filterbank energies at some frequencies mainly outside the telephone bandwidth possess more speaker discriminative power due to physiological characteristics of speakers , and derived a set of cepstral coefficients that outperformed a-mfccs in non-telephone speech . using telephone speech , we 've discovered that a-mfccs gave 21.5 % and 15.0 % relative eer improvements over a-mfccs in nasal and non-nasal consonant regions , agreeing with our filterbank energy f-ratio analysis . we 've also found that using only the vowel region with a-mfccs gives a 9.1 % relative improvement over using all speech . last , we 've shown that a-mfccs are valuable in combination , contributing to a system with 17.3 % relative improvement over our baseline .
the length and complexity of a linear equalizer filter is highly dependent on the nature of the channel effects it must mitigate . the governing design rules are typically stated in terms of the channel 's temporal characteristics , i.e. impulse response duration . equalizer implementa-tional complexity is a principal limiting factor for high bandwidth data communication applications , and consequently there is motivation for reexamining accepted design guidelines . recently , it was demonstrated in -lsb- 1 -rsb- that for relatively benign conditions on the effective channel and transmitter pulse shaping , there exists a linear equalizer that perfectly mitigates intersymbol interference , and whose span matches that of the composite distortion . our paper examines the implications of the minimal-length equalizer in light of accepted design rules , and shows that a tangible loss in performance can be assigned to this complexity reduction . actual line-of-sight microwave radio channels are used to demonstrate the nature of the performance loss .
our goal is to incorporate polarization in appearance-based modeling in an efficient and meaningful way . polarization has been used in numerous prior studies for separating diffuse and specular reflectance components , but in this work we show that polarization also can be used to separate surface reflectance contributions from individual light sources . our approach is called polarization multiplexing and polarization has significant impact in appearance modeling and bidirectional imaging where the image as a function of illumination direction is needed . multiple unknown light sources can illuminate the scene simultaneously , and the individual contributions to the overall surface reflectance can be estimated . to develop the method of polarization multiplexing , we use a relationship between light source direction and intensity modulation . inverting this transformation enables the individual intensity contributions to be estimated . in addition to polarization multiplexing , we show that phase his-tograms from the intensity modulations can be used to estimate scene properties including the number of light sources .
we introduce dropconnect , a generalization of dropout -lrb- hinton et al. , 2012 -rrb- , for regular-izing large fully-connected layers within neu-ral networks . when training with dropout , a randomly selected subset of activations are set to zero within each layer . dropconnect instead sets a randomly selected subset of weights within the network to zero . each unit thus receives input from a random subset of units in the previous layer . we derive a bound on the generalization performance of both dropout and dropconnect . we then evaluate dropconnect on a range of datasets , comparing to dropout , and show state-of-the-art results on several image recognition benchmarks by aggregating multiple dropconnect-trained models .
we introduce a generic scheme for accelerating first-order optimization methods in the sense of nesterov , which builds upon a new analysis of the accelerated prox-imal point algorithm . our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems , leading to faster convergence . this strategy applies to a large class of algorithms , including gradient descent , block coordinate descent , sag , saga , sdca , svrg , finito/miso , and their proximal variants . for all of these methods , we provide acceleration and explicit support for non-strongly convex objectives . in addition to theoretical speed-up , we also show that acceleration is useful in practice , especially for ill-conditioned problems where we measure significant improvements .
the focus of this paper is the modeling of a class of stationary non-gaussian auto-regressive processes that often find applications in statistical signal processing . we propose a general simulation procedure for constructing a time series model with a near-laplace marginal distributions . our approach is based on a class of monte carlo rejection algorithms . a theoretical analysis of the average complexity of the proposed algorithms for simulating the time series model is included .
a wide variety of dirichlet-multinomial ` topic ' models have found interesting applications in recent years . while gibbs sampling remains an important method of inference in such dirichlet-multinomial ` topic ' models , variational techniques have certain advantages such as easy assessment of convergence , easy optimization without the need to maintain detailed balance , a bound on the marginal likelihood , and side-stepping of issues with topic-identifiability . the most accurate variational technique thus far , namely collapsed variational latent dirichlet allocation , did not deal with model selection nor did variational technique include inference for hyperparameters . we address both issues by generalizing the technique , obtaining the first variational algorithm to deal with the hierarchical dirichlet process and to deal with hyperparameters of dirichlet variables . experiments show a significant improvement in accuracy .
a knowledge based system for text understanding will incorporate both lexical and encyclopaedic information . the lexical and encyclopaedic information is the basis of the parsing process while the encyclopaedic information forms the target representation and is used in the knowledge acquisition process . this paper describes twig , a text understanding system where these two knowledge bases arc integrated into one representation . there is some theoretical justification for this and knowledge based system has the advantage of reducing duplication of information in the knowledge based system . this integration also has the advantage of making conceptual information available during the parsing process . most of all this integration of diverse information forms a natural basis for a blackboard architecture .
we propose a network flow based optimization method for data association needed for multiple object tracking . the maximum-a-posteriori data association problem is mapped into a cost-flow network with a non-overlap constraint on trajectories . the optimal data association is found by a min-cost flow algorithm in the cost-flow network . the cost-flow network is augmented to include an explicit occlusion model to track with long-term inter-object occlu-sions . a solution to the cost-flow network is found by an iterative approach built upon the original min-cost flow algorithm . initialization and termination of trajectories and potential false observations are modeled by the formulation intrinsi-cally . the method is efficient and does not require hypotheses pruning . performance is compared with previous results on two public pedestrian datasets to show its improvement .
this paper describes how the performance of a continuous speech recognizer for dutch has been improved by modeling within-word and crossword pronunciation variation . a relative improvement of 8.8 % in wer was found compared to baseline system performance . however , as wer do not reveal the full effect of modeling pronunciation variation , we performed a detailed analysis of the differences in recognition results that occur due to modeling pronunciation variation and found that indeed a lot of the differences in recognition results are not reflected in the error rates . furthermore , error analysis revealed that testing sets of variants in isolation does not predict their behavior in combination . however , these results appeared to be corpus dependent .
motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity . binary codes are well suited to large-scale applications as binary codes are storage efficient and permit exact sub-linear knn search . the framework is applicable to broad families of mappings , and uses a flexible form of triplet ranking loss . we overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss , inspired by latent structural svms . we develop a new loss-augmented inference algorithm that is quadratic in the code length . we show strong retrieval performance on cifar-10 and mnist , with promising classification results using no more than knn on the binary codes .
this blue sky presentation focuses on a major shift toward a notion of '' ambient intelligence '' that transcends general applications targeted at the general population . the focus is on highly personalized agents that accommodate individual differences and changes over time . this notion of extended ambient intelligence concerns adaptation to a person 's preferences and experiences , as well as changing capabilities , most notably in an environment where conversational engagement is central . an important step in moving this research forward is the accommodation of different degrees of cognitive capability -lrb- including speech processing -rrb- that may vary over time for a given user -- whether through improvement or through deterioration . we suggest that the application of divergence detection to speech patterns may enable adaptation to a speaker 's increasing or decreasing level of speech impairment over time . taking an adaptive approach toward technology development in this arena may be a first step toward empowering those with special needs so that they may live with a high quality of life . it also represents an important step toward a notion of ambient intelligence that is personalized beyond what can be achieved by mass-produced , one-size-fits-all software currently in use on mobile devices .
with the popularity of online multimedia videos , there has been much interest in recent years in acoustic event detection and classification for the improvement of online video search . the audio component of a video has the potential to contribute significantly to multimedia event classification . recent research in audio document classification has drawn parallels to text and image document retrieval by employing what is referred to as the bag-of-audio words method . compared to supervised approaches where audio concept detectors are trained using annotated data and extracted labels are used as low-level features for multimedia event classification . the bag-of-audio words method extracts audio concepts in an unsupervised fashion . hence this bag-of-audio words method has the advantage that bag-of-audio words method can be employed easily for a new set of audio concepts in multimedia videos without going through a laborious annotation effort . in this paper , we explore variations of the bag-of-audio words method and present results on nist 2011 multimedia event detection -lrb- med -rrb- dataset .
the super-exponential algorithm is a block-based technique for blind channel equalization and system identii-cation . due to its fast convergence rate , and no a pri-ori parameterization other than the block length , it is a useful tool for linear equalization of moderately distortive channels . this paper presents a recursive implementation of the super-exponential algorithm for fractionally-sampled pam signals . although the resulting super-exponential algorithm is still block-based , recursive propagation of several key variables allows the block length to be signiicantly reduced without compromising the super-exponential algorithm 's accuracy or speed , thereby enhancing its ability t o t r a c k c hannel variations . the convergence rate is only mildly innuenced by speciic channel responses , and oversampling provides smaller output variance and almost perfect tolerance to sampling errors . simulation results demonstrate the eeectiveness of the proposed super-exponential algorithm .
in the regression context , boosting and bagging are techniques to build a committee of regressors that may be superior to a single regressor . we use regression trees as fundamental building blocks in bagging committee machines and boosting committee machines . performance is analyzed on three non-linear functions and the boston housing database . in all cases , boosting is at least equivalent , and in most cases better than bagging in terms of prediction error .
we present a linear method for global camera pose registration from pairwise relative poses encoded in essential matrices . our linear method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets . this linear method does not suffer from the typical ` unbalanced scale ' problem in linear methods relying on pairwise translation direction constraints , i.e. an algebraic error ; nor the system degeneracy from collinear motion . in the case of three cameras , our linear method provides a good linear approximation of the trifocal tensor . linear method can be directly scaled up to register multiple cameras . the results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment . we evaluate the linear method performance with different types of data and demonstrate its effectiveness . our linear method produces good accuracy , robustness , and outperforms some well-known systems on efficiency .
microphone arrays are beneficial for distant speech capture because the signals they capture can be exploited with beamforming to suppress noise and reverberation . the theory for the design and analysis of microphone arrays is well established , however the performance of a microphone array beamformer is often subject to conflicting criteria that need to be assessed manually . this paper describes bfgui , a interactive graphical tool for bfgui , for simulating microphone arrays and synthesizing beamformers , and whose parameters can be modified and performance metrics monitored in real-time . primarily aimed at teaching and research , this bfgui provides the user with an intuitive insight into the effects of microphone types , number and geometry , and the influence of design constraints such as regulariza-tion and white noise gain on derived metrics . the resulting direc-tivity pattern , directivity index and front-back ratio are examples of such metrics . multiple analytic microphone models are supported and external measured microphone directivity patterns can also be loaded . the multiple analytic microphone models can be then exported in a variety of formats for processing of real-world data .
this paper addresses the problems involved in performing speech recognition over mobile and ip networks . the main problem is speech data loss caused by packet loss in the network . we present two missing-feature-based approaches that recover lost regions of speech data . these missing-feature-based approaches are based on reconstruction of missing frames or on marginal distributions . for comparison , we also use a tacking method , which recognizes only received data . we evaluate these missing-feature-based approaches with packet loss models , i.e. , random loss and gilbert loss models . the results show that the missing-feature-based approaches is most effective for a packet loss environment ; the degradation of word accuracy is only 5 % when the packet loss rate is 30 % and only 3 % when mean burst loss length is 24 frames .
in this paper we propose a low-complexity fuzzy video rate control algorithm with buffer constraint designed for real-time streaming applications . while in low delay video communications bit streams with constant bitrate are required , in streaming application more delay and variation in bitrate is acceptable . the described video rate control algorithm provides a variable bitrate video by control of the quantization scale on picture basis . the quantization scale is mainly controlled by a fuzzy controller such that it minimizes the variation of quantization scale to provide encoded video with high visual quality so as to utilize the variable bitrate benefits as much as possible . the proposed rate control algorithm -lrb- rca -rrb- has been implemented in the mpeg-4 , h. 263 and h. 264/avc standard video codecs and the experimental results show that it provides high level average quality for encoded video while it strictly obeys streaming constraints .
automatic speech recognition -lrb- asr -rrb- is difficult in environments such as multiparty meetings because of adverse acoustic conditions : background noise , reverberation and cross-talk . microphone arrays can increase asr accuracy dramatically in such situations . however , most existing beamforming techniques use time-domain signal processing theory and are based on a geometric analysis of the relationship between sources and microphones . this limits their application , and leads to performance degradation when the geometric properties are unavailable , or heterogeneous channels are used . we present a new posterior-based approach for microphone array speech recognition . instead of enhancing speech signals , we enhance posterior phone probabilities which are used in a tandem ann-hmm system . significant improvements were achieved over a single channel baseline . combining beamforming and our posterior-based approach is significantly better than beamforming alone , especially in a moving speakers scenario .
in this paper , automatic dysarthria severity classification is explored as a tool to advance objective intelli-gibility prediction of spastic dysarthric speech . a ma-halanobis distance-based discriminant analysis classifier is developed based on a set of acoustic features formerly proposed for intelligibility prediction and voice pathology assessment . feature selection is used to sift salient features for both the disorder severity classification and intelligibility prediction tasks . experimental results show that a two-level severity classifier combined with a 9-dimensional intelligibility prediction mapping can achieve 0.92 correlation and 12.52 root-mean-square error with subjective intelligibility ratings . the effects of classification errors on intelligibility accuracy are also explored and shown to be insignificant .
a high-parallel vlsi core architecture for mpeg-4 motion estimation is proposed in this paper . high-parallel vlsi core architecture possesses the characteristics of low memory bandwidth and low clock rate requirements , thus primarily aiming at 3g mobile applications . based on a one-dimensional tree architecture , the high-parallel vlsi core architecture employs the dual-register/buffer technique to reduce the preload and alignment cycles . as an example , full-search block matching algorithm has been mapped onto this high-parallel vlsi core architecture using a 16-pe array that has the ability to calculate the motion vectors of qcif video sequences in real time at 1 mhz clock rate and using 15.5 mbytes/s memory bandwidth .
in many areas of signal processing , the phases of complex-valued random variables are used to estimate system parameters , the magnitudes being discarded . in this paper , we consider the implications of doing this : the loss of statistical information and subsequent increase in asymptotic variance . two particular cases , those of estimating the phase of the mean of a complex distribution , and estimating the frequency of a complex sinusoid in white noise , are considered . the estimators are motivated by estimation under von mises distributional assumptions . the asymptotic distributional properties are obtained under general assumptions , and are tested using a small number of simulations .
in this paper a system was developed for robot behavior acquisition using kinesthetic demonstrations . it enables a humanoid robot to imitate constrained reaching gestures directed towards a target using a learning algorithm based on gaussian mixture models . the imitation trajectory can be reshaped in order to satisfy the constraints of the task and imitation trajectory can adapt to changes in the initial conditions and to target displacements occurring during movement execution . the potential of this method was evaluated using experiments with the nao , aldebaran 's humanoid robot .
in this paper , we present an empirical study that utilizes morph-syntactical information to improve translation quality . with three kinds of language pairs matched according to morph-syntactical similarity or difference , we investigate the effects of various morpho-syntactical information , such as base form , part-of-speech , and the relative positional information of a word in a statistical machine translation framework . we learn not only translation models but also word-based/class-based language models by manipulating morphological and relative positional information . and we integrate the models into a log-linear model . experiments on multilingual translations showed that such morphological and relative positional information as part-of-speech and base form are effective for improving performance in morphologically rich language pairs and that the relative positional features in a word group are useful for reordering the local word orders . moreover , the use of a class-based n-gram language model improves performance by alleviating the data sparseness problem in a word-based language model .
four dimensional medical data are sequences of volumetric images captured in time . these data sets are typically very large in size and demand a great amount of resources for storage and transmission . in this paper , we present a lossless compression technique for 4d medical images which is based on the h. 264/avc video coding standard . our lossless compression technique efficiently exploits spatial and temporal redundancies between 2d image slices and 3d images in 4d medical images and eliminates any concerns regarding the effects of compression on image quality for diagnostic purposes . performance evaluations have shown that the proposed lossless compression technique outperforms current 4d compression methods by 70 % .
mixture of gaussian processes mixture of gaussian processes models extended a single gaussian process with ability of modeling multi-modal data and reduction of training complexity . previous inference algorithms for these mixture of gaussian processes models are mostly based on gibbs sampling , which can be very slow , particularly for large-scale data sets . we present a new generative mixture of experts model . each expert is still a gaussian process but is reformulated by a linear model . this breaks the dependency among training outputs and enables us to use a much faster variational bayesian algorithm for training . our variational bayesian algorithm is more flexible than previous generative approaches as inputs for each expert are modeled by a gaussian mixture model . the number of experts and number of gaussian components for an expert are inferred automatically . a variety of tests show the advantages of our method .
-- t-sne is a well-known approach to embedding high-dimensional data and has been widely used in data visualization . the basic assumption of t-sne is that the data are non-constrained in the euclidean space and the local proximity can be modelled by gaussian distributions . this assumption does not hold for a wide range of data types in practical applications , for instance spherical data for which the local proximity is better modelled by the von mises-fisher distribution instead of the gaussian distributions . this paper presents a vmf-sne embedding algorithm to embed spherical data . an iterative process is derived to produce an efficient embedding . the results on a simulation data set demonstrated that t-sne produces better embeddings than t-sne for spherical data .
givealink.org is a social bookmarking site where users may donate and view their personal bookmark files online securely . the bookmarks are analyzed to build a new generation of intelligent information retrieval techniques to recommend , search , and personalize the web . givealink.org does not use tags , content , or links in the submitted web pages . instead we present a semantic similarity measure for urls that takes advantage both of the hierarchical structure in the bookmark files of individual users , and of collaborative filtering across users . in addition , we build a recommendation and search engine from ranking algorithms based on popularity and novelty measures extracted from the similarity-induced network . search results can be personalized using the bookmarks submitted by a user . we evaluate a subset of the proposed semantic similarity measure by conducting a study with human subjects .
in many domains generalized plans can only be computed if certain high-level state features , i.e. features that capture key concepts to accurately distinguish between states and make good decisions , are available . in most applications of generalized planning such features are hand-coded by an expert . this paper presents a novel method to automatically generate high-level state features for solving a generalized planning problem . our method extends a compilation of generalized planning into classical planning and integrates the computation of generalized plans with the computation of features , in the form of conjunctive queries . experiments show that we generate features for diverse generalized planning problems and hence , compute generalized plans without providing a prior high-level representation of the states . we also bring a new landscape of challenging benchmarks to classical planning since our compilation naturally models classification tasks as classical planning problems .
we present the first tractable , exact solution for the problem of identifying actions ' effects in partially observable strips domains . our algorithms resemble version spaces and logical filtering , and they identify all the models that are consistent with observations . they apply in other deterministic domains -lrb- e.g. , with conditional effects -rrb- , but are inexact -lrb- may return false positives -rrb- or inefficient -lrb- we could not bound the representation size -rrb- . our experiments verify the theoretical guarantees , and show that we learn strips actions efficiently , with time that is significantly better than approaches for hmms and reinforcement learning -lrb- which are inexact -rrb- . our results are especially surprising because of the inherent intractability of the general deterministic case . these results have been applied to an autonomous agent in a virtual world , facilitating decision making , diagnosis , and exploration .
this paper is concerned with the design of an optimal set of analog signals with prescribed magnitude spectrum and quadratic phase structure such that the maximum cross-correlation is minimized . an analytic expression for the maximum cross-correlation between two signals is derived through mathematical analysis . the optimal set of signals with the lowest maximum cross-correlation is explicitly characterized under certain conditions .
we present a novel approach to solving quantified boolean formulas that combines a search-based qbf solver with machine learning techniques . we show how classification methods can be used to predict run-times and to choose optimal heuristics both within a portfolio-based , and within a dynamic , online approach . in the dynamic method variables are set to a truth value according to a scheme that tries to maximize the probability of successfully solving the remaining sub-problem efficiently . since each variable assignment can drastically change the problem-structure , new heuristics are chosen dynamically , and a classifier is used online to predict the usefulness of each heuristic . experimental results on a large corpus of example problems show the usefulness of our approach in terms of run-time as well as the ability to solve previously unsolved problem instances .
traditional camera models are often the result of a compromise between the ability to account for non-linearities in the image formation model and the need for a feasible number of degrees of freedom in the estimation process . these considerations led to the definition of several ad hoc models that best adapt to different imaging devices , ranging from pinhole cameras with no radial distortion to the more complex catadioptric or polydioptric optics . in this paper we propose the use of an unconstrained model even in standard central camera settings dominated by the ad hoc models , and introduce a novel calibration approach that can deal effectively with the huge number of free parameters associated with unconstrained model , resulting in a higher precision calibration than what is possible with the standard ad hoc models with correction for radial distortion . this effectively extends the use of general models to settings that traditionally have been ruled by parametric approaches out of practical considerations . the benefit of such an unconstrained model to quasi-pinhole central cameras is supported by an extensive experimental validation .
in this paper , we propose a two-stage transcription task design for crowdsourcing with an automatic quality control mechanism embedded in each stage . for the first stage , a support vector machine classifier is utilized to quickly filter poor quality transcripts based on acoustic cues and language patterns in the transcript . in the second stage , word level confidence scores are used to estimate a transcription quality and provide instantaneous feedback to the transcriber . the proposed two-stage transcription task design was evaluated using amazon mechanical turk and tested on seven hours of academic lecture speech , which is typically conversational in nature and contains technical material . compared to baseline transcripts which were also collected from mturk using a rover-based method , we observed that the new two-stage transcription task design resulted in higher quality transcripts while requiring less transcriber effort .
as one of the major chinese dialects , hakka typically has a tone system with six lexical tones . the traditional 5-level notation of tones in hakka varies in previous references due to its subjective and relative nature . in order to overcome the limitations of the traditional approach , the command-response model for the process of f 0 contour generation is employed to analyze quantitatively the tones in continuous speech of two varieties of hakka , spoken in meixian and in shataukok , respectively . by providing both phonological descriptions to each tone type and quantitative approximations to continuous f 0 contours , the model-based approach provides an efficient connection between phonetics and phonology of hakka tones .
we derive a new set of necessary and sufficient conditions for the filter coefficients of the two-scale difference equation to yield an orthogonal wavelet of compact support . the conditions constitute a linear set of equations of an arbitrary decision vector of half the filter size . the vector of the filter coefficients is a differentiable function of the decision vector . the formulation enables the optimization of the filter design under any regular objective function . the proposed parametrization is used to design customized orthonormal wavelets and to reproduce the classical orthogonal wavelets as a solution of a nonlinear optimization problem .
multiplexing is a common technique for encoding high-dimensional image data into a single , two-dimensional image . examples of spatial multiplexing include bayer patterns to capture color channels , and integral images to encode light fields . in the fourier domain , optical heterodyn-ing has been used to acquire light fields . in this paper , we develop a general theory of multiplex-ing the dimensions of the plenoptic function onto an image sensor . our theory enables a principled comparison of plenoptic multiplexing schemes , including noise analysis , as well as the development of a generic reconstruction algorithm . the framework also aides in the identification and optimization of novel multiplexed imaging applications .
in this paper we develop a spectral framework for estimating mixture distributions , specifically gaussian mixture models . in physics , spectroscopy is often used for the identification of substances through their spectrum . treating a kernel function <i> k -lrb- x , y -rrb- </i> as `` light '' and the sampled data as `` substance '' , the spectrum of their interaction -lrb- eigenvalues and eigenvectors of the kernel matrix <i> k </i> -rrb- unveils certain aspects of the underlying parametric distribution <i> p </i> , such as the parameters of a gaussian mixture . our spectral framework extends the intuitions and analyses underlying the existing spectral techniques , such as spectral clustering and kernel principal components analysis . we construct spectral framework to estimate parameters of gaussian mixture models , including the number of mixture components , their means and covariance matrices , which are important in many practical applications . we provide a theoretical framework and show encouraging experimental results .
genome-wide expression data consists of millions of measurements towards large number of genes , and thus gene expression data is challenging for human beings to directly analyze such large-scale data . clustering provides a more convenient way to analyze gene expression data because gene expression data can subdivide raw data into comprehensive classes . however , the number of probed genes is rather greater than the number of samples , and this makes conventional clustering methods perform unsatisfactorily . in this paper , we propose a gauss-seidel based non-negative matrix factorization -lrb- gsnmf -rrb- method to overcome such imbalance deficiency between features and samples . in particular , gsnmf iteratively projects gene expression data onto the learned subspace followed by adaptively updating the cluster centroids based on the projected data . since this gene expression data significantly reduces the influence of imbalance between the number of samples and the number of genes , gsnmf performs better than traditional clustering methods in gene expression clustering . since gsnmf updates each factor matrix by solution of a linear system obtained by the gauss-seidel method , gene expression data converges rapidly without neither complex line search nor matrix inverse operators . experimental results on several cancer expression datasets confirm both efficiency and effectiveness of gsnmf comparing with the representative nmf methods and conventional clustering methods .
while the resolution of term ambiguity is important for information extraction systems , the cost of resolving each instance of an entity can be prohibitively expensive on large datasets . to combat this , this work looks at ambiguity detection at the term , rather than the instance , level . by making a judgment about the general ambiguity of a term , a system is able to handle ambiguous and unambigu-ous cases differently , improving through-put and quality . to address the term ambiguity detection problem , we employ a model that combines data from language models , ontologies , and topic mod-eling . results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline f-measure of 0.96 .
this paper introduces the minimal local reconstruction error as a similarity measure and presents a mlre-based classifier . from the geometric meaning of the minimal local reconstruction error , we derive that the mlre-based classifier is a generalization of the conventional nearest neighbor classifier and the nearest neighbor line and plane classifiers . we further apply the minimal local reconstruction error to characterize the within-class and between-class local scatters and then develop a mlre measure based discriminant feature extraction method . the proposed mlre measure based discriminant feature extraction method is in line with the mlre-based classification method in spirit , thus the two methods can be seamlessly combined in applications . the experimental results on the cenparmi handwritten numeral database and the feret face image database show effectiveness of the proposed mlre-based feature extraction and classification method .
we present a multi-microphone signal activity detection scheme for hearing aids to differentiate between the periods of activity of desired and interfering sources . the multi-microphone signal activity detection scheme is designed to provide robust performance in the presence of simultaneously active desired and interfering sources . we exploit knowledge from the hearing aid domain , and the directional processing present in modern hearing aids , to present a multi-microphone signal activity detection scheme to design appropriate thresholds for the detection . experiments confirm robust performance under practical reverberant conditions .
local part-based human detectors are capable of handling partial occlusions efficiently and modeling shape ar-ticulations flexibly , while global shape template-based human detectors are capable of detecting and segmenting human shapes simultaneously . we describe a bayesian approach to human detection and segmentation combining local part-based and global template-based schemes . the bayesian approach relies on the key ideas of matching a part-template tree to images hierarchically to generate a reliable set of detection hypotheses and optimizing bayesian approach under a bayesian map framework through global likelihood re-evaluation and fine occlusion analysis . in addition to detection , our bayesian approach is able to obtain human shapes and poses simultaneously . we applied the bayesian approach to human detection and segmentation in crowded scenes with and without background subtraction . experimental results show that our bayesian approach achieves good performance on images and video sequences with severe occlusion .
while convolutional neural networks have been excellent for object recognition , the greater spatial variability in scene images typically meant that the standard full-image cnn features are suboptimal for scene classification . in this paper , we investigate a framework allowing greater spatial flexibility , in which the fisher vector -lrb- fv -rrb- encoded distribution of local cnn features , obtained from a multitude of region proposals per image , is considered instead . the cnn features are computed from an augmented pixel-wise representation comprising multiple modalities of rgb , hha and surface normals , as extracted from rgb-d data . more significantly , we make two postulates : -lrb- 1 -rrb- component sparsity -- that only a small variety of region proposals and their corresponding fv gmm components contribute to scene discriminability , and -lrb- 2 -rrb- modal non-sparsity -- within these discriminative components , all modalities have important contribution . in our framework , these are implemented through regularization terms applying group lasso to gmm components and exclusive group lasso across modalities . by learning and combining regres-sors for both proposal-based fv features and global cn-n features , we were able to achieve state-of-the-art scene classification performance on the sunrgbd dataset and nyu depth dataset v2 .
the computation and memory required for kernel machines with n training samples is at least o -lrb- n 2 -rrb- . such a complexity is significant even for moderate size problems and is prohibitive for large datasets . we present an approximation technique based on the improved fast gauss transform to reduce the computation to o -lrb- n -rrb- . we also give an error bound for the approximation , and provide experimental results on the uci datasets .
in this paper , we propose some novel normalization and fusion techniques for biometric matching score level fusion in person verification . while conventional matching score level fusion methods use global score statistics , we consider in this work both genuine and impostor statistics separately . performing a joint mean normalization of the separate monomodal scores , multimodal scores with less separate variance than the monomodal ones are obtained . furthermore , a weighting method has been designed in order to minimize the variance sum of the separate multimodal statistics . this weighting method obtains a minor sum of genuine and impostor variances for the multimodal biometric than that of the monomodal ones . the results obtained in speech and face scores fusion upon polycost and xm2vts databases show that the proposed normalization and fusion techniques provide better results than the conventional methods .
the principle of maximum entropy provides a powerful framework for statistical models of joint , conditional , and marginal distributions . however , there are many important distributions with elements of interaction and feedback where its applicability has not been established . this work presents the principle of maximum causal entropy -- an approach based on causally conditioned probabilities that can appropriately model the availability and influence of sequentially revealed side information . using this principle , we derive models for sequential data with revealed information , interaction , and feedback , and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks .
we propose a novel representation of continuous , closed curves in ℝ -lrb- n -rrb- that is quite efficient for analyzing their shapes . we combine the strengths of two important ideas - elastic shape metric and path-straightening methods - in shape analysis and present a fast algorithm for finding geodesics in shape spaces . the fast algorithm allows for optimal matching of features while path-straightening provides geodesics between curves . efficiency results from the fact that the fast algorithm becomes the simple -lrb- 2 -rrb- metric in the proposed representation . we present step-by-step algorithms for computing geodesics in this fast algorithm , and demonstrate step-by-step algorithms with 2-d as well as 3-d examples .
we present a novel approach for detecting global behaviour anomalies in multiple disjoint cameras by learning time delayed dependencies between activities cross camera views . specifically , we propose to model multi-camera activities using a time delayed probabilistic graphical model with different nodes representing activities in different semantically decomposed regions from different camera views , and the directed links between nodes encoding causal relationships between the activities . a novel two-stage structure learning algorithm is formulated to learn globally optimised time-delayed dependencies . a new cumulative abnormality score is also introduced to replace the conventional log-likelihood score for gaining significantly more robust and reliable real-time anomaly detection . the effectiveness of the proposed approach is validated using a camera network installed at a busy underground station .
texture analysis is an image processing task that can be conducted using the mathematical framework of texture analysis to study the regularity fluctuations of image intensity and the practical tools for their assessment , such as -lrb- wavelet -rrb- leaders . a recently introduced statistical model for leaders enables the bayesian estimation of multifractal parameters . statistical model significantly improves performance over standard -lrb- linear regression based -rrb- estimation . however , the computational cost induced by the associated nonstandard posterior distributions limits its application . the present work proposes an alternative bayesian model for texture analysis that leads to more efficient algorithms . statistical model relies on three original contributions : a novel generative model for the fourier coefficients of log-leaders ; an appropriate reparametrization for handling its inherent constraints ; a data-augmented bayesian model yielding standard conditional posterior distributions that can be sampled exactly . numerical simulations using synthetic multifractal images demonstrate the excellent performance of the proposed statistical model , both in terms of estimation quality and computational cost .
we propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering . like the latter , clustering method associates every data point with a vector in hilbert space , and like the former clustering method puts emphasis on their total sum , that is equal to the scale-space probability function . the novelty of our clustering method is the study of an operator in hilbert space , represented by the schrödinger equation of which the probability function is a solution . this schrödinger equation contains a potential function that can be derived analytically from the probability function . we associate minima of the potential with cluster centers . the clustering method has one variable parameter , the scale of its gaussian kernel . we demonstrate its applicability on known data sets . by limiting the evaluation of the schrödinger potential to the locations of data points , we can apply this clustering method to problems in high dimensions .
speech speech model combination with the background noise has been shown effective to improve the pattern classification rate of noisy speech . the speech model combination can be performed by the addition of the spectral statistics such as the means and the variances . since the speech feature for pattern classification has to be expressed in the cepstral domain , the combined spectral statistics have to be transferred into the cepstral domain for speech recognition . in our previous study , we have proposed a direct adaptation scheme of the cepstral variance that is without the mapping from the spectral domain to the cepstral domain . in this paper , an improved version to perform the adaptation is proposed . from the study , it is observed that the adapted variance can be expressed as a linear interpolation of the speech and the noise variances to obtain a comparable recognition rate that is obtained with the mapping process . due to the direct adaptation of the variances , a lot of computation can be reduced to perform the environmental adaptation .
in this paper we propose to use digital fractional sample delayers to perform high precision beam steering at the baseband sampling frequency . the major advantages of the proposed technique are that the fractional sample delayer used has a very flat magnitude response within the baseband width allowing greater than 20-bit resolution for a 35-tap finite impulse response filter . it also has a delay which is continuously variable providing resolutions greater than 220,000 ths of the baseband sampling time . owing to the signal delay being performed at the baseband rate , elements with different delays may be placed in parallel , allowing for the formation of multiple beams -lrb- e.g. tracking and surveillance capability simultaneously -rrb- .
most dft domain based speech enhancement methods are dependent on an estimate of the noise power spectral density . for non-stationary noise sources it is desirable to estimate the noise psd also in spectral regions where speech is present . in this paper a new method for noise tracking is presented , based on eigenvalue decompositions of correlation matrices that are constructed from time series of noisy dft coefficients . the presented method can estimate the noise psd at time-frequency points where both speech and noise are present . in comparison to state-of-the-art noise tracking algorithms the proposed algorithm reduces the estimation error between the estimated and the true noise psd and improves segmental snr when combined with an enhancement system with several db .
we describe an information-driven active selection approach to determine which detectors to deploy at which location in which frame of a video to minimize semantic class label uncertainty at every pixel , with the smallest computational cost that ensures a given uncertainty bound . we show minimal performance reduction compared to a '' paragon '' algorithm running all detectors at all locations in all frames , at a small fraction of the computational cost . our information-driven active selection approach can handle uncertainty in the labeling mechanism , so information-driven active selection approach can handle both '' oracles '' -lrb- manual annotation -rrb- or noisy detectors -lrb- automated annotation -rrb- .
planning with epistemic goals has received attention from both the dynamic logic and planning communities . in the single-agent case , under the epistemic closed-world assumption , epistemic planning can be reduced to contingent planning . however , it is inappropriate to make the epistemic closed-world assumption in some epistemic planning scenarios , for example , when the agent is not fully introspective , or when the agent wants to devise a generic plan that applies to a wide range of situations . in this paper , we propose a complete single-agent epis-temic planner without the epistemic closed-world assumption . we identify two normal forms of epistemic formulas : weak minimal epistemic dnf and weak minimal epistemic cnf , and present the progression and entailment algorithms based on these normal forms . we adapt the single-agent epis-temic planner for contingent planning from the literature as the main planning algorithm and develop a complete epistemic planner called single-agent epis-temic planner . our experimental results show that single-agent epis-temic planner can generate solutions effectively for most of the epistemic planning problems we have considered including those without the epistemic closed-world assumption .
a rapid design methodology for orthonormal wavelet transform cores has been developed . this rapid design methodology is based on a generic , scaleable rapid design methodology utilising time-interleaved coefficients for the wavelet transform filters . the rapid design methodology has been captured in vhdl and parameterised in terms of wavelet family , wavelet type , data word length and coefficient word length . the control circuit is embedded within the cores and allows control circuit to be cascaded without any interface glue logic for any desired level of decomposition . case studies for stand alone and cascaded silicon cores for single and multi-stage wavelet analysis respectively are reported . the design time to produce silicon layout of a wavelet based system has been reduced to typically less than a day . the cores are comparable in area and performance to hand-crafted designs . the rapid design methodology are portable across a range of foundries and are also applicable to fpga and pld implementations .
in our earlier work -lsb- 1 , 2 -rsb- , we developed a robust detector for multipath constrained environments when the transmitted signal is known . in this paper , we extend these results to the case where the transmitted signal is a random process . the approach i n -lsb- 1 , 2 -rsb- is to replace the orthogonal projection on the multipath signal subspace s by the orthogonal projection on a representation subspace g , such that g and s are close in the gap metric sense . when the signal is random , s is no longer a linear subspace but a set with a given structure . the gap metric applies only when s and g are subspaces . in this paper , we introduce the modi-ed de blockinection as the appropriate measure to be used in the random signal case . we design the representation subspace g to match the multipath signal set s in the modied de blockinection sense . wavelet multiresolution tools are used to facilitate the design .
we analyze the foundations of cyclic causal models for discrete variables , and compare structural equation models to an alternative semantics as the equilibrium distribution of a markov chain . we show under general conditions , discrete cyclic sems can not have independent noise ; even in the simplest case , cyclic structural equation models imply constraints on the noise . we give a formalization of an alternative markov chain equilibrium semantics which requires not only the causal graph , but also a sample order . we show how the resulting equilibrium is a function of the sample ordering , both theoretically and empirically .
in weighted voting games , each agent has a weight , and a coalition of players is deemed to be winning if its weight meets or exceeds the given quota . an agent 's power in such games is usually measured by her shapley value , which depends both on the agent 's weight and the quota . -lsb- zuckerman et al. , 2008 -rsb- show that one can alter a player 's power significantly by modifying the quota , and investigate some of the related algorithmic issues . in this paper , we answer a number of questions that were left open by -lsb- zuckerman et al. , 2008 -rsb- : we show that , even though deciding whether a quota maximizes or minimizes an agent 's shapley value is conp-hard , finding a shapley value-maximizing quota is easy . minimizing a player 's power appears to be more difficult . however , we propose and evaluate a heuristic for this problem , which takes into account the voter 's rank and the overall weight distribution . we also explore a number of other algorithmic issues related to quota manipulation .
a recent development in feature extraction is the use of neural network feature extractors , where the parameterized signal is passed through a neural network trained to discriminate between targets representing e.g. different phone classes or speakers . while the transformed feature representation often enhances class discrim-inability and thereby overall performance , the transformation performed by the neural network can not directly be interpreted by human experts . however , explicit knowledge about this transformation could lead to the definition of a simpler function on the input features which might eventually be incorporated into the basic pa-rameterization method . in this paper we investigate a rule extraction technique for transforming the trained neural network into a set of if-then rules capable of representing the transformation in a more transparent way , and apply it to the problem of distinguishing between the english fricative classes / f , v / and / s , z / from the timit and ogi numbers95 speech corpora .
in this paper , we propose a new method based on particle filters for maximum likelihood estimation of the parameters of autoregressive conditional heteroscedasticity and generalized autoregressive conditional heteroscedasticity models . our method is based on gradient descend method and active set method for maximizing the likelihood function over parameters under stationarity constraints . the gradient of the likelihood function of observation given the parameters of the model , which is needed for gradient based optimization algorithm , is estimated using particle methods . simulation results show the advantage of the proposed method over competing techniques .
in many real-world applications , euclidean distance in the original space is not good due to the curse of dimensionality . in this paper , we propose a new method , called discriminant neighborhood embedding -lrb- dne -rrb- , to learn an appropriate metric space for classification given finite training samples . we define a discriminant adjacent matrix in favor of classification task , i.e. , neighboring samples in the same class are squeezed but those in different classes are separated as far as possible . the optimal dimensionality of the metric space can be estimated by spectral analysis in the proposed method , which is of great significance for high-dimensional patterns . experiments with various datasets demonstrate the effectiveness of our method .
learning theory has largely focused on two main learning scenarios : the classical statistical setting where instances are drawn i.i.d. from a fixed distribution , and the adversarial scenario wherein , at every time step , an adversarially chosen instance is revealed to the player . it can be argued that in the real world neither of these assumptions is reasonable . we define the minimax value of a game where the adversary is restricted in his moves , capturing stochastic and non-stochastic assumptions on data . building on the sequential symmetrization approach , we define a notion of distribution-dependent rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case . the bounds let us immediately deduce variation-type bounds . we study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with infinite littlestone dimension learnable .
minwise hashing is a standard procedure in the context of search , for efficiently estimating set similarities in massive binary data such as text . recently , b-bit minwise hashing has been applied to large-scale learning and sublinear time near-neighbor search . the major drawback of minwise hashing is the expensive pre-processing , as the method requires applying -lrb- e.g. , -rrb- k = 200 to 500 permutations on the data . this paper presents a simple solution called one permutation hashing . conceptually , given a binary data matrix , we permute the columns once and divide the permuted columns evenly into k bins ; and we store , for each data vector , the smallest nonzero location in each bin . the probability analysis illustrates that this one permutation scheme should perform similarly to the original -lrb- k-permutation -rrb- minwise hashing . our experiments with training svm and logistic regression confirm that one permutation hashing can achieve similar -lrb- or even better -rrb- accuracies compared to the k-permutation scheme . see more details in arxiv :1208.1259 .
a novel combination of ideas from cognitive linguistics and spatial occupancy models in robotics has led to the wip -lrb- words into pictures -rrb- system . wip -lrb- words into pictures -rrb- system automatically generates depictions of natural language descriptions of indoor scenes . a qualitative layer in the conceptual representation of objects underlies a mechanism by which alternative depictions arise for qualitatively distinct interpretations , as often occurs as a result of deictic/intrinsic reference frame ambiguity . at the same time , a quantitative layer , in conjunction with a potential field model of the semantics of projective prepositions , is used in the process of capturing the inherently fuzzy character of the meaning of natural language spatial predications .
we describe the construction and characterization of an event-based hardware vision system that learns to classify spatio-temporal trajectories . our characterization so far showed that stimuli of two different shapes on a rotating disk could simultaneously be discriminated and their position extracted at level of the object chip . event-based hardware vision system is the largest aer system yet assembled . event-based hardware vision system is a step towards efficient architectures for data-driven adaptive real time vision systems .
bilateral or group text chatting over the internet has become a favoured pastime for many people across the world . yet it would seem that , in general , text chat is a severely impoverished mode of on-line communication compared to , e.g. , fully situated human-human spoken conversation , video conferencing , or even speaking over the telephone . this paper explores what happens when on-line multi-speaker conversation over the internet is added to text chat , creating what may become a widespread mode of communication in the near future . the system used is called the magic lounge . magic lounge offers a multimodal combination of text chat and spoken conversation for meetings and other encounters among ubiquitous users who may join the communication from workstations , pdas and wap phones . in addition , the system has a series of meeting history tools which provide various forms of structure to the spoken and text chat records of the meeting as it unfolds and after the meeting . the paper presents rather clear-cut results on the respective communicative roles of speech and text chat from a series of user tests with the system in which different groups of users performed scenarios designed to explore the combined use of text chat and speech . the results reported may generalise to a wide range of applications which combine text and spoken information representation .
we propose a geometric method for visual tracking , in which the 2-d affine motion of a given object template is estimated in a video sequence by means of coordinate-invariant particle filtering on the 2-d affine group aff -lrb- 2 -rrb- . visual tracking performance is further enhanced through a geometrically defined optimal importance function , obtained explicitly via taylor expansion of a principal component analysis based measurement function on aff -lrb- 2 -rrb- . the efficiency of our geometric method to visual tracking is demonstrated via comparative experiments .
code-switching speech is an utterance containing two or more languages . usually , the switching linguistic unit is in clause or word levels . in this paper , a two-stage framework is proposed , containing a language identifier and then a speech recognizer , to evaluate on a mandarin-taiwanese code-switching utterance . in the language identifier , we use multiple cues including acoustic , prosodic and phonetic features . in order to integrate the cues to distinguish one language from another , we used a maximum a posteriori decision rule to connect an acoustic model , a duration model and a language model . in the experiments , we have achieved 34.5 % -lrb- lid -rrb- and 17.7 % -lrb- asr -rrb- error rate reduction comparing with one stage lvcsr-based system .
beginning in 2001 , we have been developing models of the vocal tract to promote a more intuitive understanding of the theories for speech science for technical and non-technical students . in this paper , we compared and contrasted four newer models of the talking heads : our original model with a gel-type tongue , a similar model including teeth and palate , a third model with teeth and palate having a default low tongue height , and a fourth ultra-malleable model made completely of gel material . results are discussed regarding their strengths and weaknesses for educational purposes and articulatory training in speech pathology and language learning .
in this paper , the theory and design of a class of pr cosine-modulated nonuniform filter bank is proposed . pr cosine-modulated nonuniform filter bank is based on a structure previously proposed by cox , where the outputs of a uniform filter bank is combined or merged by means of the synthesis section of another filter bank with smaller channel number . simplifications are imposed on this structure so that the design procedure can be considerably simplified . due to the use of cmfb as the original and recombination filter banks , excellent filter quality and low design and implementation complexities can be achieved . problems with these merging techniques such as spectrum inversion , equivalent filter representations and protrusion cancellation are also addressed . as the merging is performed after the decimation , the arithmetic complexity is lower than other conventional approaches . design examples show that pr nonuniform filter banks with high stopband attenuation and low design and implementation complexities can be obtained by the proposed pr cosine-modulated nonuniform filter bank .
we study a pattern classification algorithm which has recently been proposed by vapnik and coworkers . pattern classification algorithm builds on a new inductive principle which assumes that in addition to positive and negative data , a third class of data is available , termed the universum . we assay the behavior of the pattern classification algorithm by establishing links with fisher discriminant analysis and oriented pca , as well as with an pca in a projected subspace -lrb- or , equivalently , with a data-dependent reduced kernel -rrb- . we also provide experimental results .
the eigenvalue decomposition of a hermitian matrix in terms of unitary matrices is well known . in this paper , we present an algorithm for the approximate evd of a para-hermitian system . here , the approximate diag-onalization is carried out successively by applying degree-1 finite impulse response -lrb- fir -rrb- paraunitary -lrb- pu -rrb- transformations . the system parameters are chosen to make the zeroth order diagonal energy nondecreasing at each stage . simulation results presented for the design of a signal-adapted pu filter bank show close agreement with the behavior of the infinite order principal component fb -lrb- pcfb -rrb- .
in all the efficient speaker recognition systems , the decision score is based on the average of the likelihood ratio computed on each frame of the sentence . except for the non speech frames which are rejected , each one has the same weight in this summation . this paper deals with the study of the speaker relevance of each frame . an automatic segmentation provides quasi stationary segments of variable length ; a weight is allocated to each frame in function of its segment position and a weighted mean of the likelihood ratio is then computed . experiments are performed with nist 2003 speaker evaluation database . they show that the frames near segment frontiers , that is to say the transient ones , are more speaker relevant than the middle frames of long segments which correspond to the steady parts of the phones .
the geometry of plane-based calibration methods is well understood , but some user interaction is often needed in practice for feature detection . this paper presents a fully automatic calibration system that uses patterns of pairs of concentric circles . the key observation is to introduce a geometric method that constructs a sequence of points strictly convergent to the image of the circle center from an arbitrary point . the fully automatic calibration system automatically detects the points of the pattern features by the construction method , and identify them by invariants . fully automatic calibration system then takes advantage of homolog-ical constraints to consistently and optimally estimate the features in the image . the experiments demonstrate the ro-bustness and the accuracy of the new fully automatic calibration system .
codes -lrb- vlc -rrb- for image and video streaming transmission over unreliable links , such as wireless networks , is a subject of increasing interest . this paper proposes an optimum maximum likelihood decoder of vlc sequences which exploits additional inherent redundancy in the source information , namely -lrb- i -rrb- the correlation between bits inside a vlc codeword as well as -lrb- ii -rrb- the correlation between vlc codewords of a vlc sequence unit -lrb- e.g. corresponding to one image block -rrb- . performance results for improving video decoding over awgn channels are then presented and compared to the prefix-based decoder as well as the recently proposed vlc decoders .
we describe a multi-layer generative approach for understanding user actions in natural language utterances . our multi-layer generative approach uses both labeled and unlabeled utterances to jointly learn aspects regarding utterance 's target domain -lrb- e.g. movies -rrb- , intention -lrb- e.g. , finding a movie -rrb- along with other semantic units -lrb- e.g. , movie name -rrb- . we inject information extracted from unstructured web search query logs as prior information to enhance the generative process of the natural language utterance understanding model . using utterances from five domains , our multi-layer generative approach shows up to 4.5 % improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model -lrb- which requires fully labeled data -rrb- .
we derive the haar filter bank for 1-d space signals , based on our recently introduced framework for 1-d space signal processing , termed this way since haar filter bank is built on a symmetric space shift operation in contrast to the directed time shift operation . the framework includes the proper notions of signal and filter spaces , '' z-transform , '' convo-lution , and fourier transform , each of which is different from their time equivalents . in this paper , we extend this framework by deriving the proper notions of a haar filter bank for space signal processing , and show that haar filter bank has a similar yet different form compared to the time case . our derivation also sheds light on the nature of filter banks and makes a case for viewing them as projections on subspaces rather than as based on filters .
in multi-class categorization tasks , knowledge about the classes ' semantic relationships can provide valuable information beyond the class labels themselves . however , existing techniques focus on preserving the semantic distances between classes -lrb- e.g. , according to a given object taxonomy for visual recognition -rrb- , limiting the influence to pairwise structures . we propose to model analogies that reflect the relationships between multiple pairs of classes simultaneously , in the form '' p is to q , as r is to s '' . we translate semantic analogies into higher-order geometric constraints called analogical parallelograms , and use semantic analogies in a novel convex regularizer for a discriminatively learned label embedding . furthermore , we show how to discover analogies from attribute-based class descriptions , and how to prioritize those likely to reduce inter-class confusion . evaluating our analogy-preserving semantic embedding on two visual recognition datasets , we demonstrate clear improvements over existing approaches , both in terms of recognition accuracy and analogy completion .
many existing explanation methods in bayesian networks , such as maximum a posteriori assignment and most probable explanation , generate complete assignments for target variables . a pri-ori , the set of target variables is often large , but only a few of them may be most relevant in explaining given evidence . generating explanations with all the target variables is hence not always desirable . this paper addresses the problem by proposing a new framework called most relevant explanation , which aims to automatically identify the most relevant target variables . we will also discuss in detail a specific instance of the framework that uses generalized bayes factor as the relevance measure . finally we will propose an approximate algorithm based on reversible jump mcmc and simulated annealing to solve most relevant explanation . empirical results show that the new approximate algorithm typically finds much more concise explanations than existing methods .
an almost-parsing 1 language model has been developed -lsb- 1 -rsb- that provides a framework for tightly integrating multiple knowledge sources . lexical features and syntactic constraints are integrated into a uniform linguistic structure -lrb- called a superarv -rrb- that is associated with words in the lexicon . the almost-parsing 1 language model has been found able to reduce perplexity and word error rate compared to trigram , part-of-speech-based , and parser-based language models on the darpa wall street journal -lrb- wsj -rrb- csr task . in this paper we further investigate the robustness of the almost-parsing 1 language model to possibly inconsistent and flawed training data , as well as its ability to scale up to sophisticated lvcsr tasks by comparing performance on the darpa wsj and hub4 -lrb- broadcast news -rrb- csr tasks .
existing controller-based approaches for centralized and decentralized pomdps are based on automata with output known as moore machines . in this paper , we show that several advantages can be gained by utilizing another type of automata , the mealy machine . mealy machines are more powerful than moore machines , provide a richer structure that can be exploited by solution methods , and can be easily incorporated into current controller-based approaches . to demonstrate this , we adapted some existing controller-based algorithms to use mealy machines and obtained results on a set of benchmark domains . the existing controller-based approaches always outperformed the existing controller-based approaches and often out-performed the state-of-the-art controller-based algorithms for both centralized and decentralized pomdps . these findings provide fresh and general insights for the improvement of existing controller-based algorithms and the development of new existing controller-based approaches .
we argue that parameterized complexity is a useful tool with which to study global constraints . in particular , we show that many global constraints which are intractable to propagate completely have natural parameters which make them fixed-parameter tractable and which are easy to compute . this tractability tends either to be the result of a simple dynamic program or of a decomposition which has a strong backdoor of bounded size . this strong backdoor is often a cycle cutset . we also show that parameterized complexity can be used to study other aspects of constraint programming like symmetry breaking . for instance , we prove that value symmetry is fixed-parameter tractable to break in the number of symmetries . finally , we argue that parameterized complexity can be used to derive results about the approximability of constraint propagation .
memory issue is the most critical problem for a high performance jpeg 2000 stripe pipeline scheme . the memory issue occupies more than 50 % of area in conventional jpeg 2000 architectures . to solve this problem , we propose a stripe pipeline scheme . for this stripe pipeline scheme , a level switch discrete wavelet transform and a code-block switch embedded block coding are proposed . with small additional memory , the ls-dwt and the code-block switch embedded block coding can process multiple levels and code-blocks in parallel by an inter-leaved scheme . as a result of above techniques , the overall memory requirements of the proposed stripe pipeline scheme can be reduced to only 8.5 % comparing with conventional architectures .
when an autoregressive process is observed through a sparse multipath environment , its ar parameters may be estimated by searching for a symmetric finite impulse response filter , which , when convolved with the observed signal 's autocorrelation sequence , yields the sparsest output . the zeros of that filter would then correspond to the poles of the ar process . when the 0-norm of the output is used as a measure of its sparsity , consistency of the resulting estimate -lrb- under some simple conditions -rrb- is readily obtained . however , due to problematic aspects of 0-norm minimization , it is often more convenient to resort to 1-norm minimization . a question of major interest in this context is whether -lrb- and if so , under what conditions -rrb- consistency of the resulting estimate is maintained . by analyzing the perturbations of the 1-norm about the desired solution , we derive -lrb- and illustrate -rrb- specific conditions for consistency . we show that when the multipath reflections are sufficiently sparse , consistency is guaranteed for a very wide range of ar parameters and reflection gains .
condensation , a form of likelihood-weighted particle filtering , has been successfully used to infer the shapes of highly constrained '' active '' contours in video sequences . however , when the contours are highly flexible -lrb- e.g. for tracking fingers of a hand -rrb- , a computationally burdensome number of particles is needed to successfully approximate the contour distribution . we show how the metropolis algorithm can be used to update a particle set representing a distribution over contours at each frame in a video sequence . we compare this metropolis algorithm to condensation using a video sequence that requires highly flexible contours , and show that the new metropolis algorithm performs dramatically better that the condensation algorithm . we discuss the incorporation of this metropolis algorithm into the '' active contour '' framework where a shape-subspace is used constrain shape variation .
rejection sampling is a technique for sampling from difficult distributions . however , its use is limited due to a high rejection rate . common adaptive rejection sampling methods either work only for very specific distributions or without performance guarantees . in this paper , we present pliable rejection sampling , a new approach to rejection sampling , where we learn the sampling proposal using a kernel estimator . since our method builds on rejection sampling , the samples obtained are with high probability i.i.d. and distributed according to f. moreover , pliable rejection sampling comes with a guarantee on the number of accepted samples .
sparse coding is a widely involved technique in computer vision . however , the expensive computational cost can hamper its applications , typically when the codebook size must be limited due to concerns on running time . in this paper , we study a special case of sparse coding in which the codebook is a cartesian product of two subcodebooks . we present algorithms to decompose this sparse coding problem into smaller subproblems , which can be separately solved . our solution , named as product sparse coding -lrb- psc -rrb- , reduces the time complexity from o -lrb- k -rrb- to o -lrb- √ k -rrb- in the codebook size k . in practice , this can be 20-100 × faster than standard sparse coding . in experiments we demonstrate the efficiency and quality of this method on the applications of image classification and image retrieval .
automatic language identification -lrb- lid -rrb- in music has received significantly less attention than lid in speech . here , we study the problem of lid in music videos uploaded on youtube . we use a '' bag-of-words '' approach based on state-of-the-art content based audiovisual features and linear svm classifiers for automatic lid . our bag-of-words '' approach obtains 48 % accuracy for a corpus of 25000 music videos and 25 different languages .
we investigate a hybrid method which improves the quality of state inference and parameter estimation in blind decon-volution of a sparse source modeled by a bernoulli-gaussian process . in this problem , when both the signal and the filter are jointly estimated , the true posterior is typically highly multimodal . therefore , when not properly initialized , standard stochastic inference methods , -lrb- mcem , sem or saem -rrb- , tend to get stuck and suffer from poor convergence . in our hybrid method , we first relax the bernoulli-gaussian prior model by a student-t model . our simulations suggest that deterministic inference in the bernoulli-gaussian prior model is not only efficient , but also provides a very good initialization for the bernoulli-gaussian prior model . we provide simulation studies that compare the results obtained with and without our bernoulli-gaussian prior model for several combinations of state inference and parameter estimation methods used for the bernoulli-gaussian prior model .
in this paper an hmm/n-gram-based linguistic processing approach for mandarin spoken document retrieval is presented . the underlying characteristics and different structures of this hmm/n-gram-based linguistic processing approach were extensively investigated . the retrieval capabilities were verified by tests with indexing features of word-and syllable -lrb- subword -rrb- - levels and comparison with the conventional vector space model approach . to further improve the discrimination capabilities of the hmms , both the expectation-maximization -lrb- em -rrb- and minimum classification error training algorithms were introduced in training . the information fusion of indexing features of word-and syllable-levels was also investigated . the spoken document retrieval experiments were performed on the topic detection and tracking corpora -lrb- tdt-2 and tdt-3 -rrb- . very encouraging retrieval performance was obtained .
this paper describes a method to adaptively control a step-size parameter which is used for updating a separation matrix to extract a target sound source accurately in blind source separation . the design of the step-size parameter is essential when we apply blind source separation to real-world applications such as robot audition systems , because the surrounding environment dynamically changes in the real world . it is common to use a fixed step-size parameter that is obtained empirically . however , due to environmental changes and noises , the performance of blind source separation with the fixed step-size parameter deteriorates and the separation matrix sometimes diverges . we propose a general method that allows adaptive step-size control . the proposed method is an extension of newton 's method utilizing a complex gradient theory and is applicable to any blind source separation . actually , we applied it to six types of blind source separation for an 8 ch microphone array embedded in honda asimo . experimental results show that the proposed method improves the performance of these six blind source separation through experiments of separation and recognition for two simultaneous speeches .
we utilize gpu implementations to accelerate an essential component for computer vision and multimedia information retrieval , i.e. scene categorization . to construct bag of word models , we modify calculation of euclidean distance so that feature clustering and visual word quantization can be processed in a parallel manner . we provide details of gpu implementations and conduct comprehensive experiments to verify the efficiency of gpu implementations on multimedia analysis .
we study the problem of unsupervised ontology learning for semantic understanding in spoken dialogue systems , in particular , learning the hierarchical semantic structure from the data . given unlabelled conversations , we augment a frame-semantic based unsupervised slot induction approach with hierarchical agglomerative clustering to merge topically-related slots -lrb- e.g. , both slots '' direction '' and '' locale '' convey location-related information -rrb- for building a coherent semantic hierarchy , and then estimate the slot importance at different levels . the high-level semantic estimation involves not only within-slot but also cross-slot relations . the experiments show that high-level semantic information can accurately estimate the prominence of slots , significantly improving the slot induction performance ; furthermore , a semantic decoder trained on the data with automatically extracted slots achieves about 68 % f-measure , which is close to the one from hand-crafted grammars .
adcs sit at the interface of the analog and digital worlds and fundamentally determine what information is available in the digital domain for processing . this paper shows that a con-figurable adc can be designed for signals with non constant information as a function of frequency such that within a fixed power budget the adcs maximizes the information in the converted signal by frequency shaping the quantization noise . quantization noise shaping can be realized via loop filter design for a single channel delta sigma adcs and extended to common time and frequency interleaved multi channel structures . results are presented for example wireline and wireless style channels .
in this paper , we report on classification results for emotional user states -lrb- 4 classes , german database of children interacting with a pet robot -rrb- . six sites computed acoustic and linguistic features independently from each other , following in part different strategies . a total of 4244 features were pooled together and grouped into 12 low level descriptor types and 6 functional types . for each of these groups , classification results using support vector machines and random forests are reported for the full set of features , and for 150 features each with the highest individual information gain ratio . the performance for the different groups varies mostly between ≈ 50 % and ≈ 60 % .
we present multi-relational latent semantic analysis which generalizes latent semantic analysis . multi-relational latent semantic analysis provides an elegant approach to combining multiple relations between words by constructing a 3-way tensor . similar to multi-relational latent semantic analysis , a low-rank approximation of the tensor is derived using a tensor decomposition . each word in the vocabulary is thus represented by a vector in the latent semantic space and each relation is captured by a latent square matrix . the degree of two words having a specific relation can then be measured through simple linear algebraic operations . we demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources , multi-relational latent semantic analysis achieves state-of-the-art performance on existing benchmark datasets for two relations , antonymy and is-a .
this paper proposes a novel method of building polarity-tagged corpus from html documents . the characteristics of this method is that it is fully automatic and can be applied to arbitrary html documents . the idea behind our method is to utilize certain layout structures and linguistic pattern . by using them , we can automatically extract such sentences that express opinion . in our experiment , the method could construct a corpus consisting of 126,610 sentences .
-- evidence suggests that magnetoencephalo-gram data have characteristics with non-gaussian distribution , however , standard methods for source localisation assume gaussian behaviour . we present a new general method for non-gaussian source estimation of stationary signals for localising brain activity in the meg data . by providing a bayesian formulation for linearly constraint minimum variance beamformer , we extend this bayesian formulation and show that how the source probability density function , which is not necessarily gaussian , can be estimated . the proposed source probability density function is shown to give better spatial estimates than the lcmv beamformer , in both simulations incorporating non-gaussian signal and in real meg measurements .
this paper presents a follow up of a study on the automatic detection of prosodic prominence in continuous speech . prosodic prominence involves two different prosodic features , pitch accent and stress , that are typically based on four acoustic parameters : fundamental frequency movements , overall syllable energy , syllable nuclei duration and mid-to-high-frequency emphasis . a careful measurement of these acoustic parameters , as well as the identification of their connection to prosodic parameters , makes it possible to build an automatic system capable of identifying prominent syllables in utterances with performance comparable with the inter-human agreement reported in the literature . this automatic system has been used to cast light on the actual correlation among the acoustic parameters and the prominence phenomenon from an typological point of view , by examining data derived from some stress-accented languages .
we propose the use of modulation spectrogram features in speaker diarization . these modulation spectrogram features carry longer term characteristics of the acoustic signals than the widely used mfccs , thus providing potential improvement by using both modulation spectrogram features in combination . using the state-of-the-art icsi speaker diarization system , an improvement of 20.77 % relative der is obtained on the nist rich transcription 2007 task with respect to the mfcc only system .
in this paper , a novel method is proposed for image interpolation . it is assumed that the pixel correlation between local regions across scales would remain similar . in addition , this a priori similarity could be extracted from a set of available image data that have the same content but different resolutions . a simple architecture is devised to estimate the correlation efficiently , which is then used to predict the unknown pixel values in a high-resolution image . evaluation shows a promising performance of the proposed algorithm .
we present a novel model , freestyle , that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics , by combining both bottom-up token based rule induction and top-down rule segmentation strategies to learn a stochas-tic transduction grammar that simultaneously learns both phrasing and rhyming associations . in this attack on the woefully under-explored natural language genre of music lyrics , we exploit a strictly unsupervised transduction grammar induction approach . our strictly unsupervised transduction grammar induction approach is particularly ambitious in that no use of any a pri-ori linguistic or phonetic information is allowed , even though the domain of hip hop lyrics is particularly noisy and unstructured . we evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction , and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses , measured on fluency and rhyming criteria as judged by human evaluators . to highlight some of the inherent challenges in adapting other algorithms to this novel strictly unsupervised transduction grammar induction approach , we also compare the quality of the responses generated by our model to those generated by an out-of-the-box phrase based smt system . we tackle the challenge of selecting appropriate training data for our strictly unsupervised transduction grammar induction approach via a dedicated rhyme scheme detection module , which is also acquired via unsupervised learning and report improved quality of the generated responses . finally , we report results with maghrebi french hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages .
this paper 1 explores the use of a maximal average margin optimality principle for the design of learning algorithms . it is shown that the application of this risk minimization principle results in a class of -lrb- computationally -rrb- simple learning machines similar to the classical parzen window classifier . a direct relation with the rademacher complexities is established , as such facilitating analysis and providing a notion of certainty of prediction . this analysis is related to support vector machines by means of a margin transformation . the power of the maximal average margin optimality principle is illustrated further by application to ordinal regression tasks , resulting in an o algorithm able to process large datasets in reasonable time .
in this paper , we propose a new feature extraction method based on higher-order local auto-correlation and fisher weight map . widely used higher-order local auto-correlation lack temporal dynamics . to solve this problem , 35 types of local auto-correlation features are computed within two-dimensional local regions . these local auto-correlation features are accumulated over more global regions by weight-ing high scores on the discriminative areas where the typical features among all phonemes are well expressed . this score map is called fisher weight map . we verified the effectiveness of the fisher weight map and fisher weight map through vowel recognition and total phoneme recognition .
this paper proposes a novel approach for estimating the spectral envelope of voiced speech independently of its harmonic structure . because of the quasi-periodicity of voiced speech , its spectrum indicates harmonic structure and only has energy at frequencies corresponding to integral multiples of 1/4 . it is hence impossible to identify transfer characteristics between the adjacent harmonics . in order to resolve this problem , multi-frame analysis is introduced . the mfa estimates a spectral envelope using many portions of speech which are vo-calised using the same vocal-tract shape . since each of the portions usually has a different 1/4 and ensuing different harmonic structure , a number of harmonics can be obtained at various frequencies to form a spectral envelope . the method thereby gives a closer approximation to the vocal-tract transfer function .
we present an over-segmentation based , dense stereo algorithm that jointly estimates segmentation and depth . for mixed pixels on segment boundaries , the over-segmentation based , dense stereo algorithm computes foreground opacity -lrb- alpha -rrb- , as well as color and depth for the foreground and background . we model the scene as a collection of fronto-parallel planar segments in a reference view , and use a generative model for image formation that handles mixed pixels at segment boundaries . our over-segmentation based , dense stereo algorithm iteratively updates the segmentation based on color , depth and shape constraints using map estimation . given a segmentation , the depth estimates are updated using belief propagation . we show that our over-segmentation based , dense stereo algorithm is competitive with the state-of-the-art based on the new middlebury stereo evaluation , and that over-segmentation based , dense stereo algorithm overcomes limitations of traditional segmentation based methods while properly handling mixed pixels . z-keying results show the advantages of combining opacity and depth estimation .
we present sirfs -lrb- shape , illumination , and reflectance from shading -rrb- , the first unified model for recovering shape , chromatic illumination , and reflectance from a single image . our model is an extension of our previous work -lsb- 1 -rsb- , which addressed the achromatic version of this problem . dealing with color requires a modified problem formulation , novel priors on reflectance and illumination , and a new optimization scheme for dealing with the resulting inference problem . our approach outperforms all previously published algorithms for intrinsic image decomposition and shape-from-shading on the mit intrinsic images dataset -lsb- 1 , 2 -rsb- and on our own '' naturally '' illuminated version of that dataset .
we propose a nonparametric bayesian factor regression model that accounts for uncertainty in the number of factors , and the relationship between factors . to accomplish nonparametric bayesian factor regression model , we propose a sparse variant of the indian buffet process and couple nonparametric bayesian factor regression model with a hierarchical model over factors , based on kingman 's coalescent . we apply nonparametric bayesian factor regression model model to two problems -lrb- factor analysis and factor regression -rrb- in gene-expression data analysis .
we introduce a conjidence measure that estimates the assurance that a graph arc -lrb- or edge -rrb- corresponds to an object houndury in an image . a weighted , planar graph is imposed onto the watershed lines of a gradient magnitude image and the conjidence measure is afunction of the cost of fixed-length paths emanating from and extending to each end of a graph arc . the confidence measure is applied to automate the detection of object boundaries and thereby reduces -lrb- often greatly -rrb- the time and effort required for object boundary definition within a user-guided image segmentation environment .
the project ritel platform aims at integrating a spoken language dialog system and an open-domain question answering system to allow a human to ask general questions -lrb- '' who is currently presiding the senate ? '' -rrb- and refine the search interactively . as this point in time the ritel platform is being used to collect a human-computer dialog corpus . the user can receive factual answers to some questions -lrb- q : who is the president of france , r : jacques chirac is the president for france since may 1995 -rrb- . this paper briefly presents the current system , the collected corpus , the problems encountered by such a system and our first answers to these problems . when the system is more advance , it will allow measuring the net worth of integrating a spoken language dialog system into a qa system . does allowing such a spoken language dialog system really enables to reach faster and more precisely the '' right '' answer to a question ?
timed failure propagation graphs -lrb- timed failure propagation graphs -rrb- are a formalism used in industry to describe failure propagation in a dynamic partially observable system . timed failure propagation graphs are commonly used to perform model-based diagnosis . as in any model-based diagnosis approach , however , the quality of the diagnosis strongly depends on the quality of the timed failure propagation graphs . approaches to certify the quality of the timed failure propagation graphs are limited and mainly rely on testing . in this work we address this problem by leverag-ing efficient satisfiability modulo theories engines to perform exhaustive reasoning on timed failure propagation graphs . we apply model-checking techniques to certify that a given timed failure propagation graphs satisfies -lrb- or not -rrb- a property of interest . moreover , we discuss the problem of refinement and diagnosability testing and empirically show that our model-checking techniques can be used to efficiently solve them .
the object function for boosting training method in acoustic modeling aims to reduce utterance level error rate . this is different from the most commonly used performance metric in speech recognition , word error rate . this paper proposes that the combination of n-best list re-ranking and rover can partly address this problem . in particular , acoustic modeling is applied to re-ranked hypotheses rather than to the original top-1 hypotheses and carried on word level . improvement of system performance is observed in our experiments . in addition , we describe and evaluate a new confidence feature that measures the correctness of frame level decoding result .
underspecification-based algorithms for processing partially disambiguated discourse structure must cope with extremely high numbers of readings . based on previous work on dominance graphs and weighted tree grammars , we provide the first possibility for computing an underspecified discourse description and a best discourse representation efficiently enough to process even the longest discourses in the rst discourse treebank .
it is a challenging vision problem to discover non-rigid shape deformation for an image ensemble belonging to a single object class , in an automatic or semi-supervised fashion . the conventional semi-supervised approach -lsb- 1 -rsb- uses a congealing-like process to propagate manual landmark labels from a few images to a large ensemble . although effective on an inter-person database with a large population , there is potential for increased labeling accuracy . with the goal of providing highly accurate labels , in this paper we present a parametric curve representation for each of the seven major facial contours . the appearance information along the curve , named curve descriptor , is extracted and used for congealing . furthermore , we demonstrate that advanced features such as histogram of oriented gradient can be utilized in the proposed congealing framework , which operates in a dual-curve congealing manner for the case of a closed contour . with extensive experiments on a 300-image ensemble that exhibits moderate variation in facial pose and shape , we show that substantial progress has been achieved in the labeling accuracy compared to the previous state-of-the-art approach .
the integral image is typically used for fast integrating a function over a rectangular region in an image . we propose a method that extends the integral image to do fast integration over the interior of any polygon that is not necessarily rectilinear . the integration time of the method is fast , independent of the image resolution , and only linear to the polygon 's number of vertices . we apply the method to viola and jones ' object detection framework , in which we propose to improve classical haar-like features with polyg-onal haar-like features . we show that the extended feature set improves object detection 's performance . the experiments are conducted in three domains : frontal face detection , fixed-pose hand detection , and rock detection for mars ' surface terrain assessment .
modern integrated circuits -lrb- ics -rrb- are designed as massively parallel systems as a consequence of diminishing silicon feature sizes . this has adversely impacted reliability because of increased errors due to process and environmental variations , and particle hits . viewing hardware errors as analogous to measurement or system noise allows us to borrow results from estimation theory and extend moore 's law . the estimation-theoretic framework provides a design optimization formalization that enables power/reliability trade-off in broad classes of applications . two applications described here show that specific instantiations of the estimation-theoretic framework yield significant power savings and system reliability .
spectral imaging is of interest in many applications , including wide-area airborne surveillance , remote sensing , and tissue spectroscopy . coded aperture spectral snapshot imaging provides an efficient mechanism to capture a 3d spectral cube with a single shot 2d measurement . coded aperture spectral snapshot imaging uses a focal plane array measurement of a spectrally dispersed , aperture coded , source . the 3d spectral cube is then attained using a compressive sensing reconstruction algorithm . in this paper , we explore a new approach referred to as random convolution snapshot spectral imaging . it is based on fpa measurements of spectrally dispersed coherent sources that have been randomly convoluted by a spatial light modulator . the new method , based on the theory of compres-sive sensing via random convolutions , is shown to outperform traditional cassi systems in terms of psnr spectral image cube reconstructions .
this paper describes a parsing model that combines the exact dynamic programming of crf parsing with the rich nonlinear fea-turization of neural net approaches . our parsing model is structurally a crf parsing that factors over anchored rule productions , but instead of linear potential functions based on sparse features , we use nonlinear potentials computed via a feedforward neu-ral network . because potentials are still local to anchored rules , structured inference -lrb- cky -rrb- is unchanged from the sparse case . computing gradients during learning involves backpropagating an error signal formed from standard crf parsing sufficient statistics -lrb- expected rule counts -rrb- . using only dense features , our parsing model already exceeds a strong baseline crf model -lrb- hall et al. , 2014 -rrb- . in combination with sparse features , our parsing model 1 achieves 91.1 f 1 on section 23 of the penn tree-bank , and more generally outperforms the best prior single parser results on a range of languages .
in this paper , we proposed a dynamic ordered subcarrier selection algorithm for ofdm based video transmission system . the proposed dynamic ordered subcarrier selection algorithm is shown to achieve lower bit error rate than the previously proposed ossa by ſrst selecting a fraction of the subcarriers with highest channel gain . the content information is then exploited in order to extend the ossa to achieve unequal error protection for packets of different importance . simulation results show that dynamic ordered subcarrier selection algorithm that utilizes the proposed dynamic ordered subcarrier selection algorithm can achieve higher bit error rate , especially at low snr , compared to those that use the equal error protection -lrb- eep -rrb- ossa .
we present a new zero-crossing based algorithm for decomposing a bandpass signal into the amplitude modulation and freqeuency modulation components . in this zero-crossing based algorithm , the fm component is first estimated using zero-crossing instant information in a k-nearest neighbour -lrb- ` k-nn ' -rrb- framework . the fm component is estimated by coherent demodulation using a time-varying lowpass filter that uses the estimated instantaneous frequency . simulation results show that the proposed zero-crossing based algorithm gives more accurate envelope and frequency estimates compared to the discrete-energy separation algorithm which uses the teager energy operator . using the proposed zero-crossing based algorithm on bandpass filtered speech and music we can extract the fine-structured modulations that occur on a micro-time scale , within an analysis frame .
most detection algorithms for hyperspectral imaging applications assume a target with a perfectly known spectral signature . in practice , the target signature is either imperfectly measured -lrb- target mismatch -rrb- and/or it exhibits spectral variability . the objective of this paper is to introduce a robust matched ¿ lter that takes the uncertainty and/or variability of target signatures into account . it is shown that , if we describe this uncertainty with an ellipsoid in the spectral space , we can design a matched ¿ lter that provides a response of the same magnitude for all spectra within this ellipsoid . thus , by changing the size of this ellipsoid , we can control the '' spectral selectivity '' of the matched ¿ lter . the ability of the robust matched ¿ lter to deal effectively with target mismatch and spectral variability is demonstrated with hyperspectral imaging data from the hydice sensor .
abduction belongs to the most fundamental reasoning methods . it is a method for reverse inference , this means one is interested in explaining observed behavior by finding appropriate causes . we study logic-based abduction , where knowledge is represented by proposi-tional formulas . the computational complexity of this logic-based abduction is highly intractable in many interesting settings . in this work we therefore present an extensive parameterized complexity analysis of abduction within various fragments of propositional logic together with -lrb- combinations of -rrb- natural parameters .
this paper proposes a novel methodology on mercer kernel construction using interpolatory strategy . based on a given symmetric and positive semi-definite matrix -lrb- gram matrix -rrb- and cholesky decomposition , it first constructs a nonlinear mapping φ , which is well-defined on the training data . this nonlinear mapping φ is then extended to the whole input feature space by utilizing lagrange interpolatory basis functions . the kernel function constructed by inner product is proven to be a mercer kernel function . the self-constructed interpolatory mercer kernel keeps the gram matrix unchanged on the training samples . to evaluate the performance of the proposed self-constructed interpolatory mercer kernel , a popular kernel direct linear discriminant analysis -lrb- kdda -rrb- method for face recognition is selected . comparing with rbf kernel based kdda method on two face databases , namely feret and cmu pie databases , the im kernel based kdda approach could increase the performance by around 20 % on cmu pie database .
this paper investigates acoustic modeling using the hybrid dbn-hmm framework in mispronunciation detection and diagnosis of l2 english . this is one of the first efforts that compare the performance of dbn-hmm with that of the best-tuned gmm-hmm trained in ml and mwe on the same set of features . previous work in asr engine has also shown the necessity of unsupervised pre-training for dbn-hmm to work well . we explore further the effect of training our asr engine in an unsupervised manner with additional unannotated l2 data from the test speakers . this is compared with the original asr engine that has been trained with annotated data in a supervised manner . experiments show that dbn-hmm can give significant improvement -lrb- between 13-18 % relative in word pronunciation error rate -rrb- but is computationally more expensive .
recent theoretical results for pattern classiication with thresh-olded real-valued functions -lrb- such as support vector machines , sig-moid networks , and boosting -rrb- give bounds on misclassiication probability that do not depend on the size of the classiier , and hence can be considerably smaller than the bounds that follow from the vc theory . in this paper , we show that these techniques can be more widely applied , by representing other boolean functions as two-layer neural networks -lrb- thresholded convex combinations of boolean functions -rrb- . for example , we show that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassiication probability no more than o ? 1 m ? n ee vcdim -lrb- u -rrb- log 2 m log d 1 = 2 , where u is the class of node decision functions , and n ee n can be thought of as the eeective number of leaves -lrb- it becomes small as the distribution on the leaves induced by the training data gets far from uniform -rrb- . this bound is qualitatively diierent from the vc bound and can be considerably smaller . we use the same technique to give similar results for dnf formulae .
color naming , which relates colors with color names , can help people with a semantic analysis of images in many computer vision applications . in this paper , we propose a novel salient color names based color descriptor -lrb- scncd -rrb- to describe colors . scncd utilizes salient color names to guarantee that a higher probability will be assigned to the color name which is nearer to the color . based on scncd , color distributions over color names in different color spaces are then obtained and fused to generate a feature representation . moreover , the effect of background information is employed and analyzed for person re-identification . with a simple metric learning method , the proposed scncd outperforms the state-of-the-art performance -lrb- without user 's feedback optimization -rrb- on two challenging datasets -lrb- viper and prid 450s -rrb- . more importantly , the proposed metric learning method can be obtained very fast if we compute scncd of each color in advance .
filter bank based transmultiplexer systems have certain advantages compared with existing dft-based multicarrier systems and filter bank based transmultiplexer systems are promising candidates for data transmission in frequency selective channels . we have recently proposed a novel and efficient channel equalization idea to be used with critically decimated perfect reconstruction cosine modulated transmultiplexer systems . the equalizer utilizes parallel cosine and sine modulated filter banks in the receiver end . this paper explores efficient realization structures for the needed parallel filter bank system , which finds applications also in other areas .
zero-resource speech processing involves the automatic analysis of a collection of speech data in a completely unsupervised fashion without the benefit of any transcriptions or annotations of the data . in this paper , our zero-resource system seeks to automatically discover important words , phrases and topical themes present in an audio corpus . this zero-resource system employs a segmental dynamic time warping algorithm for acoustic pattern discovery in conjunction with a probabilistic model which treats the topic and pseudo-word identity of each discovered pattern as hidden variables . by applying an expectation-maximization algorithm , our zero-resource system estimates the latent probability distributions over the pseudo-words and topics associated with the discovered patterns . using this information , we produce acoustic summaries of the dominant topical themes of the audio document collection .
in this paper we introduce a novel approach to manifold alignment , based on procrustes analysis . our approach differs from `` semi-supervised alignment '' in that it results in a mapping that is defined everywhere - when used with a suitable dimensionality reduction method - rather than just on the training data points . we describe and evaluate our approach both theoretically and experimentally , providing results showing useful knowledge transfer from one domain to another . novel applications of our method including cross-lingual information retrieval and transfer learning in markov decision processes are presented .
in this paper , we address the visual video summarization problem in a bayesian framework in order to detect and describe the underlying temporal transformation symmetries in a video sequence . given a set of time correlated frames , we attempt to extract a reduced number of image-like data structures which are semantically meaningful and that have the ability of representing the sequence evolution . to this end , we present a generative model which involves jointly the representation and the evolution of appearance . applying linear dynamical system theory to this visual video summarization problem , we discuss how the temporal information is encoded yielding a manner of grouping the iconic representations of the video sequence in terms of invariance . the formulation of this visual video summarization problem is driven in terms of a probabilistic approach , which affords a measure of perceptual similarity taking both learned appearance and time evolution models into account .
full-perspective mappings between 3d objects and 2d images are more complicated than weak-perspective mappings , which consider only rotation , translation and scaling . therefore , in 3d model-based robot navigation , it is important to understand how and when full-perspective m ust be taken into account . in this paper we use a probabilistic combinatorial optimization algorithm to search for an optimal match between 3d landmark and 2d image features . three variations are considered : a w eak-perspective algorithm rotates , translates and scales an initial 2d projection of the 3d landmark . a full-perspective algorithm always recomputes the robot 's pose and repro-jects the landmark when testing alternative matches . finally , a h ybrid algorithm uses weak-perspective t o select a most promising alternative , but then updates the pose and reprojects the landmark . the full-perspective algorithm appears to combine the best attributes of the other two . like the full-perspective algorithm , it reliably recovers the true pose of the robot , and like t h e weak-perspective algorithm , it runs 5 to 10 faster than the full-perspective algorithm .
numerous applications in statistics , signal processing , and machine learning regularize using total variation penalties . we study anisotropic -lrb- ℓ 1-based -rrb- tv and also a related ℓ 2-norm variant . we consider for both variants associated -lrb- 1d -rrb- proximity operators , which lead to challenging optimization problems . we solve these problems by developing newton-type methods that outperform the state-of-the-art algorithms . more importantly , our 1d-tv algorithms serve as building blocks for solving the harder task of computing 2 - -lrb- and higher -rrb- - dimensional tv proximity . we illustrate the computational benefits of our 1d-tv algorithms by applying 1d-tv algorithms to several applications : -lrb- i -rrb- image de-noising ; -lrb- ii -rrb- image deconvolution -lrb- by plugging in our tv solvers into publicly available software -rrb- ; and -lrb- iii -rrb- four variants of fused-lasso . the results show large speedups -- and to support our claims , we provide software accompanying this paper .
performance of n-gram language models depends to a large extent on the amount of training text material available for building the n-gram language models and the degree to which this text matches the domain of interest . the language modeling community is showing a growing interest in using large collections of auxiliary textual material to supplement sparse in-domain resources . one of the problems in using such n-gram language models is that n-gram language models may differ significantly from the specific nature of the domain of interest . in this paper , we propose three different methods for adapting language models for a speech to speech translation system when n-gram language models are of different genre and domain . the proposed methods are based on cen-troid similarity , n-gram ratios and resampled language models . we show how these methods can be used to select out of domain textual data such as newswire text to improve a s2s system . we were able to achieve an overall relative improvement of 3.8 % in bleu score over a baseline system that uses only in-domain conversational data .
we introduce a family of unsupervised algorithms , numerical taxonomy clustering , to simultaneously cluster data , and to learn a taxonomy that encodes the relationship between the clusters . the unsupervised algorithms work by maximizing the dependence between the taxonomy and the original data . the resulting taxonomy is a more informative visualization of complex data than simple clustering ; in addition , taking into account the relations between different clusters is shown to substantially improve the quality of the clustering , when compared with state-of-the-art unsupervised algorithms in the literature -lrb- both spectral clustering and a previous dependence maximization approach -rrb- . we demonstrate our algorithm on image and text data .
kernel methods have gained a great deal of popularity in the machine learning community as a method to learn indirectly in high-dimensional feature spaces . those interested in relational learning have recently begun to cast learning from structured and relational data in terms of kernel operations . we describe a general family of kernel functions built up from a description language of limited expressivity and use it to study the benefits and drawbacks of kernel learning in relational domains . learning with kernels in this family directly models learning over an expanded feature space constructed using the same description language . this allows us to examine issues of time complexity in terms of learning with these and other relational kernels , and how these relate to generalization ability . the tradeoffs between using kernels in a very high dimensional implicit space versus a restricted feature space , is highlighted through two experiments , in bioinformatics and in natural language processing .
this paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task . syntax is derived from constituent and dependency parse trees whereas semantics concerns to entity types and lexical sequences . we investigate the effectiveness of such representations in the automated relation extraction from text . we process the above data by means of support vector machines along with the syntactic tree , the partial tree and the word sequence kernels . our study on the ace 2004 corpus illustrates that the combination of the above kernels achieves high effectiveness and significantly improves the current state-of-the-art .
discrimination discovery is to unveil discrimination against a specific individual by analyzing the historical dataset . in this paper , we develop a general technique to capture discrimination based on the legally grounded situation testing methodology . for any individual , we find pairs of tuples from the dataset with similar characteristics apart from belonging or not to the protected-by-law group and assign them in two groups . the individual is considered as discriminated if significant di ↵ erence is observed between the decisions from the two groups . to find similar tuples , we make use of the causal bayesian networks and the associated causal inference as a guideline . the causal structure of the dataset and the causal e ↵ ect of each attribute on the decision are used to facilitate the similarity measurement . through empirical assessments on a real dataset , our approach shows good ecacy both in accuracy and eciency .
we describe and analyze a simple algorithm for principal component analysis and singular value decomposition , vr-pca , which uses computa-tionally cheap stochastic iterations , yet converges exponentially fast to the optimal solution . in contrast , existing algorithms suffer either from slow convergence , or computationally intensive iterations whose runtime scales with the data size . the algorithm builds on a recent variance-reduced stochastic gradient technique , which was previously analyzed for strongly convex optimization , whereas here we apply it to an inherently non-convex problem , using a very different analysis .
performance of speech technologies can benefit greatly from a deeper appreciation of the nature of the information-bearing features in continuous speech . to explore these features , we focus here on the role of the spectral and temporal modulations in maintaining the intelligibility of speech as it becomes severely degraded by low-pass filtering and additive babble noise . these spectral and temporal modulations are estimated using a biological model of auditory processing which approximates the representation of sound in the cortex . intelligibility of the noisy speech is computed directly from this model via the spectro-temporal modulation index -lsb- 1 -rsb- , and the validity of this metric is confirmed by a detailed comparison with results of psychoacoustic tests . our analysis reveals quantitatively why certain types of noise are more disruptive to speech intelligibility than others -lrb- e.g. , babble vs. white noise -rrb- . it also highlights the important contribution of both spectral and temporal modulations in accurately predicting the intelligibility of speech under adverse conditions .
non-linear continuous change is common in real-world problems , especially those that model physical systems . we present an algorithm which builds upon existent temporal planning techniques based on linear programming to approximate non-linear continuous monotonic functions . these are integrated through a semantic attachment mechanism , allowing external libraries or functions that are difficult to model in native pddl to be evaluated during the planning process . a new planning system implementing this algorithm was developed and evaluated . results show that the addition of this algorithm to the planning process can enable planning system to solve a broader set of planning problems .
the interaction of people with autonomous agents has become increasingly prevalent . some of these settings include security domains , where people can be characterized as uncooperative , hostile , manipulative , and tending to take advantage of the situation for their own needs . this makes it challenging to design proficient agents to interact with people in such environments . evaluating the success of the agents automatically before evaluating them with people or deploying them could alleviate this challenge and result in better designed agents . in this paper we show how peer designed agents -- computer agents developed by human subjects -- can be used as a method for evaluating autonomous agents in security domains . such evaluation can reduce the effort and costs involved in evaluating autonomous agents interacting with people to validate their efficacy . our experiments included more than 70 human subjects and 40 peer designed agents developed by students . the study provides empirical support that peer designed agents can be used to compare the proficiency of autonomous agents when matched with people in security domains .
this paper considers matrix models , a class of matrix models which generally exhibit significant symmetries . it proposed the idea of lexleader feasibility checkers that verify , during search , whether the current partial assignment can be extended into a canonical solution . the lexleader feasibility checkers are based on a novel result by -lsb- katsirelos et al. , 2010 -rsb- on how to check efficiently whether a solution is canonical . the paper generalizes this result to partial assignments , various variable orderings , and value symmetries . empirical results on 5 standard benchmarks shows that lexleader feasibility checkers may bring significant performance gains , when jointly used with doublelex or snakelex .
several recent methods for speech enhancement in presence of diffuse background noise use frequency domain blind signal separation to estimate the diffuse noise and a nonlin-ear post filter to suppress this estimated noise . this paper presents a frequency domain blind signal extraction method for estimating the diffuse noise in place of the frequency domain blind signal separation . the frequency domain blind signal extraction method is based on the minimization by means of a complex newton algorithm of a cost function depending of the modulus of the extracted component . the proposed complex newton method is compared to the gradient descent on the same cost function and to the blind signal separation approach .
this paper proposes a stereophonic acoustic echo canceller without pre-processing which can identify the correct echo-paths . by dividing the filter coefficients into two portions and update one part at a time , the filter coefficient have an unique solution . convergence analysis clarifies the condition for correct echo-path identification . for fast convergence and stable adaptation , a convergence detection and an adaptive step-size are also introduced . the modification amount of the filter coefficients detects the convergence and also determines the step-size . computer simulations show 10db smaller coefficient error than those of the conventional algorithms .
learning from multi-view data is important in many applications . in this paper , we propose a novel convex subspace representation learning method for unsuper-vised multi-view clustering . we first formulate the sub-space learning with multiple views as a joint optimization problem with a common subspace representation matrix and a group sparsity inducing norm . by exploiting the properties of dual norms , we then show a convex min-max dual formulation with a sparsity inducing trace norm can be obtained . we develop a proximal bundle optimization algorithm to globally solve the joint optimization problem . our empirical study shows the proposed convex subspace representation learning method can effectively facilitate multi-view clustering and induce superior clustering results than alternative multi-view clustering methods .
unlike unsupervised approaches such as autoencoders that learn to reconstruct their inputs , this paper introduces an alternative approach to unsupervised feature learning called divergent discriminative feature accumulation -lrb- ddfa features -rrb- that instead continually accumulates features that make novel discriminations among the training set . thus ddfa features are inherently discriminative from the start even though ddfa features are trained without knowledge of the ultimate classification problem . interestingly , ddfa features also continues to add new features indefinitely -lrb- so features does not depend on a hidden layer size -rrb- , is not based on minimizing error , and is inherently divergent instead of convergent , thereby providing a unique direction of research for unsupervised feature learning . in this paper the quality of its learned features is demonstrated on the mnist dataset , where its performance confirms that indeed ddfa features is a viable technique for learning useful features .
in this paper , we propose a decision tree based coding mode selection method for the amr-wb + audio coder . in order to obtain improved performance with reduced computation , decision tree classifier is adopted with the closed loop mode selection results as the target classification labels . to secure the practical feasibility of this decision tree based coding mode selection method , the size of the decision tree classifier is controlled by pruning . through an evaluation on a database covering both speech and music materials , the proposed decision tree based coding mode selection method is found to increase the mode selection accuracy compared with the open loop mode selection module in the amr-wb + .
level-set methods have been shown to be an effective way to solve optimisation problems that involve closed curves . they are well known for their capacity to deal with flexible topology and do not require manual initialisation . computational complexity has previously been addressed by using banded algorithms which restrict computation to the vicinity of the zero set of the level-set function . so far , such schemes have used finite difference representations which suffer from limited accuracy and require re-initialisation procedures to stabilise the evolution . this paper shows how banded computation can be achieved using finite elements . we give details of the novel representation and show how to build the signed distance constraint into the presented numerical scheme . we apply the algorithm to the geodesic contour problem -lrb- including the automatic detection of nested contours -rrb- and demonstrate its performance on a variety of images . the resulting algorithm has several advantages which are demonstrated in the paper : it is inherently stable and avoids re-initialisation ; it is convergent and more accurate because of the capabilities of finite elements ; it achieves maximum sparsity because with finite elements the band can be effectively of width 1 .
fuzzy description logics -lrb- dls -rrb- are used to represent and reason about vague and imprecise knowledge that is inherent to many application domains . it was recently shown that the complexity of reasoning in finitely valued fuzzy dls is often not higher than that of the underlying classical dl . we show that this does not hold for fuzzy extensions of the lightweight dl el , which is used in many biomedical ontologies , under the łukasiewicz semantics . the complexity of reasoning increases from fuzzy description logics to exptime , even if only one additional truth value is introduced . the same lower bound holds also for infinitely valued łukasiewicz extensions of el .
this paper proposes a double layer speech recognition and utterance verification system based on the analysis of the temporal evolution of hmm 's state scores . for the lower layer , it uses standard hmm-based acoustic modeling , followed by a viterbi grammar-free decoding step which provides us with the state scores of the acoustic models . in the second layer , these state scores are added to the regular set of acoustic parameters , building a new set of expanded hmms . using this expanded set of hmms for speech recognition a significant improvement in performance is achieved . next , we will use this new architecture for utterance verification in a '' second opinion '' framework . we will consign to the second layer evaluating the reliability of decoding using the acoustic models from the first layer . an outstanding improvement in performance versus a baseline verification algorithm has been achieved .
a number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables . it is shown , however , that these reinforcement learning algorithms can easily become unstable when implemented directly with a general function-approximation system , such as a sigmoidal multilayer perceptron , a radial-basis-function system , a memory-based learning system , or even a linear function-approximation system . a new class of reinforcement learning algorithms , residual gradient algorithms , is proposed , which perform gradient descent on the mean squared bellman residual , guaranteeing convergence . i shown , however , that they may learn very slowly in some cases . a larger class of reinforcement learning algorithms , residual gradient algorithms , is proposed that has the guaranteed convergence of the residual gradient algorithms , yet can retain the fast learning speed of direct algorithms . in fact , both direct and residual gradient algorithms are shown to be special cases of residual gradient algorithms , and it is shown that residual gradient algorithms can combine the advantages of each approach . the direct , residual gradient , and residual forms of value iteration , q-learning , and advantage learning are all presented . theoretical analysis is given explaining the properties these reinforcement learning algorithms have , and simulation results are given that demonstrate these properties .
existing structural similarity index comprises of one term on luminance comparison and the other term on contrast and structure comparison . in this paper , the ssim index is first improved by introducing three weighting factors to the second term such that ssim index is adaptive to local intensities of two images to be compared . the improved ssim index is further extended for two images with possibly different exposures . experimental results show that the proposed indices are more robust to large intensity changes of two images from the same scene and more sensitive to two images from different scenes than the existing ssim index .
in this work a new online learning algorithm that uses automatic relevance determination is proposed for fast adaptive non-linear filtering . a sequential decision rule for inclusion or deletion of basis functions is obtained by applying a recently proposed fast variational sparse bayesian learning method . the proposed online learning algorithm uses a sliding window estimator to process the data in an online fashion . the noise variance can be implicitly estimated by the online learning algorithm . it is shown that the described online learning algorithm has better mean square error -lrb- mse -rrb- performance than a state of the art kernel re-cursive least squares -lrb- kernel-rls -rrb- online learning algorithm when using the same number of basis functions .
spatial multi-resolution video sequences provide video at multiple frame sizes , allowing extraction of only the resolution or bit rate required by the user . this paper proposes fine-to-coarse motion estimation for multi-resolution video coding . while fine-to-coarse motion estimation , used in previously proposed coding schemes , can provide a better estimate at the coarsest resolution , fine-to-coarse motion estimation is outperformed by fine-to-coarse motion estimation at finer resolutions due to the inability of fine-to-coarse motion estimation to accurately track motion at finer resolutions . at the finest resolution , fine-to-coarse motion estimation provides a psnr improvement of up to 1 db , for the sequences tested , and better visual quality at all resolutions . in addition , fine-to-coarse motion estimation provides more accurate and thus more compressible motion estimates .
we describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains , designated as the source domains . we automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains . the created thesaurus is then used to expand feature vectors to train a binary classifier . unlike previous cross-domain sentiment classification methods , our sentiment classification method can efficiently learn from multiple source domains . our sentiment classification method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing amazon user reviews for different types of products .
pronouns are frequently dropped in chi-nese sentences , especially in informal data such as text messages . in this work we propose a solution to recover dropped pronouns in sms data . we manually annotate dropped pronouns in 684 sms files and apply machine learning algorithms to recover them , leveraging lexical , contextual and syntactic information as features . we believe this is the first work on recovering dropped pronouns in chinese text messages .
the weights of an optimum pb -lrb- pre-steered broadband -rrb- antenna array processor are often obtained by solving a lcmv -lrb- linearly constrained minimum variance -rrb- problem . the objective function is the mean output power -lrb- variance -rrb- and the constraints space is a set of linear equations which ensure a constant gain in a fixed direction known as the look direction . however , errors in a practical scenario could degrade the performance of the lcmv processor significantly , namely , mismatches between the look direction and the actual doa -lrb- direction of arrival -rrb- of the desired signal , positional errors in the sensors and quantization errors in the pre-steered front end of the broadband processor . the main contribution of this paper is the derivation of a new set of constraints , referred to as the pre-steering derivative constraints , which is able to maintain the processor robustness in the general 3d -lrb- three dimensional -rrb- space scenario with all the errors mentioned above .
the design of strategies for branching in mixed integer programming is guided by cycles of parameter tuning and offline experimentation on an extremely heterogeneous testbed , using the average performance . once devised , these strategies -lrb- and their parameter settings -rrb- are essentially input-agnostic . to address these issues , we propose a machine learning framework for variable branching in mip . our machine learning framework observes the decisions made by strong branching , a time-consuming strategy that produces small search trees , collecting features that characterize the candidate branching variables at each node of the tree . based on the collected data , we learn an easy-to-evaluate surrogate function that mimics the strong branching , by means of solving a learning-to-rank problem , common in ml . the learned ranking function is then used for branching . the learning is instance-specific , and is performed on-the-fly while executing a branch-and-bound search to solve the instance . experiments on benchmark instances indicate that our machine learning framework produces significantly smaller search trees than existing heuristics , and is competitive with a state-of-the-art commercial solver .
in this paper an iterative chinese new phrase extraction method based on the intra-phrase association and context variation statistics is proposed . a chinese language model enhancement framework including lexicon expansion is then developed . extensive experiments for chinese broadcast news transcription were then performed to explore the achievable improvements with respect to the degree of temporal consistency for the adaptation corpora . very encouraging results were obtained and detailed analysis discussed .
performance of traditional speech enhancement techniques like spectral subtraction and log-minimum mean squared error short time spectral amplitude -lrb- log-mmse stsa -rrb- estimation degrades in presence of highly non-stationary noises like babble noise . this is mainly due to inaccurate noise estimation during the voiced segment of the speech signal . in this paper , we propose to exploit the fine structure of the phase spectra of the voiced speech in the baseband stft domain . this phase structure is used to detect the noise dominant frequency bins in the voiced frames . this information is used to achieve better non-stationary noise power spectral density -lrb- psd -rrb- estimation . using this estimation , performance of spectral subtraction and log-mmse stsa is improved overall by 0.3 and 0.2 , respectively , in terms of perceptual evaluation of speech quality -lrb- pesq -rrb- measure over the original algorithms when noisy speech is used for pitch estimation . we also present the combination of these two algorithms -lrb- spectral subtraction and log-mmse stsa -rrb- to achieve the overall pesq improvement of 0.5 over standard log-mmse stsa when accurate pitch estimation is available .
this paper explores the usefulness of prosody in automatically compressing dialogue acts from meeting speech . specifically , this work attempts to compress utterances by preserving the pitch contour of the original whole utterance . two methods of doing this are described in detail and are evaluated subjectively using human an-notators and objectively using edit distance with a human-authored gold-standard . both metrics show that such a prosodic approach is much better than the random baseline approach and significantly better than a simple text compression method .
the fact that speakers vary pronunciations of the same word within their own speech is well known , but little has been done to automatically categorize and predict a speaker 's pronunciation distribution for unit selection speech synthesis . recent work demonstrated how to automatically identify a speaker 's choice between full and reduced pronunciations using acoustic modeling techniques from speech recognition . here , we extend this approach and show how its results can be used to predict a speaker 's choice of pronunciations for synthesis . we apply machine learning techniques to the automatically categorized data to produce a pronunciation variation prediction model given only the utterance text -- allowing the system to synthesize novel phrases with variations like those the speaker would make . empirical studies emphasize that we can improve automatic pronunciation labels and successfully utilize the results for prediction of future synthesized examples . the prediction results based on these automatic pronunciation labels are very similar to those trained from human labeled data -- allowing us to reduce manual effort while still achieving comparable results .
we present and analyze a mechanism for the combina-torial public project problem . the problem asks to select k out of m available items , so as to maximize the social welfare for autonomous agents with combi-natorial preferences -lrb- valuation functions -rrb- over subsets of items . the combina-torial public project problem constitutes an abstract model for decision making by autonomous agents and has been shown to present severe computational hardness , in the design of truthful approximation mechanisms . we study a non-truthful mechanism that is , however , practically relevant to multi-agent environments , by virtue of its natural simplicity . combina-torial public project problem employs an item bidding interface , wherein every agent issues a separate bid for the inclusion of each distinct item in the outcome ; the k items with the highest sums of bids are chosen and agents are charged according to a vcg-based payment rule . for fairly expressive classes of the agents ' valuation functions , we establish existence of socially optimal pure nash and strong equilibria , that are resilient to coordinated deviations of subsets of agents . subsequently we derive tight worst-case bounds on the approximation of the optimum social welfare achieved in equilibrium . we show that the mechanism 's performance improves with the number of agents that can coordinate , and reaches half of the optimum welfare at strong equilibrium .
in single-channel speech enhancement the spectral amplitude of the noisy signal is often modified while the noisy spectral phase is directly employed for signal reconstruction . recently , additional improvement in speech enhancement performance has been reported when the noisy spectral phase is modified . in this work , we propose a bayesian estimator for phase of harmonics given the noisy speech . the proposed bayesian estimator relies on the fundamental frequency and the signal-to-noise ratio at harmonics . throughout our experiments , we evaluate the performance of the proposed single-channel speech enhancement in comparison with the noisy spectral phase , a benchmark and the clean phase as the upper-bound . the proposed bayesian estimator leads to joint improvement in quality and intelligibility at different snrs and noise types .
while stable matching problems are widely studied , little work has investigated schemes for effectively eliciting agent preferences using either preference -lrb- e.g. , comparison -rrb- queries or interviews -lrb- to form such comparisons -rrb- ; and no work has addressed how to combine both . we develop a new model for representing and assessing agent preferences that accommodates both forms of information and -lrb- heuristically -rrb- minimizes the number of queries and interviews required to determine a stable matching . our refine-then-interview -lrb- rti -rrb- scheme uses coarse preference queries to refine knowledge of agent preferences and relies on interviews only to assess comparisons of relatively '' close '' options . empirical results show that rti compares favorably to a recent pure interview minimization algorithm , and that the number of interviews it requires is generally independent of the size of the market .
this paper describes an ontology for inland water features built using formal concept analysis and su-pervaluation semantics . the first is used to generate a complete lattice of the water domain , whereas supervaluation semantics is used to model the variability of the concepts in terms of threshold parameters . we also present an algorithm for a mechanism of individuation and classification of water features , from snapshots of river networks , according to the proposed ontology .
a new approach for the computation of probability distributions for coupled first-order chemical reactions is introduced . the approach is based on system theory , where the system theory states are chemical species and the signals are probabilities . the transfer functions of the so defined systems for coupled reactions can be applied to both closed and open reaction environments . the use of block diagrams offers a clear , visual , and convenient way to decompose a complicated reaction system into simpler subsystems and vice versa . since the state of the system theory is defined as a molecule species instead of molecule population , with this method one can study chemical reactions involving any number of molecules . such analysis is shown on an enzyme catalyst reaction . in addition , a special form of reaction topology , the first-order reaction chain , is studied , and the analytic solution for its distribution is derived .
generalized linear models -lrb- generalized linear models -rrb- are an increasingly popular framework for modeling neural spike trains . they have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-fit using methods from point-process theory , e.g. the time-rescaling theorem . however , high neural firing rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection . here , we show how goodness-of-fit tests from point-process theory can still be applied to generalized linear models by constructing equivalent surrogate point processes out of time-series observations . furthermore , two additional tests based on thinning and complementing point processes are introduced . they augment the instruments available for checking model adequacy of point processes as well as discretized models .
the standard approach to the classification of objects is to consider the examples as independent and identically distributed -lrb- iid -rrb- . in many real world settings , however , this assumption is not valid , because a topo-graphical relationship exists between the objects . in this contribution we consider the special case of image segmentation , where the objects are pixels and where the underlying topography is a 2d regular rectangular grid . we introduce a classification method which not only uses measured vectorial feature information but also the label configuration within a to-pographic neighborhood . due to the resulting dependence between the labels of neighboring pixels , a collective classification of a set of pixels becomes necessary . we propose a new method called ` topographic support vector machine ' -lrb- tsvm -rrb- , which is based on a topographic kernel and a self-consistent solution to the label assignment shown to be equivalent to a recurrent neural network . the performance of the algorithm is compared to a conventional svm on a cell image segmentation task .
detection of activity is a key capability for microphone arrays . an array system should tell when a source of interest is present and evaluate the usability of the computed spatial estimates . this work proposes activity features that are computed from spatial data only , using time delays and direction of arrival . the activity features are validated with a loudspeaker experiment . results show that the activity features are effective : tolerable detection errors are achievable with simple detection methods . in addition , direction of arrival estimation error is reduced down to one third when unreliable estimates are discarded .
despite recent breakthroughs in the applications of deep neural networks , one setting that presents a persistent challenge is that of '' one-shot learning . '' traditional gradient-based networks require a lot of data to learn , often through extensive iterative training . when new data is encountered , the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference . architec-tures with augmented memory capacities , such as neural turing machines , offer the ability to quickly encode and retrieve new information , and hence can potentially obviate the down-sides of conventional models . here , we demonstrate the ability of a memory-augmented neu-ral network to rapidly assimilate new data , and leverage this data to make accurate predictions after only a few samples . we also introduce a new method for accessing an external memory that focuses on memory content , unlike previous methods that additionally use memory location-based focusing mechanisms .
we propose a content-based approach to mine parallel resources from the entire web using cross lingual information retrieval with search query relevance score . our content-based approach improves mining recall by going beyond url matching to find parallel documents from non-parallel sites . we introduce search query relevance score to improve the precision of mining . our content-based approach makes use of search engines to query for target document given each source document and therefore does not require downloading target language documents in batch mode , reducing computational cost on the local machines and bandwidth consumption . we obtained a very high mining precision -lrb- 88 % -rrb- on the parallel documents by the pure clir approach . after extracting parallel sentences from the mined documents and using them to train an cross lingual information retrieval , we found that the smt performance , with 29.88 bleu score , is comparable to that obtained with high quality manually translated parallel sentences with 29.54 bleu score , illustrating the excellent quality of the mined parallel material .
a time domain simulation of acoustic propagation in the vocal tract requires the spatial and temporal discretization of the equations of motion and continuity . in the classic transmission line model of the vocal tract with lumped elements , the spatial discretization is provided by the piece-wise constant area function . the temporal finite-difference approximation of the differential equations can , however , vary from one implementation to the other -lrb- e.g. , -lsb- 4 -rsb- vs. -lsb- 5 -rsb- -rrb- . in this study , we have adopted a general finite-difference scheme that depends on a parameter θ where 0 ≤ θ ≤ 1 . as special cases , this general finite-difference scheme includes the trapezoid rule -lrb- θ = 0.5 -rrb- as well as the implicit -lrb- θ = 1 -rrb- and explicit -lrb- θ = 0 -rrb- finite-difference schemes . we have examined how formant frequencies and bandwidths of simulated vowels are effected by the choice of θ . the experiments were conducted for the sampling rates of 44.1 khz and 88.2 khz and compared with the accurate and thus desirable frequencies and bandwidths measured in frequency domain simulations of the vocal tract . it can be shown that optimal values for θ are slightly above 0.5 depending on the sampling rate .
non-negative matrix factorization -lrb- non-negative matrix factorization -rrb- has previously been shown to be a useful decomposition for multivariate data . two different multi-plicative algorithms for non-negative matrix factorization are analyzed . non-negative matrix factorization differ only slightly in the multiplicative factor used in the update rules . one algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized kullback-leibler divergence . the monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the expectation-maximization algorithm . the algorithms can also be interpreted as diagonally rescaled gradient descent , where the rescaling factor is optimally chosen to ensure convergence .
this paper aims at a newly raising task in visual surveillance : re-identifying people at a distance by matching body information , given several reference examples . most of existing works solve this task by matching a reference template with the target individual , but often suffer from large human appearance variability -lrb- e.g. different poses/views , illumination -rrb- and high false positives in matching caused by conjunctions , occlusions or surrounding clutters . addressing these problems , we construct a simple yet expressive template from a few reference images of a certain individual , which represents the body as an articulated assembly of compositional and alternative parts , and propose an effective matching algorithm with cluster sampling . this algorithm is designed within a candidacy graph whose vertices are matching candidates -lrb- i.e. a pair of source and target body parts -rrb- , and iterates in two steps for convergence . -lrb- i -rrb- it generates possible partial matches based on compatible and competitive relations among body parts . -lrb- ii -rrb- it confirms the partial matches to generate a new matching solution , which is accepted by the markov chain monte carlo mechanism . in the experiments , we demonstrate the superior performance of our approach on three public databases compared to existing methods .
many ai problems , when formalized , reduce to evaluating the probability that a propo-sitional expression is true . in this paper we show that this problem is computationally intractable even in surprisingly restricted cases and even if we settle for an approximation to this probability . we consider various methods used in approximate reasoning such as computing degree of belief and bayesian belief networks , as well as reasoning techniques such as constraint satisfaction and knowledge compilation , that use approximation to avoid computational diiculties , and reduce them to model-counting problems over a propositional domain . we prove that counting satisfying assignments of propositional languages is intractable even for horn and monotone formulae , and even when the size of clauses and number of occurrences of the variables are extremely limited . this should be contrasted with the case of deductive reasoning , where horn theories and theories with binary clauses are distinguished by the existence of linear time satissability algorithms . what is even more surprising is that , as we show , even approximating the number of satisfying assignments -lrb- i.e. , \ approximating '' approximate reasoning -rrb- , is intractable for most of these restricted theories . we also identify some restricted classes of propositional formulae for which eecient algorithms for counting satisfying assignments can be given .
we propose a novel attentional model for simultaneous object tracking and recognition that is driven by gaze data . motivated by theories of the human perceptual system , the attentional model consists of two interacting pathways : ventral and dorsal . the ventral pathway models object appearance and classification using deep -lrb- factored -rrb- - restricted boltzmann machines . at each point in time , the observations consist of retinal images , with decaying resolution toward the periphery of the gaze . the dorsal pathway models the location , orientation , scale and speed of the attended object . the posterior distribution of these states is estimated with particle filtering . deeper in the dorsal pathway , we encounter an attentional mechanism that learns to control gazes so as to minimize tracking uncertainty . the attentional model is modular -lrb- with each module easily replaceable with more sophisticated algorithms -rrb- , straightforward to implement , practically efficient , and works well in simple video sequences .
in this paper , we present a novel segmentation-insensitive approach for mining common patterns from 2 images . we develop an segmentation-insensitive approach using the earth movers distance framework , unary and adaptive neighborhood color similarity . we then propose a novel local flow maximization approach to provide the best estimation of location and scale of the common pattern . this is achieved by performing an iterative optimization in search of the most stable flows ' centroid . common pattern discovery is difficult owing to the huge search space and problem domain . we intend to solve this problem by reducing the search space through identifying the location and a reduced spatial space for common pattern discovery . experimental results justify the effectiveness and the potential of the local flow maximization approach .
we propose a novel model to automatically extract transliteration pairs from parallel corpora . our model is efficient , language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings . we model transliter-ation mining as an interpolation of translitera-tion and non-transliteration sub-models . we evaluate on news 2010 shared task data and on parallel corpora with competitive results .
in ∀ ∃ - rules , the conclusion may contain existen-tially quantified variables , which makes reasoning tasks -lrb- as deduction -rrb- non-decidable . these rules have the same logical form as tgd -lrb- tuple-generating dependencies -rrb- in databases and as conceptual graph rules . we extend known decidable cases by combining backward and forward chaining schemes , in association with a graph that captures exactly the notion of dependency between rules . finally , we draw a map of known decidable cases , including an extension obtained by combining our approach with very recent results on tgd -lrb- tuple-generating dependencies -rrb- .
we consider the image retrieval problem of finding the images in a dataset that are most similar to a query image . our goal is to reduce the number of vector operations and memory for performing a search without sacrificing accuracy of the returned images . we adopt a group testing formulation and design the decoding architecture using either dictionary learning or eigendecomposition . the latter is a plausible option for small-to-medium sized problems with high-dimensional global image descriptors , whereas dictionary learning is applicable in large-scale scenarios . we evaluate our approach for global descriptors obtained from both sift and cnn features . experiments with standard image search benchmarks , including the ya-hoo100m dataset comprising 100 million images , show that our method gives comparable -lrb- and sometimes superior -rrb- accuracy compared to exhaustive search while requiring only 10 % of the vector operations and memory . moreover , for the same search complexity , our method gives significantly better accuracy compared to approaches based on dimen-sionality reduction or locality sensitive hashing .
we investigate the coarse synchronization performance of matched-filter-type -lrb- mf -rrb- and minimum-variance-distortionless-response-type linear self-synchronized receivers for asynchronous direct-sequence code-division-multiple-access communications under finite data record adaptation . analytic expressions are derived that approximate closely the probability of coarse synchronization error and provide low-cost highly-accurate alternatives to the computationally demanding performance evaluation through simulations . the expressions are explicit functions of the data record size n and the filter order p and reveal the effect of short-data-record sample-matrix-inversion implementations on the coarse synchronization performance . 1 . introduction the effectiveness of a receiver designed for rapidly changing wireless direct-sequence code-division-multiple-access communication environments depends on establishing a successful tradeoff among the following three design objectives : -lrb- z -rrb- low computational complexity , -lrb- 22 -rrb- multiple-access-interference -lrb- mai -rrb- near-far resistance , and -lrb- iii -rrb- system adaptivity with superior performance under limited data support . adaptive short-data-record designs appear as the natural next step -lsb- l -rsb- to a matured discipline that has extensively addressed the first two design objectives in ideal setups -lrb- perfectly known or asymptotically estimated statistical properties -rrb- -lsb- 2 -rsb- . system adaptivity based on short data records is necessary for the development of receivers that exhibit superior bit-error-rate performance when system adaptivity operate in rapidly changing communication environments that limit substantially the available data support . in -lsb- 3 -rsb- , we considered self-synchronized receivers -lrb- integrated synchronizers/demodulators -rrb- and we presented
we demonstrate real-time face tracking and pose estimation in an unconstrained ooce environment with an active foveated camera . using vision routines previously implemented for an interactive environment , we determine the spatial location of a user 's head and guide an active camera to obtain foveated images of the face . faces are analyzed using a set of eigenspaces indexed over both pose and world location . closed loop feedback from the estimated facial location is used to guide the camera when a face is present in the foveated view . our system can detect the head pose of an unconstrained user in real-time as he or she moves about an open room .
association field models have attempted to explain human contour grouping performance , and to explain the mean frequency of long-range horizontal connections across cortical columns in v1 . however , association fields only depend on the pairwise statistics of edges in natural scenes . we develop a spectral test of the suf-ficiency of pairwise statistics and show there is significant higher order structure . an analysis using a probabilistic spectral embedding reveals curvature-dependent components .
in this paper , we present our latest investigations of language modeling for code-switching . since there is only little text material for code-switching speech available , we integrate syntactic and semantic features into the language modeling process . in particular , we use part-of-speech tags , language identifiers , brown word clusters and clusters of open class words . we develop factored language models and convert recurrent neu-ral network language models into backoff language models for an efficient usage during decoding . a detailed error analysis reveals the strengths and weaknesses of the different language models . when we interpolate the backoff language models linearly , we reduce the perplexity by 15.6 % relative on the seame evaluation set . this is even slightly better than the result of the unconverted recurrent neural network . we also combine the language models during decoding and obtain a mixed error rate reduction of 4.4 % relative on the seame evaluation set .
recent works have shown that the particle filter using color as observation feature is a powerful technique for tracking deformable objects in image sequences with complex backgrounds . this paper presents a hybrid valued sequential state estimation algorithm , and its particle filter-based solution , that extends the standard color particle filter in two ways . firstly , track initialization is embedded in the particle filter without relying on an external target detection algorithm . secondly , the hybrid valued sequential state estimation algorithm is able to track multiple objects sharing the same color description . we evaluate the performance of the proposed hybrid valued sequential state estimation algorithm on various real-world video sequences with appearing and disappearing targets .
one of the major problems in multiple target tracking is to obtain an accurate association between targets and noisy measurements . we introduce a new scheme , called constrained optimal data association -lrb- coda -rrb- , that finds the optimal data association by a map estimation method and uses a new energy function . in this scheme , the natural constraints between targets and measurements are defined so that natural constraints may contain missed detection and false alarm errors . most current algorithms involve many heuris-tic adjustments of the parameters . instead , this paper suggests an adaptive mechanism for such parameter updation . in this manner , the system automatically adapts to the clutter environment as it continuously changes in time and space , resulting in better association . experimental results , using pda , nnf , and coda , show that the new approach reduces position errors in crossing trajecto-ries by 13.9 % on average compared to nnf .
an important problem in concatenative synthesis is the occurence of spectral discontinuities or '' concatenation mismatch '' between sonorant speech units . in this paper , we present an approach to reduce concatenation mismatch by combining spectral information from two sequences of speech units selected in parallel . con-catenation units , on one hand , define initial spectral trajectories for a target utterance . fusion units , on the other hand , define the desired transitions between concatenated units . the two con-catenation units are '' fused '' by imposing dynamic constraints defined by the fusion units on the spectral trajectories of the concatenation units . to regenerate the modified speech units , we use a synthesis algorithm based on sinusoidal + all-pole analysis of speech , which overcomes the limitations of residual-excited lpc . results from a perceptual test show that our method is highly successful at removing concatenation artifacts in speech generated from an inventory of diphones .
we address the problem of speaker clustering for robust un-supervised speaker diarization . we model each speaker-homogeneous segment as one single full multivariate gaussian probability density function -lrb- pdf -rrb- and take into consideration the riemannian property of gaussian pdfs . by assuming that segments from different speakers lie on different -lrb- possibly intersected -rrb- sub-manifolds of the manifold of gaussian pdfs , we formulate the original problem as a riemannian manifold clustering problem . to apply the computationally simple rieman-nian locally linear embedding algorithm , we impose a constraint on the length of each segment so as to ensure the fitness of single-gaussian modeling and to increase the chance that all k-nearest neighbors of a pdf are from the same sub-manifold -lrb- speaker -rrb- . experiments on the microphone-recorded conversational interviews from nist 2010 speaker recognition evaluation set demonstrate promising results of less than 1 % der .
we present a variational bayesian inference algorithm for the stick-breaking construction of the beta process . we derive an alternate representation of the beta process that is amenable to variational inference , and present a bound relating the truncated beta process to its infinite counterpart . we assess performance on two matrix factorization problems , using a non-negative factorization model and a linear-gaussian model .
the multiple signal classification algorithm for acoustic imaging most commonly assumes that all sources have undirectional radiation pattern . we here propose a modification of this multiple signal classification algorithm such that the concept is applicable for arbitrary directional characteristics of the sources . this is accomplished by fitting for each frequency the real valued amplitudes of the acoustic imaging rather than assuming a fixed functional form . the mathematical problem can be solved analytically resulting in an eigenvalue problem of a real valued hamiltonian matrix . the performance is illustrated in simulations using pure monopolar , dipolar and quadrupolar sources .
recent advances in bayesian learning with large-scale data have witnessed emergence of stochastic gradient mcmc algorithms , such as stochas-tic gradient langevin dynamics , stochastic gradient hamiltonian mcmc , and the stochastic gradient thermostat . while finite-time convergence properties of the stochas-tic gradient langevin dynamics with a 1st-order euler integrator have recently been studied , corresponding theory for general stochastic gradient mcmc algorithms has not been explored . in this paper we consider general stochastic gradient mcmc algorithms with high-order integrators , and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures . our theoretical results show faster convergence rates and more accurate invariant measures for stochastic gradient mcmc algorithms with higher-order integrators . for example , with the proposed efficient 2nd-order symmetric splitting integrator , the mean square error of the posterior average for the stochastic gradient mcmc algorithms achieves an optimal convergence rate of l − 4/5 at l iterations , compared to l − 2/3 for the stochastic gradient mcmc algorithms and stochas-tic gradient langevin dynamics with 1st-order euler integrators . furthermore , convergence results of stochastic gradient mcmc algorithms are also developed , with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence . experiments on both synthetic and real datasets verify our theory , and show advantages of the proposed method in two large-scale real applications .
we address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems . unlike previous studies that focus on user 's knowledge or typical kinds of users , the user modeling we propose is more comprehensive . specifically , we set up three dimensions of user modeling : skill level to the system , knowledge level on the target domain and the degree of hastiness . moreover , the user modeling are automatically derived by decision tree learning using real dialogue data collected by the system . we obtained reasonable classification accuracy for all dimensions . dialogue strategies based on the user modeling are implemented in kyoto city bus information system that has been developed at our laboratory . experimental evaluation shows that the cooperative responses adaptive to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users .
we provide a method that approximates the bayes error rate and the shannon entropy with high probability . the bayes error rate approximation makes possible to build a clas-sifier that polynomially approaches bayes error rate . the shannon entropy approximation provides provable performance guarantees for learning trees and bayesian networks from continuous variables . our results rely on some reasonable regularity conditions of the unknown probability distributions , and apply to bounded as well as unbounded variables .
most common approaches to phonotactic language recognition deal with several independent phone decoders . decodings are processed and scored in a fully uncoupled way , their time alignment -lrb- and the information that may be extracted from it -rrb- being completely lost . recently , we have presented a new approach to phonotactic language recognition which takes into account time alignment information , by considering cross-decoder co-occurrences of phones or phone n-grams at the frame level . experiments on the nist lre2007 database demonstrated that using co-occurrence statistics could improve the performance of baseline phonotactic recognizers . in this work , the approach based on cross-decoder co-occurrences of phone n-grams is further developed and evaluated . systems were built by means of open software -lrb- brno university of technology phone de-coders , liblinear and focal -rrb- and experiments were carried out on the nist lre2007 database . a system based on co-occurrences of phone n-grams -lrb- up to 4-grams -rrb- outperformed the baseline phonotactic recognizers , yielding around 8 % relative improvement in terms of eer . the best fused system attained 1,90 % eer -lrb- a 16 % improvement with regard to the baseline phonotactic recognizers -rrb- , which supports the use of cross-decoder dependencies for improved language modeling .
recently there has been a considerable focus on information retrieval for multimedia databases . when speech is used as the source material for multimedia indexing , the effect of transcriber error on retrieval effectiveness must be considered . this paper describes a method for measuring the relevance of documents to queries when information about the probability of word transcription error is available . to support the use of this technique , a method is presented for estimating word error probability in speech recognition engines that use word graphs -lrb- lattices -rrb- . an information retrieval experiment using this technique on a large corpus of spoken documents is discussed . the method was able to reduce the difference in retrieval effectiveness between reference texts and hypothesized texts by 13 % -38 % depending on the size of the document set .
translucent objects pose a difficult problem for traditional structured light 3d scanning techniques . subsurface scattering corrupts the range estimation in two ways : by drastically reducing the signal-to-noise ratio and by shifting the intensity peak beneath the surface to a point which does not coincide with the point of incidence . in this paper we analyze and compare two descattering methods in order to obtain reliable 3d coordinates for translucent objects . by using polarization-difference imaging , subsurface scattering can be filtered out because multiple scattering ran-domizes the polarization direction of light while the surface reflectance partially keeps the polarization direction of the illumination . the descattered reflectance can be used for reliable 3d coordinates using traditional optical 3d scanning techniques , such as structured light . phase-shifting is another effective descattering technique if the frequency of the projected pattern is sufficiently high . we demonstrate the performance of these two techniques and the combination of phase-shifting on scanning real-world translucent objects .
in the framework of a theory for invariant sensory signal representations , a signature which is invariant and selective for speech sounds can be obtained through projections in template signals and pooling over their transformations under a group . for locally compact groups , e.g. , translations , the theory explains the resilience of convolutional neural networks with filter weight sharing and max pooling across their local translations in frequency or time . in this paper we propose a discrimina-tive approach for learning an optimum set of templates , under a family of transformations , namely frequency transpositions and perturbations of the vocal tract length , which are among the primary sources of speech variability . implicitly , we generalize convolutional networks to transformations other than translations , and derive data-specific templates by training a deep network with convolution-pooling layers and densely connected layers . we demonstrate that such a convolutional networks , combining group-generalized convolutions , theoretical invariance guarantees and discriminative template selection , improves frame classification performance over standard translation-cnns and dnns on timit and wall street journal datasets .
different approaches of minima tracking based noise estimation algorithms are compared and modifications increasing their efficiency are proposed . estimated noise is used by noise suppression algorithm that is a part of speech recognition system . moreover , the noise suppression algorithm are developed to be applied in feature extraction of distributed speech recognition . therefore we propose such modifications to the noise estimation techniques that are quickly adaptable on varying noise and do not need so much information from past segments . we also minimized the algorithmic delay . the robustness of proposed noise suppression algorithm were tested under several noisy conditions on five speech-dat car -lrb- sdc -rrb- and aurora 2 evaluation databases .
this paper describes a crosslinguistic disfluency perception experiment . we tested the recognizability of pause fillers and partial words in english , german and mandarin . subjects were speakers of english with no knowledge of mandarin or ger-man . we found that subjects could identify disfluent from fluent utterances at a level above chance . pause fillers were easier to identify than partial words . accuracy rates were highest for english , followed by german and then mandarin . although german accuracy rates were higher than those for mandarin , discriminability analysis suggests that this is due to conservative bias towards false negatives rather than non-recognition of the acoustic material . the fact that subjects could identify dis-fluent speech in languages they did not know shows that there are real phonetic crosslinguistic cues to disfluency .
we address the problem of learning object class models and object segmentations from unannotated images . we introduce locus -lrb- learning object classes with unsupervised segmentation -rrb- which uses a generative probabilistic model to combine bottom-up cues of color and edge with top-down cues of shape and pose . a key aspect of this generative probabilistic model is that the object appearance is allowed to vary from image to image , allowing for significant within-class variation . by iteratively updating the belief in the object 's position , size , segmentation and pose , locus -lrb- learning object classes avoids making hard decisions about any of these quantities and so allows for each to be refined at any stage . we show that locus -lrb- learning object classes successfully learns an object class model from unlabeled images , whilst also giving segmentation accuracies that rival existing supervised methods . finally , we demonstrate simultaneous recognition and segmentation in novel images using the learned models for a number of object classes , as well as unsupervised object discovery and tracking in video .
this paper presents a fixed-and low-dimensional , perceptually based dynamic sinusoidal model of speech referred to as pdm -lrb- perceptual dynamic model -rrb- . to decrease and fix the number of sinusoidal components typically used in the standard sinusoidal model , we propose to use only one dynamic sinusoidal component per critical band . for each band , the si-nusoid with the maximum spectral amplitude is selected and associated with the centre frequency of that critical band . the model is expanded at low frequencies by incorporating sinu-soids at the boundaries of the corresponding bands while at the higher frequencies a modulated noise component is used . a listening test is conducted to compare speech reconstructed with pdm -lrb- perceptual dynamic model -rrb- and state-of-the-art models of speech , where all models are constrained to use an equal number of parameters . the results show that pdm -lrb- perceptual dynamic model -rrb- is clearly preferred in terms of quality over the other systems .
the emime project aims to build a personalized speech-to-speech translator , such that spoken input of a user in one language is used to produce spoken output that still sounds like the user 's voice however in another language . this distinctiveness makes unsupervised cross-lingual speaker adaptation one key to the project 's success . so far , research has been conducted into unsupervised and cross-lingual cases separately by means of decision tree marginalization and hmm state mapping respectively . in this paper we combine the two techniques to perform unsupervised cross-lingual speaker adaptation . the performance of eight speaker adaptation systems -lrb- supervised vs. unsupervised , intra-lingual vs. cross-lingual -rrb- is compared using objective and subjective evaluations . experimental results show the performance of unsupervised cross-lingual speaker adaptation is comparable to that of the supervised case in terms of spectrum adaptation in the emime scenario , even though automatically obtained transcriptions have a very high phoneme error rate .
we present a novel algorithm for extracting a high-quality case base from raw data while preserving and sometimes improving the competence of case-based reasoning . we extend the framework of smyth and keane 's case-deletion policy with two additional features . first , we build a case base using a statistical distribution that is mined from the input data so that the case-base competence can be preserved or even increased for future problems . second , we introduce a nonlin-ear transformation of the data set so that the case-base sizes can be further reduced while ensuring that the competence be preserved and even increased . we show that smyth and keane 's deletion-based algorithm is sensitive to noisy cases , and that our deletion-based algorithm solves this problem more satisfactorily . we show the theoretical foundation and empirical evaluation on several data sets .
in this paper , we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo . previous work utilizing single person 's nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is com-putationally feasible in computer vision . however , in practice , multiple people with arbitrary poses are common in a photo , and recognizing their occupations is even more challenging . we argue that with appropriately built visual attributes , co-occurrence , and spatial configuration model that is learned through structure svm , we can recognize multiple people 's occupations in a photo simultaneously . to evaluate our method 's performance , we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7k images . results on this database validate our method 's effectiveness and show that occupation recognition is solv-able in a more general case .
in this paper we present npen + + , a connectionist system for writer independent , large vocabulary on-line cursive handwriting recognition . this npen + + combines a robust input representation , which preserves the dynamic writing information , with a neural network architecture , a so called multi-state time delay neural network , which integrates rec.ognition and segmen-tation in a single framework . our preprocessing transforms the original coordinate sequence into a -lrb- still temporal -rrb- sequence offea-ture vectors , which combine strictly local features , like curvature or writing direction , with a bitmap-like representation of the co-ordinate 's proximity . the ms-tdnn architecture is well suited for handling temporal sequences as provided by this robust input representation . our npen + + is tested both on writer dependent and writer independent tasks with vocabulary sizes ranging from 400 up to 20,000 words . for example , on a 20,000 word vocabulary we achieve word recognition rates up to 88.9 % -lrb- writer dependent -rrb- and 84.1 % -lrb- writer independent -rrb- without using any language models .
a stretch of speech is often consistent with multiple words , e.g. , the sequence / haem / is consistent with ` ham ' but also with the first syllable of ` hamster ' , resulting in temporary ambiguity . however , to what degree does this lexical embedding occur ? analyses on two corpora of spoken dutch showed that 11.9 % -19.5 % of polysyllabic word tokens have word-initial embedding , while 4.1 % -7.5 % of monosyllabic word tokens can appear word-initially embedded . this is much lower than suggested by an analysis of a large dictionary of dutch . speech processing thus appears to be simpler than one might expect on the basis of statistics on a dictionary .
we have investigated the properties of neurons in inferior temporal cortex in monkeys performing a pattern matching task . simple back-propagation networks were trained to discriminate the various stimulus conditions on the basis of the measured neuronal signal . we also trained back-propagation networks to predict the neuronal response waveforms from the spatial patterns of the stimuli . the results indicate t.hat it neurons convey temporally encoded information about both current and remembered patterns , as well as about their behavioral context .
we present a sample re-weighting scheme inspired by recent results in margin theory . the basic idea is to add to the training set replicas of samples which are not classified with a sufficient margin . we prove the convergence of the input distribution obtained in this way . as study case , we consider an instance of the sample re-weighting scheme involving a 1-nn classifier implementing a vector quantization algorithm that accommodates tangent distance models . the tangent distance models created in this way have shown a significant improvement in generalization power with respect to the standard tangent models . moreover , the obtained models were able to outperform state of the art algorithms , such as svm .
this work continues in development of the recently proposed bottleneck features for asr . a five-layers mlp used in bottleneck feature extraction allows to obtain arbitrary feature size without dimensionality reduction by transforms , independently on the mlp training targets . the mlp topology -- number and sizes of layers , suitable training targets , the impact of output feature transforms , the need of delta features , and the dimensionality of the final feature vector are studied with respect to the best asr result . optimized features are employed in three lvcsr tasks : arabic broadcast news , english conversational telephone speech and english meetings . improvements over standard cepstral features and probabilistic mlp features are shown for different tasks and different neural net input representations . a significant improvement is observed when phoneme mlp training targets are replaced by phoneme states and when delta features are added .
we address the reconstruction problem of a high resolution image from its undersampled measurements accross multiple fir channels with unknown response . our method consists of two stages : blind multi-input multi-output deconvolution using fir filters and blind separation of mixed polyphase components . the proposed deconvolution method is based on the mutually referen-ced equalizers algorithm previously developed for blind equalization in digital communications . for sources separation , a method is proposed for separating mixed polyphase components of a bandlimited signal . the existing blind source separation algorithms assume that the source signals are either independent or uncorrelated , which is not the case when the sources are polyphase components of a bandlimited signal . simulation results on artificial and photographics images are given .
even though the quality of unsupervised dependency parsers grows , unsupervised dependency parsers often fail in recognition of very basic dependencies . in this paper , we exploit a prior knowledge of stop-probabilities -lrb- whether a given word has any children in a given direction -rrb- , which is obtained from a large raw corpus using the reducibility principle . by incorporating this knowledge into dependency model with valence , we managed to considerably outperform the state-of-the-art results in terms of average attachment score over 20 treebanks from conll 2006 and 2007 shared tasks .
the ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision . however , the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text . we address this challenge with contributions in two folds : first , we introduce the new task of image caption generalization , formulated as visually-guided sentence compression , and present an efficient algorithm based on dynamic beam search with dependency-based constraints . second , we release a new image-text parallel corpus with 1 million image-caption pairs achieving tighter content alignment between images and text . evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new image-text parallel corpus with respect to a concrete application of image caption transfer .
we address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution -- problems also referred to as classification under covariate shift . we derive a solution that is purely discriminative : neither training nor test distribution are modeled explicitly . we formulate the general problem of learning under covariate shift as an integrated optimization problem . we derive a kernel logistic regression classifier for differing training and test distributions .
this paper proposes non-intrusive iris image capturing system , which consists of pan-tilt-zoom camera and light stripe projection . light stripe projection provides the position of user . after panning according to user 's position , adaboost-based face detection finds tilt angle . with user 's position and tilt angle , zoom and focus position are initialized . user 's position replaces 2d face search with 1d face search . exact zoom and focus position enable fast control and narrow search range . consequently , experimental results show that proposed non-intrusive iris image capturing system can capture iris image within acceptable time .
recently , neural network models have achieved consistent improvements in statistical machine translation . however , most neural network models only use one-hot encoded input vectors of words as their input . in this work , we investigated the exponentially decaying bag-of-words input features for feed-forward neural network translation models and proposed to train the decay rates along with other weight parameters . this novel bag-of-words model improved our phrase-based state-of-the-art system , which already includes a neural network translation model , by up to 0.5 % bleu and 0.6 % ter on three different translation tasks and even achieved a similar performance to the bidirectional lstm translation model .
the matérn class is a parametric family of autocorrelation functions that is commonly used in geostatistics . we argue that a generalized , anisotropic version of this model is suitable for capturing the correlation structure of a variety of natural images . we specify the optimal space for the mmse reconstruction of stochastic matérn signals from their uniformly-sampled noisy measurements -lrb- generalized sampling problem -rrb- . we prove that the optimal reconstruction space is generated by the multi-integer shifts of a matérn function which form a riesz basis . based on this representation , we propose a practical lter-based reconstruction method that relies on the prior identi cation of the matérn parameters from the measured data . we present experimental results to justify the use of the matérn model and to demonstrate the performance of our lter-based reconstruction method .
the 1-regularized gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covari-ance matrix even under high-dimensional settings . however , it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of gaussian variables . state-of-the-art methods thus do not scale to problems with more than 20 , 000 variables . in this paper , we develop an algorithm bigquic , which can solve 1 million dimensional 1-regularized gaussian mle problems -lrb- which would thus have 1000 billion parameters -rrb- using a single machine , with bounded memory . in order to do so , we carefully exploit the underlying structure of the problem . our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations ; and allowing for inexact computation of specific components . in spite of these modifications , we are able to theoretically analyze our block-coordinate descent method and show that bigquic can achieve super-linear or even quadratic convergence rates .
binarization of synchronous context free grammars -lrb- scfg -rrb- is essential for achieving polynomial time complexity of decoding for scfg parsing based machine translation systems . in this paper , we first investigate the excess edge competition issue caused by a left-heavy binary scfg derived with the method of zhang et al. -lrb- 2006 -rrb- . then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary scfgs . we present an algorithm that iteratively improves the resulting binary scfgs , and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous bina-rization method in zhang et al. -lrb- 2006 -rrb- on the nist machine translation evaluation tasks .
we consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients , analyzing their finite-sample convergence rates . we show that if pairs of function values are available , algorithms that use gradient estimates based on random perturbations suffer a factor of at most √ d in convergence rate over traditional stochastic gradient methods , where d is the problem dimension . we complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems , which show that our bounds are sharp with respect to all problem-dependent quantities : they can not be improved by more than constant factors .
retrieving similar questions in online q&a community sites is a difficult task because different users may formulate the same question in a variety of ways , using different vocabulary and structure . in this work , we propose a new neural network architecture to perform the task of semantically equivalent question retrieval . the proposed neural network architecture , which we call bow-cnn , combines a bag-of-words representation with a distributed vector representation created by a convolutional neural network . we perform experiments using data collected from two stack exchange communities . our experimental results evidence that : -lrb- 1 -rrb- bow-cnn is more effective than bow based information retrieval methods such as tfidf ; -lrb- 2 -rrb- bow-cnn is more robust than the pure cnn for long texts .
in order to achieve a complete image description , we introduce no-feature-features -lrb- nf-features -rrb- representing object regions where regular interest point detectors do not detect features . as these regions are usually non-textured , stable re-localization in different images with conventional methods is not possible . therefore , a technique is presented which re-localizes once-detected nf-features using correspondences of regular features . furthermore , a distinctive nf descriptor for non-textured regions is derived which has invariance towards affine transformations and changes in illumination . for the matching of nf descriptors , an approach is introduced that is based on local image statistics . nf-features can be used complementary to all kinds of regular feature detection and description approaches that focus on textured regions , i.e. points , blobs or contours . using sift , mser , hessian-affine or surf as regular detectors , we demonstrate that our approach is not only suitable for the description of non-textured areas but that precision and recall of the nf-features is significantly superior to those of regular features . in experiments with high variation of the perspective or image perturbation , at unchanged precision we achieve nf recall rates which are better by more than a factor of two compared to recall rates of regular features .
we present an approach for recovering the reflectance of a static scene with known geometry from a collection of images taken under distant , unknown illumination . in contrast to previous work , we allow the illumination to vary between the images , which greatly increases the applicability of the approach . using an all-frequency relighting framework based on wavelets , we are able to simultaneously estimate the per-image incident illumination and the per-surface point reflectance . the wavelet framework allows for incorporating various reflection models . we demonstrate the quality of our results for synthetic test cases as well as for several datasets captured under laboratory conditions . combined with multi-view stereo reconstruction , we are even able to recover the geometry and reflectance of a scene solely using images collected from the internet .
we develop an automated variational method for approximate inference in gaus-sian process models whose posteriors are often intractable . using a mixture of gaussians as the variational distribution , we show that -lrb- i -rrb- the variational objective and its gradients can be approximated efficiently via sampling from univari-ate gaussian distributions and -lrb- ii -rrb- the gradients wrt the gp hyperparameters can be obtained analytically regardless of the model likelihood . we further propose two instances of the variational distribution whose covariance matrices can be parametrized linearly in the number of observations . these results allow gradient-based optimization to be done efficiently in a black-box manner . our automated variational method is thoroughly verified on five models using six benchmark datasets , performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative mcmc sampling approaches . our automated variational method can be a valuable automated variational method for practitioners and researchers to investigate new models with minimal effort in deriving model-specific inference algorithms .
we investigate methods for unsupervised learning of sub-word acoustic units of a language directly from speech . we demonstrate that states of a hidden markov model '' grown '' using a novel modification of the maximum likelihood successive state splitting algorithm correspond very well with the phones of the language . in particular , the correspondence between the viterbi state sequence for unseen speech from the training speaker and the phone transcription of the speech is over 85 % , and generalizes to a large extent -lrb- ∼ 63 % -rrb- to speech from a different speaker . furthermore , we are able to bridge more than half the gap between the speaker-dependent and cross-speaker correspondence of the automatically learned units to phones -lrb- ∼ 75 % accuracy -rrb- by unsupervised adaptation via mllr .
modeling the distribution of natural images is a landmark problem in unsupervised learning . this task requires an image model that is at once expressive , tractable and scalable . we present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions . our deep neural network models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image . architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks . we achieve log-likelihood scores on natural images that are considerably better than the previous state of the art . our main results also provide benchmarks on the diverse imagenet dataset . samples generated from the deep neural network appear crisp , varied and globally coherent .
in this paper , we explore spatial vs. multiuser diversity tradeoffs in a cellular system with selective feedback . we first derive closed-form expressions of the average system capacity for both siso and stbc transmission schemes in order to analytically assess the impact of the number of terminals and bandwidth restrictions in the feedback channel . next , we analyze several design trade-offs in terms of increased average -lrb- long term -rrb- system capacity vs. robustness to short-term snr fluctuations for both transmission schemes under consideration .
gsm amr speech codec being used for both internet and mobile networks , robustness to both frame erasures and random bit errors assumes significance . this paper proposes a new context-based error recovery technique for the celp-based speech codec accomplishing recovery of erased frames , updating decoder state during erasure spells and reliable estimation of codec parameters in case of bit errors . previous error concealment techniques do not adequately make use of the context in which concealment is being done . the proposed context-based error recovery technique is intended to retrieve and use contextual information for better performance . the context-based error recovery technique is solely receiver based , uses no look ahead , makes use of implicitly available codec parameters and buffers for parameter estimation and is hence computationally efficient . segmental itakura-saito measure and mos scores are used to compare the output speech quality of the proposed technique with those of the basic techniques as recommended by the standard .
in this paper , we deal with antenna selection at the receiver side for space-time coded systems over frequency-selective fading channels . we reveal that introducing antenna selection based on the signal-to-noise-ratio observed can still achieve the full diversity available , if the underlying space-time code is full-rank -lrb- i.e. , if space-time code achieves full diversity without antenna selection over the frequency-selective fading channel -rrb- . we also argue that if the space-time code is not full-rank , antenna selection results in a loss in the diversity of the system .
we present new training methods that aim to mitigate local optima and slow convergence in unsupervised training by using additional imperfect objectives . in its simplest form , lateen em alternates between the two objectives of ordinary '' soft '' and '' hard '' expectation max-imization algorithms . switching objectives when stuck can help escape local optima . we find that applying a single such alternation already yields state-of-the-art results for en-glish dependency grammar induction . more elaborate training methods track both objectives , with each validating the moves proposed by the other . disagreements can signal earlier opportunities to switch or terminate , saving iterations . de-emphasizing fixed points in these ways eliminates some guesswork from tuning em . an evaluation against a suite of unsu-pervised dependency parsing tasks , for a variety of languages , showed that training methods significantly speed up training of both expectation max-imization algorithms , and improve accuracy for local optima .
we present a fast parameter sensitivity analysis by combining recent developments from uncertainty quantification with image processing operators . the fast parameter sensitivity analysis is not based on a sampling strategy , instead we combine the polynomial chaos expansion and stochastic finite elements with pde-based image processing operators . with our fast parameter sensitivity analysis and a moderate number of parameters in the models the full sensitivity analysis is obtained at the cost of a few monte carlo runs . to demonstrate the efficiency and simplicity of the fast parameter sensitivity analysis we show a parameter sensitivity analysis for perona-malik diffusion , random walker and ambrosio-tortorelli segmentation , and discontinuity-preserving optical flow computation .
we revisit recently proposed algorithms for probabilistic clustering with pair-wise constraints between data points . we evaluate and compare existing techniques in terms of robustness to misspecified constraints . we show that the technique that strictly enforces the given constraints , namely the chunklet model , produces poor results even under a small number of misspecified constraints . we further show that methods that penalize constraint violation are more robust to misspecified constraints but have undesirable local behaviors . based on this evaluation , we propose a new learning technique , extending the chunklet model to allow soft constraints represented by an intuitive measure of confidence in the constraint .
this paper presents an energy normalization transform as a method to reduce system errors in the lf-asd brain-computer interface . the energy normalization transform has two major benefits to the system performance . first , it can increase class separation between the active and idle eeg data . second , it can desensitize the system to the signal amplitude variability . for four subjects in the study , the benefits resulted in the performance improvement of the lf-asd in the range from 7.7 % to 18.9 % , while for the fifth subject , who had the highest non-normalized accuracy of 90.5 % , the performance did not change notably with normalization .
in this paper , we present a multi-domain spoken dialogue system equipped with the capability of parallel computation of speech-recognition engines that are assigned to each domain . the experimental multi-domain spoken dialogue system is set up to handle three different domains -lrb- restaurant information , weather report , and news query -rrb- in an in-car usage . all of these tasks are of information retrieval nature . the domain of a particular utterance is determined based on the likelihood of each speech recognizer . in addition to the human-machine interaction , synthesized voice of the route subsystem interrupts the dialogue frequently . experimental evaluation has yielded 95 percent recognition accuracy in selecting the task domain based on a specially designed scoring method .
discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning . indeed , finite mixtures and infinite mixtures , relying on dirichlet processes and modifications , have become a standard tool . one important issue that arises in using discrete mixtures is low separation in the components ; in particular , different components can be introduced that are very similar and hence redundant . such redundancy leads to too many clusters that are too similar , degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings . redundancy can arise in the absence of a penalty on components placed close together even when a bayesian approach is used to learn the number of components . to solve this problem , we propose a novel prior that generates components from a repulsive process , automatically penalizing redundant components . we characterize this repulsive prior theoretically and propose a markov chain monte carlo sampling algorithm for posterior computation . the methods are illustrated using synthetic examples and an iris data set .
we present the problem of estimating cortical connectivity between different regions of the cortex from scalp electroen-cephalographic -lrb- eeg -rrb- or magnetoencephalographic data as system identification of a nonlinear state-space model . the state equation is based on a nonlinear multivariate au-toregressive model with radial basis function kernels . the radial basis function kernels capture the nonlinear dynamics of the cortical signals and provide a framework for measuring interactions between cortical regions of interest -lrb- rois -rrb- based on the definition of granger causality . the observation equation relates the cortical signals associated with each roi to the observed e/meg data using a set of parsimonious spatial bases to represent spatially extended cortical sources . an expectation-maximization algorithm is derived to obtain maximum likelihood estimates of the nonlinear state-space model parameters directly from the observed data . we show that this integrated approach for measuring cortical connectivity performs significantly better than the conventional decoupled approach in which cortical signals are first estimated by solving the inverse problem followed by fitting a mvar model .
this paper examines a downlink transmit beamforming scheme recently proposed in -lsb- 1 -rsb- . the idea is based on the use of an adaptive channel estimator at the base-station and requires the mobile to selectively feed back the value of the prediction error . this paper proposes a joint maximum-likelihood and set-membership filtering algorithm for adaptive channel estimation that provides robustness against incorrect feedback due to mobile decision errors . the amount of power and bandwidth-saving possible with this joint maximum-likelihood and set-membership filtering algorithm is quantified via an uplink capacity analysis .
reduced rank regression has found application in various fields of signal processing . in this paper we propose a novel extension of the rank regression which we call sparse variable reduced rank regression . by using a vector l1 penalty we remove variables completely from the rank regression . the proposed estimation algorithm involves optimization on the stiefel manifold and we illustrate estimation algorithm both on a simulated and a real functional magnetic resonance imaging -lrb- fmri -rrb- data set .
the steered response power with phat transform -lrb- srp-phat -rrb- or global coherence field , has become a standard method for acoustic source localization , thanks to their simplicity , computational inexpensiveness and robustness against mid-high reverberation . however , originally formulated for the single source localiza-tion case , steered response power does not apply satisfactorily to the multiple source case . in this paper , we analyze the structure of the spatial function and reshape steered response power according to a generic multidimensional metric . we show that traditional functions are based on the l1 norm which is prone to generate ambiguous locations with high likelihood -lrb- i.e. ghosts -rrb- . a more generic multidimensional kernel based on higher norms and on a partitioned representation of the cross-power spectrum is introduced , which better exploits the source sparseness in the discrete time-frequency domain . evaluation results over simulated data show that the new spatial function considerably improve the detection of multiple competing sources in both spatial and multidimensional tdoa domains .
we consider the problem of learning the structure of undirected graphical models with bounded treewidth , within the maximum likelihood framework . this is an np-hard problem and most approaches consider local search techniques . in this paper , we pose np-hard problem as a combi-natorial optimization problem , which is then relaxed to a convex optimization problem that involves searching over the forest and hyperforest polytopes with special structures , independently . a supergradient method is used to solve the dual problem , with a run-time complexity of o -lrb- k 3 n k +2 log n -rrb- for each iteration , where n is the number of variables and k is a bound on the treewidth . we compare our supergradient method to state-of-the-art methods on synthetic datasets and classical benchmarks , showing the gains of the novel convex approach .
an mcmc -lrb- markov chain monte carlo -rrb- algorithm is proposed for nonlinear time series prediction with hierarchical bayesian framework . the mcmc -lrb- markov chain monte carlo -rrb- algorithm computes predictive mean and error bar by drawing samples from predictive distributions . the mcmc -lrb- markov chain monte carlo -rrb- algorithm is tested against time series generated by -lrb- chaotic -rrb- rössler system and mcmc -lrb- markov chain monte carlo -rrb- algorithm outperforms quadratic approximations previously proposed by the authors .
rosetta is an esa cornerstone mission that will reach the comet 67p/churyumov-gerasimenko in august 2014 and will escort the comet for a 1.5 year nominal mission offering the most detailed study of a comet ever undertaken by humankind . the rosetta orbiter has 11 scientific instruments -lrb- 4 remote sensing -rrb- and the philae lander to make complementary measurements of the comet nucleus , coma -lrb- gas and dust -rrb- , and surrounding environment . the esa rosetta science ground segment has developed a science planning and scheduling system that includes an automated scheduling capability to assist in developing science plans for the rosetta orbiter . while automated scheduling capability is a small portion of the overall science ground segment as well as the overall scheduling system , this paper focuses on the automated and semi-automated scheduling software -lrb- called aspen-rssc -rrb- and how this software is used . specifically , the rosetta orbiter uses an incremental planning process of successive refinement of the science mission plan beginning with skeleton planning , long term planning , medium term planning , and short term planning . these phases represent the evolution of the science mission plan from one year before execution running through just before execution . we also report on aspen-rssc experience and usage during the pre-landing operations phase thus far .
the difficulty of building an effective digital rights management system stems from the fact that traditional cryptographic primitives such as encryption or scrambling do not protect audio or video signals once they are played in plain-text . this fact , commonly referred to as '' the analog hole , '' has been responsible for the popularity of multimedia file sharing which can not be controlled , at least technically , by content 's copyright owners . in this paper , we explore a specific issue in multimedia fingerprinting as an answer to '' the analog hole '' problem . we analyze the collusion resistance of three large classes of spread-spectrum fingerprints using a recently introduced collusion procedure , the gradient attack . surprisingly , we show that the collusion resistance of direct-sequence and uniformly distributed spread spectrum fingerprints is a small constant that does not depend on the object size , whereas bounded gaussian fingerprints demonstrate significantly better robustness to the gradient attack .
the ſxed-complexity sphere decoder has been previously proposed for multiple input-multiple output -lrb- mimo -rrb- detection to overcome the two main drawbacks of the original sphere decoder , namely its variable complexity and sequential structure . as such , the ſxed-complexity sphere decoder is highly suitable for hardware implementation and has shown remarkable performance through simulations . herein , we explore the theoretical aspects of the algorithm and prove that the ſxed-complexity sphere decoder achieves the same diversity order as the maximum likelihood detector . further , we show that the coding loss can be made negligible in the high signal to noise ratio regime with a signiſcantly lower complexity than that of the mld .
we introduce a sparse covariance estimation method for the high dimensional setting when the covariance matrix decomposes as a kronecker product , i.e. , σ 0 = a 0 ⊗ b 0 , and the observations are gaussian . we propose an 1 penalized maximum-likelihood approach to solve this problem . the sparse covariance estimation method motivates an iterative algorithm -lrb- penalized flip-flop ; ffp -rrb- based on a block coordinate-descent approach . although the 1-penalized log-likelihood function -lrb- objective function -rrb- is non-convex in general and non-smooth , we show that ffp converges to a local maximum under relatively mild assumptions . for the fixed dimension case , large-sample statistical consistency is proved and a rate of convergence bound is derived . simulations show that ffp outperforms its non-penalized counterpart and the naive glasso algorithm for sparse kronecker-decomposable covariance matrix .
one-shot learning is usually tackled by using generative models or discriminative embeddings . discriminative methods based on deep learning , which are very effective in other learning scenarios , are ill-suited for one-shot learning as discriminative methods need large amounts of training data . in this paper , we propose a method to learn the parameters of a deep model in one shot . we construct the learner as a second deep network , called a learnet , which predicts the parameters of a pupil network from a single exemplar . in this manner we obtain an efficient feed-forward one-shot learner , trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation . in order to make the construction feasible , we propose a number of factorizations of the parameters of the pupil network . we demonstrate encouraging results by learning characters from single exemplars in omniglot , and by tracking visual objects from a single initial exemplar in the visual object tracking benchmark .
most of the web-based methods for lexicon augmenting consist in capturing global semantic features of the targeted domain in order to collect relevant documents from the web . we suggest that the local context of the out-of-vocabulary words contains relevant information on the oov words . with this information , we propose to use the web to build locally-augmented lexicons which are used in a final local decoding pass . we first demonstrate the relevance of the web for the oov word retrieval . then , different methods are proposed to retrieve the hypothesis words . finally we present the integration of new words in the transcription process based on part-of-speech models . this technique allows to recover 7.7 % of the significant oov words and the accuracy of the system is slightly improved .
query processing of owl-dl ontologies is intractable in the worst case , but we present a novel technique that in practice allows for efficient querying of ontologies with large aboxes in secondary storage . we focus on the processing of instance retrieval queries , i.e. , queries that retrieve individuals in the abox which are instances of a given concept c . our technique uses summarization and refinement to reduce instance retrieval to a small relevant subset of the original abox . we demonstrate the effectiveness of this technique in aboxes with up to 7 million assertions . our results are applicable to the very expressive description logic shin , which corresponds to owl-dl minus nominals and datatypes .
singleton-based consistencies have been shown to dramatically improve the performance of constraint solvers on some difficult instances . however , they are in general too expensive to be applied exhaustively during the whole search . in this paper , we focus on partition-one-ac , a singleton-based consistency which , as opposed to singleton arc consistency , is able to prune values on all variables when partition-one-ac performs singleton tests on one of them . we propose adaptive variants of partition-one-ac that do not necessarily run until having proved the fixpoint . the pruning can be weaker than the full version but the computational effort can be significantly reduced . our experiments show that adaptive partition-one-ac can obtain significant speed-ups over arc consistency and over the full version of partition-one-ac .
the synthesis-based sparse representation model for signals has drawn a considerable interest in the past decade . such a synthesis-based sparse representation model assumes that the signal of interest can be decomposed as a linear combination of a few atoms from a given dictionary . in this paper we concentrate on an alternative , analysis-based model , where an analysis dictionary multiplies the signal , leading to a sparse outcome . our goal is to learn the analysis dictionary from a set of signal examples , and the analysis-based model taken is parallel and similar to the one adopted by the k-svd algorithm that serves the corresponding problem in the synthesis model . we present the development of the analysis-based model , which include two greedy tailored pursuit algorithms and a penalty function for the dictionary update stage . we demonstrate its effectiveness in several experiments , showing a successful and meaningful recovery of the analysis dictionary .
the objective of single-channel inverse filtering is to design an inverse filter that achieves dereverberation while being robust to an inaccurate room impulse response -lrb- rir -rrb- measurement or estimate . since a stable and causal inverse filter typically does not exist , approximate time-domain inverse filtering techniques such as single-channel least-squares have been proposed . however , besides being computationally expensive and often infeasible , single-channel least-squares generally leads to distortions in the output signal in the presence of rir inaccuracies . in this paper , a theoretical analysis is initially provided , showing that the direct inversion of the acoustic transfer function in the frequency-domain generally yields instability and acausality issues . in order to resolve these issues , a novel frequency-domain inverse filtering technique is proposed that incorporates regularization and uses a single-channel speech enhancement scheme . experimental results demonstrate that the proposed frequency-domain inverse filtering technique yields a higher dere-verberation performance and has a significantly lower computational complexity compared to the scls technique .
search engines are increasingly relying on large knowledge bases of facts to provide direct answers to users ' queries . however , the construction of these knowledge bases is largely manual and does not scale to the long and heavy tail of facts . open information extraction tries to address this challenge , but typically assumes that facts are expressed with verb phrases , and therefore has had difficulty extracting facts for noun-based relations . we describe renoun , an open information extraction system that complements previous efforts by focusing on nominal attributes and on the long tail . renoun 's approach is based on leveraging a large on-tology of noun attributes mined from a text corpus and from user queries . renoun creates a seed set of training data by using specialized patterns and requiring that the facts mention an attribute in the ontol-ogy . renoun then generalizes from this seed set to produce a much larger set of extractions that are then scored . we describe experiments that show that we extract facts with high precision and for attributes that can not be extracted with verb-based techniques .
swiftfile is an intelligent assistant that helps users organize their e-mail into folders . swiftfile uses a text classifier to predict where each new message is likely to be filed by the user and provides shortcut buttons to quickly file messages into one of its predicted folders . one of the challenges faced by swiftfile is that the user 's mail-filing habits are constantly changing -- users are frequently creating , deleting and rearranging folders to meet their current filing needs . in this paper , we discuss the importance of incremen-tal learning in swiftfile . we present several criteria for judging how well incremental learning algorithms adapt to quickly changing data and evaluate swiftfile 's classifier using these criteria . we find that swiftfile 's classifier is surprisingly responsive and does not require the extensive training that is often assumed in most learning systems .
this paper presents an improvement to model learning when using multi-class logitboost for classification . motivated by the statistical view , logitboost can be seen as additive tree regression . two important factors in this setting are : 1 -rrb- coupled classifier output due to a sum-to-zero constraint , and 2 -rrb- the dense hessian matrices that arise when computing tree node split gain and node value fittings . in general , this setting is too complicated for a tractable model learning algorithm . however , too aggressive simplification of the setting may lead to degraded performance . for example , the original logitboost is outper-formed by logitboost due to the lat-ter 's more careful treatment of the above two factors . in this paper we propose techniques to address the two main difficulties of the log-itboost setting : 1 -rrb- we adopt a vector tree -lrb- i.e. , each node value is vector -rrb- that enforces a sum-to-zero constraint , and 2 -rrb- we use an adaptive block coordinate descent that exploits the dense hessian when computing tree split gain and node values . higher classification accuracy and faster convergence rates are observed for a range of public data sets when compared to both the original and the abc-logitboost implementations .
we present a robust framework for estimating non-rigid 3d shape and motion in video sequences . given an input video sequence , and a user-specified region to reconstruct , the algorithm automatically solves for the 3d time-varying shape and motion of the object , and estimates which pixels are outliers , while learning all system parameters , including a pdf over non-rigid deformations . there are no user-tuned parameters -lrb- other than initialization -rrb- ; all parameters are learned by maximizing the likelihood of the entire image stream . we apply our method to both rigid and non-rigid shape reconstruction , and demonstrate it in challenging cases of occlusion and variable illumination .
we address vehicle classification and mensuration problems using acoustic and video sensors . in this paper , we show how to estimate a vehicle 's speed , width , and length by jointly estimating its acoustic wave-pattern using a single passive acoustic sensor that records the vehicle 's drive-by noise . the acoustic wave-pattern is approximated using three envelope shape components , which approximate the shape of the received signal 's power envelope . we incorporate the parameters of the envelope shape components along with estimates of the vehicle engine rpm and number of cylinders to create a vehicle profile vector that forms an intuitive discriminatory feature space . in the companion paper , we discuss vehicle classification and mensuration based on silhouette extraction and wheel detection , using a video sensor . vehicle speed estimation and classification results are provided using field data .
in exemplar-based speech enhancement systems , lower dimensional features are preferred over the full-scale dft features for their reduced computational complexity and the ability to better generalize for the unseen cases . but in order to obtain the wiener-like filter for noisy dft enhancement , the speech and noise estimates obtained in the feature space need to be mapped to the dft space , which yield a low-rank approximation of the estimates resulting in a sub-optimal filter . this paper proposes a novel method using coupled dictionaries where the exemplars for the required feature space and the dft space are jointly extracted and the estimates are directly obtained in the dft space following the decomposition in the chosen feature space . simulation experiments revealed that the proposed approach , where the activations of exemplars calculated using the mel resolution are directly used to obtain the wiener filter in the dft space , results in improved signal-to-distortion ratio when compared to the system without coupled dictionaries . to further motivate the use of coupled dictionaries , the paper also investigates the use of modulation envelope features for the exemplar-based speech enhancement .
for two-class classification , it is common to classify by setting a threshold on class probability estimates , where the threshold is determined by roc curve analysis . an analog for two-class classification is learning a new class partitioning of the multiclass probability simplex to minimize empirical misclassification costs . we analyze the interplay between systematic errors in the class probability estimates and cost matrices for multiclass classification . we explore the effect on the class partitioning of five different transformations of the cost matrix . experiments on benchmark datasets with naive bayes and quadratic discriminant analysis show the effectiveness of learning a new partition matrix compared to previously proposed methods .
this paper proposes an efficient approximation of the forward-backward algorithm , for the purpose of estimating missing features , based on downsampling statistical models . the paper discusses the role of hidden markov models in the estimation process , and presents an approximation to the forward-backward algorithm by developing hmms based on lower resolution quantizers , which are obtained through a tree-structure mapping of quantizer centroids . to illustrate the effectiveness of the proposed forward-backward algorithm , we apply forward-backward algorithm to the problem of error concealment in remote speech recognition , using the aurora-2 database . the fb approximation provides comparable word recognition accuracy results relative to the standard forward-backward algorithm , while reducing the computational load by a large factor -lrb- > 250 in this case -rrb- .
the gaussian process is a popular way to specify dependencies between random variables in a probabilistic model . in the bayesian framework the covariance structure can be specified using unknown hyperparameters . integrating over these unknown hyperparameters considers different possible explanations for the data when making predictions . this integration is often performed using markov chain monte carlo sampling . however , with non-gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly . in this paper we present a slice sampling approach that requires little tuning while mixing well in both strong-and weak-data regimes .
this paper addresses a practical problem associated with multimedia display systems that utilize motion platforms to create for users both haptic and vestibular sensations associated with their movement through a displayed environment . given audiovisual content for which motion data is not available , the motion data that is required for motion platform control can be generated automatically from multichannel audio data such as that distributed on dvds presenting popular movie titles . this paper presents initial results of a study designed to test the effectiveness of a subspace projection of multichannel audio data for automatic control of motion-platform-based multimedia display .
random indexing is a vector space technique that provides an efficient and scal-able approximation to distributional similarity problems . we present experiments showing random indexing to be poor at handling large volumes of data and evaluate the use of weighting functions for improving the performance of random indexing . we find that random indexing is robust for small data sets , but performance degrades because of the influence of high frequency attributes in large data sets . the use of appropriate weight functions improves this significantly .
we develop a new exponential family probabilistic model for permutations that can capture hierarchical structure and that has the mallows and generalized mal-lows models as subclasses . we describe how to do parameter estimation and propose an approach to structure search for this class of models . we provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations .
typical speech enhancement algorithms that operate in the fourier domain only modify the magnitude component . it is commonly understood that the phase component is perceptually unimportant , and thus , phase component is passed directly to the output . in recent intelligibility experiments , phase component has been reported that the short-time fourier transform phase spectrum can provide significant intelligibility when estimated using a window function lower in dynamic range than the typical ham-ming window . motivated by this , we investigate the role of the window function for short-time fourier transform phase spectrum in relation to speech enhancement . using a modified stft analysis-modification-synthesis framework , we show that noise reduction can be achieved by modifying the window function used to estimate the stft phase spectra . we demonstrate this through spec-trogram plots and results from two objective speech quality measures .
this paper describes a new method for building compact context-dependency transducers for finite-state transducer-based asr decoders . instead of the conventional phonetic decision tree growing followed by fst compilation , this approach incorporates the phonetic context splitting directly into the transducer construction . the objective function of the compact context-dependency transducers is augmented with a regularization term that measures the number of transducer states introduced by a split . we give results on a large spoken-query task for various n-phone orders and other phonetic features that show this method can greatly reduce the size of the resulting context-dependency transducer with no significant impact on recognition accuracy . this permits using context sizes and features that might otherwise be unmanageable .
this paper presents plis , an open source probabilistic lexical inference system which combines two functionalities : -lrb- i -rrb- a tool for integrating lexical inference knowledge from diverse resources , and -lrb- ii -rrb- a framework for scoring textual inferences based on the integrated knowledge . we provide plis with two probabilistic implementation of this framework . plis is available for download and developers of text processing applications can use plis as an off-the-shelf component for injecting lexical knowledge into their applications . plis is easily configurable , components can be extended or replaced with user generated ones to enable system customiza-tion and further research . plis includes an online interactive viewer , which is a powerful tool for investigating lexical inference processes .
we present the first provably optimal polynomial time dynamic programming algorithm for best-first shift-reduce parsing , which applies the polynomial time dynamic programming algorithm idea of huang and sagae -lrb- 2010 -rrb- to the best-first parser of sagae and lavie -lrb- 2006 -rrb- in a non-trivial way , reducing the complexity of the latter from exponential to polynomial . we prove the correctness of our polynomial time dynamic programming algorithm rigorously . experiments confirm that polynomial time dynamic programming algorithm leads to a significant speedup on a probablistic best-first shift-reduce parser , and makes exact search under such a model tractable for the first time .
a technique is introduced for extracting and reconstructing a wide class of building types from a registered range image and optical image . an attentional focus stage , followed by model indexing , allows top-down robust surface ttting to reconstruct the 3d nature of the buildings in the data . because of the effectiveness of model indexing , top-down processing of noisy range data still succeeds and the algorithm is capable of detecting and reconstructing several diierent building roof classes , including at single level , at multi-leveled , peaked , and curved rooftops . the algorithm is applicable to range data that may have been collected from several diierent range sensor types . we demonstrate reconstructions of diier-ent buildings classes in the presence of large amounts of noise . our results underline the usefuless of range data when processed in the context of a focus-of-attention area derived from the monocular optical image .
this paper addresses the problem of automatic evaluation of text simplification systems for spanish . we test whether already-existing readability for-mulae would be suitable for this task . we adapt three existing readability indices -lrb- two measuring lexical complexity and one measuring syntactic complexity -rrb- to be computed automatically , which are then applied to a corpus of original news texts and their manual simplifications aimed at people with cognitive disabilities . we show that there is a significant correlation between each of the three readability indices and several linguistically motivated features which might be seen as reading obstacles for various target populations . furthermore , we show that there is a significant correlation between the two read-ability indices which measure lexical complexity .
accurate dense 3d reconstruction of dynamic scenes from natural images is still very challenging . most previous methods rely on a large number of fixed cameras to obtain good results . some of these methods further require separation of static and dynamic points , which are usually restricted to scenes with known background . we propose a novel dense depth estimation method which can automatically recover accurate and consistent depth maps from the synchronized video sequences taken by a few handheld cameras . unlike fixed camera arrays , our data capturing setup is much more flexible and easier to use . our dense depth estimation method simultaneously solves bilayer segmentation and depth estimation in a unified energy minimization framework , which combines different spatio-temporal constraints for effective depth optimization and segmentation of static and dynamic points . a variety of examples demonstrate the effectiveness of the proposed dense depth estimation method .
in natural speech , there is a moderate correlation between the fundamental frequency and formant frequencies across talkers . the present study used a high-quality vocoder to manipulate these properties and determine their contribution to perceived naturalness and voice gender . the stimuli were re-synthesized sentences spoken by two adult males and two adult females . scale factors were chosen for each sentence and for each talker to produce frequency-shifted versions with a specified mean fundamental frequency -lrb- f 0 -rrb- ranging from 60 hz to 450 hz in 10 steps , paired with 10 steps in geometric mean formant frequencies ranging from 850 hz to 2500 hz . listeners judged frequency-shifted sentences as more natural when f 0 and formant frequencies followed the co-variation of f 0 and formant frequencies in natural voices . sentences with low f 0 s and low formant frequencies were perceived as masculine , while sentences with high f 0 and high formant frequencies were assigned high ratings of femininity . sentences with '' mismatched '' f 0 and formant frequencies were assigned ratings near the midpoint of the range , indicating gender ambiguity . frequency-shifted sentences derived from male talkers received consistently higher ratings of masculinity than those derived from females , while sentences from female talkers received higher ratings of femininity , even when assigned scale factors appropriate for the opposite gender , indicating that factors other than f 0 and mean formant frequencies contribute to perceived gender .
compact explicit feature compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning , but deriving such compact explicit feature maps for many types of kernels remains a challenging open problem . among the commonly used kernels for non-linear classification are polynomial kernels , for which low approximation error has thus far necessitated explicit feature compact explicit feature maps of large dimensionality , especially for higher-order polynomials . meanwhile , because polynomial kernels are unbounded , polynomial kernels are frequently applied to data that has been normalized to unit 2 norm . the question we address in this work is : if we know a priori that data is normalized , can we devise a more compact map ? we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting , and introduce a new approximation paradigm , spherical random fourier features , which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere . compared to prior work , spherical random fourier features are less rank-deficient , more compact , and achieve better kernel approximation , especially for higher-order polynomials . the resulting predictions have lower variance and typically yield better classification accuracy .
producing attractive trailers for videos needs human expertise and creativity , and hence is challenging and costly . different from video summarization that focuses on capturing storylines or important scenes , trailer generation aims at producing trailers that are attractive so that viewers will be eager to watch the original video . in this work , we study the problem of automatic trailer generation , in which an attractive trailer is produced given a video and a piece of music . we propose a sur-rogate measure of video attractiveness named fix-ation variance , and learn a novel self-correcting point process-based attractiveness model that can effectively describe the dynamics of attractiveness of a video . furthermore , based on the attractiveness model learned from existing training trailers , we propose an efficient graph-based trailer generation algorithm to produce a max-attractiveness trailer . experiments demonstrate that our graph-based trailer generation algorithm outper-forms the state-of-the-art trailer generators in terms of both quality and efficiency .
in this paper , we improve microblog users ' demographic prediction via the videos they talk about . first , we collect the describing words of currently popular videos , including video names , actor names and video keywords , from video websites . next , we search these describing words in users ' microblogs , and build the direct relationships between users and the appeared words . after that , we propose a bayesian method to calculate the probability of connections between users and video describing words . if the probability is beyond a threshold , an indirect relationship is founded . last , two models are built to predict users ' demographics with the obtained direct and indirect relationships . based on a large real-world dataset , experiment results show that our bayesian method can significantly improve these words ' demographic predictive ability by more than 15 % on average .
in this paper , a method of domain adaptation for clustered language models is developed . it is based on a previously developed clustering algorithm , but with a modied optimi-sation criterion . the results are shown to be slightly superior to the previously published ` fillup ' method , which can be used to adapt standard n-gram models . however , the improvement both methods give compared to models built from scratch on the adaptation data is quite small -lrb- less than 11 % relative improvement i n w ord error rate -rrb- . this suggests that both methods are still unsatisfactory from a practical point of view .
the popular bag-of-features representation for object recognition collects signatures of local image patches and discards spatial information . some have recently attempted to at least partially overcome this limitation , for instance by '' spatial pyramids '' and '' proximity '' kernels . we introduce the general formalism of '' relaxed matching kernels '' -lrb- rmks -rrb- that includes such approaches as special cases , allow us to derive useful general properties of these kernels , and to introduce new ones . as an example , we introduce a kernel based on matching graphs of features and one based on matching information-compressed features . we show that all rmks are competitive and outperform in several cases recently published state-of-the-art results on standard datasets . however , we also show that a proper implementation of a baseline bag-of-features algorithm can be extremely competitive , and outperform the other methods in some cases .
we introduce the generalized semi-markov decision process as an extension of continuous-time mdps and semi-markov decision processes -lrb- generalized semi-markov decision process -rrb- for modeling stochastic decision processes with asynchronous events and actions . using phase-type distributions and uniformization , we show how an arbitrary gsmdp can be approximated by a discrete-time mdp , which can then be solved using existing mdp techniques . the techniques we present can also be seen as an alternative approach for solving generalized semi-markov decision process , and we demonstrate that the introduction of phases allows us to generate higher quality policies than those obtained by standard smdp solution techniques .
this paper introduces a tuning algorithm of self-quotient ε-filter and support vector machine , and its application to noise robust human detection combining self-quotient ε-filter , histograms of oriented gradients , and self-quotient ε-filter . although human detection combining histograms of oriented gradients and self-quotient ε-filter is a powerful approach , as it uses local intensity gradients , it is difficult to handle noise corrupted images . on the other hand , although human detection combining self-quotient ε-filter , histograms of oriented gradients and self-quotient ε-filter can realize noise robust human detection , self-quotient ε-filter requires manual parameter setting . our aim is not only to set the parameter of self-quotient ε-filter but also to train self-quotient ε-filter by using numerous images without noise and a small amount of images with noise .
we present a novel dynamic bayesian network for pedestrian path prediction in the intelligent vehicle domain . the dynamic bayesian network incorporates the pedestrian situational awareness , situation criticality and spatial layout of the environment as latent states on top of a switching linear dynamical system to anticipate changes in the pedestrian dynamics . using computer vision , situational awareness is assessed by the pedestrian head orientation , situation criticality by the distance between vehicle and pedestrian at the expected point of closest dynamic bayesian network , and spatial layout by the distance of the pedestrian to the curbside . our particular scenario is that of a crossing pedestrian , who might stop or continue walking at the curb . in experiments using stereo vision data obtained from a vehicle , we demonstrate that the proposed dynamic bayesian network results in more accurate path prediction than only slds , at the relevant short time horizon -lrb- 1 s -rrb- , and slightly outperforms a computationally more demanding state-of-the-art method .
we characterize a class of videos consisting of very small but potentially complicated motions . we find that in these scenes , linear appearance variations have a direct relationship to scene motions . we show how to interpret appearance variations captured through a pca decomposition of the image set as a scene-specific non-parametric motion basis . we propose fast , robust tools for dense flow estimates that are effective in scenes with small motions and potentially large image noise . we show example results in a variety of applications , including motion segmentation and long-term point tracking .
two central issues in stereo algorithm design are the matching criterion and the underlying smoothness assumptions . in this paper we propose a new stereo algorithm with novel approaches to both issues . we start with a careful analysis of the properties of the continuous disparity space image , and derive a new matching cost based on the reconstructed image signals . we then use a symmetric matching process that employs visibility constraints to assign disparities to a large fraction of pixels with minimal smoothness assumptions . while the matching operates on integer disparities , sub-pixel information is maintained throughout the process . global smoothness assumptions are delayed until a later stage in which disparities are assigned in textureless and occluded areas . we validate our stereo algorithm with experimental results on stereo images with ground truth .
the asynchronous hidden markov model can asynchronous hidden markov model the joint likelihood of two observation sequences , even if the streams are not synchronised . previously this asynchronous hidden markov model has been applied to audiovisual recognition tasks . the main drawback of the concept is its rather high training and decoding complexity . in this work we show how the complexity can be reduced significantly with advanced running indices for the calculations . yet , the ahmm characteristics and its advantages are preserved . the improvement also allows a scaling procedure to keep numerical values in a reasonable range . in an experimental section we compare the complexity of the original and the improved concept and validate the theoretical results . then the asynchronous hidden markov model is tested on a bimodal speech and gesture user input fusion task : compared to a late fusion hmm an improvement of more than 10 % absolute recognition performance has been achieved .
this paper describes the work done in the framework of the vidivideo european project in terms of audio event detection . our first experiments concerned the detection of non-voice sounds , such as birds , machines , traffic , water and steps . given the unavailability of a corpus labelled in terms of audio events , we used a relatively small sound effect corpus for training . our initial experiments with one-against-all svm classi-fiers for these 5 classes showed us the feasibility of using this type of data for training , thus avoiding the extremely morose task of manual labelling of a very high number of audio events . preliminary integration experiments are quite promising .
sound source tracking is an important function for a robot operating in a daily environment , because the robot should recognize where a sound event such as speech , music and other environmental sounds originates from . this paper addresses sound source tracking by integrating a room and a robot microphone array . the room microphone array consists of 64 microphones attached to the walls . it provides 2d sound source localization based on a weighted delay-and-sum beamforming method . the robot microphone array consists of eight microphones installed on a robot head , and localizes multiple sound sources in azimuth . the lo-calization results are integrated to track sound sources by using a particle filter for multiple sound sources . the experimental results show that particle filter based integration reduces localization errors and provides accurate and robust 2d sound source tracking .
recently the general game description language has been extended so as to cover arbitrary games with in-complete/imperfect information . learning -- without human intervention -- to play such games poses a reasoning challenge for general game-playing systems that is much more intricate than in case of complete information games . action formalisms like the situation calculus have been developed for precisely this purpose . in this paper we present a full embedding of the game description language into the situation calculus -lrb- with scherl and levesque 's knowledge fluent -rrb- . we formally prove that this provides a sound and complete reasoning method for players ' knowledge about game states as well as about the knowledge of the other players .
in this paper , we investigate imposture using synthetic speech . although this problem was first examined over a decade ago , dramatic improvements in both speaker verification and speech synthesis have renewed interest in this problem . we use a hmm-based speech synthesizer which creates synthetic speech for a targeted speaker through adaptation of a background model . we use two sv systems : standard gmm-ubm-based and a newer svm-based . our results show when the systems are tested with human speech , there are zero false acceptances and zero false rejections . however , when the systems are tested with synthesized speech , all claims for the targeted speaker are accepted while all other claims are rejected . we propose a two-step process for detection of synthesized speech in order to prevent this imposture . overall , while sv systems have impressive accuracy , even with the proposed detector , high-quality synthetic speech will lead to an unacceptably high false acceptance rate .
speaker recognition techniques have traditionally relied on purely acoustic features and models . during the last few years , however , the field of speaker recognition has started to show interest in the use of higher level features . in particular , phonetic decodings modeled with statistical language models -lrb- n-grams -rrb- have already shown its effectiveness in several research works . however , the relationship between phonetic modeling precision and the accuracy of phonetic speaker recognition has not yet been sufficiently analyzed . as part of our preparation for the nist 2005 speaker recognition evaluation , we have performed a number of experiments that show that there is a negligible correlation between phonetic modeling precision and phonetic speaker recognition accuracy . furthermore , our experimental results show that phonetic speaker recognition results may even be better when using phonetic decodings in languages different from that of the speech .
recently , structured precision matrix models were found to outper-form the conventional diagonal covariance matrix models . minimum phone error discriminative training of these diagonal covariance matrix models gave very good unadapted performance on large vocabulary continuous speech recognition systems . to obtain state-of-the-art performance , it is important to apply adaptation techniques efficiently to these diagonal covariance matrix models . in this paper , simple row-by-row iterative formulae are described for both mllr mean and constrained mllr transform estimations of these diagonal covariance matrix models . these update formulae are derived within the standard expectation maximisation framework and are guaranteed to increase the likelihood of the adaptation data . efficient approximate schemes for these adaptation techniques are also investigated to further reduce the computation . experimental results are presented based on the mpe trained subspace for precision and mean models , evaluated on both broadcast news and conversational telephone speech english tasks .
the accuracy levels achieved by state-of-the-art speaker verification systems are high enough for the technology to be used in real-life applications . unfortunately , the transfer from the lab to the field is not as straightforward as could be : the best performing systems can be computationally expensive to run and need large speaker model footprints . in this paper , we compare two speaker verification algorithms -lrb- gmm-svm su-pervectors and kharroubi 's gmm-svm vectors -rrb- and propose an improvement of kharroubi 's system that : -lrb- a -rrb- achieves up to 17 % relative performance improvement when compared to the supervectors algorithm ; -lrb- b -rrb- is 24 % faster in run time and -lrb- c -rrb- makes use of speaker models that are 94 % smaller than those needed by the supervectors algorithm .
it is often argued that acoustic-phonetic or articulatory features could be beneficial to automatic speech recognition because acoustic-phonetic or articulatory features provide a convenient interface between the acoustic and the linguistic level . former research has shown that a combination of acoustic and articulatory information can lead to improved asr . however there exists no purely articulatory driven asr system that outper-forms state-of-the-art systems driven by acoustic features . in this paper we propose a novel method for improving asr on the basis of articulatory features . it is designed to take account of -lrb- 1 -rrb- the correlations between articulatory features and -lrb- 2 -rrb- the fact that not all articulatory features are relevant for the description of a certain phonetic unit . we also investigate to what extend an acoustic and an articulatory feature driven system make different errors .
to date , systems for the identification of spoken languages have normally used magnitude-based parameterization methods such as the mfcc and regression-based shifted delta cepstrum . this paper investigates the use of the recently proposed modified group delay function coefficients in combination with traditional magnitude-based features in a gaussian mixture model based system . we also examine the application of feature warping to magnitude-based features and the modgdf and find that it can offer a significant cumulative improvement . we find that the addition of a modified regression-based shifted delta cepstrum further improves gaussian mixture model based system performance beyond that obtained by a more standard sdc configuration . the combination of regression-based shifted delta cepstrum , feature warping and the proposed modified group delay function coefficients achieved an accuracy of 88.4 % in tests on 10 languages in the ogi ts corpus , which compares very favourably with alternative language identification systems reported in the literature .
we consider empirical risk minimization in the context of stochastic optimization with exp-concave and smooth losses -- a general optimization framework that captures several important learning problems including linear and logistic regression , learning svms with the squared hinge-loss , portfolio selection and more . in this setting , we establish the first evidence that empirical risk minimization is able to attain fast generalization rates , and show that the expected loss of the empirical risk minimization in d dimensions converges to the optimal expected loss in a rate of d/n . this rate matches existing lower bounds up to constants and improves by a log n factor upon the state-of-the-art , which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms .
studied are two lifting-based families of symmetry-preserving reversible integer-to-integer wavelet transforms . the transforms from both of these families are shown to be compatible with symmetric extension , which permits the treatment of arbitrary length signals in a nonexpansive manner . throughout this work , particularly close attention is paid to rounding functions , and the properties that they must possess in various instances . symmetric extension is also shown to be equivalent to constant per-lifting-step extension in certain circumstances .
protein interactions typically arise from a physical interaction of one or more small sites on the surface of the two proteins . identifying these sites is very important for drug and protein design . in this paper , we propose a computational method based on probabilistic relational model that attempts to address this task using high-throughput protein interaction data and a set of short sequence motifs . we learn the computational method using the em algorithm , with a branch-and-bound algorithm as an approximate inference for the e-step . our computational method searches for motifs whose presence in a pair of interacting proteins can explain their observed interaction . it also tries to determine which motif pairs have high affinity , and can therefore lead to an interaction . we show that our computational method is more accurate than others at predicting new protein-protein interactions . more importantly , by examining solved structures of protein complexes , we find that 2/3 of the predicted active motifs correspond to actual interaction sites .
tremendous compute throughput is becoming available in personal desktop and laptop systems through the use of graphics processing units . however , exploiting this resource requires re-architecting an application to fit a data parallel programming model . the complex graph traversal routines in the inference process for large vocabulary continuous speech recognition have been considered by many as unsuitable for extensive parallelization . we explore and demonstrate a fully data parallel implementation of a speech inference engine on nvidia 's gtx280 gpu . our implementation consists of two phases-compute-intensive observation probability computation phase and communication-intensive graph traver-sal phase . we take advantage of dynamic elimination of redundant computation in the compute-intensive phase while maintaining close-to-peak execution efficiency . we also demonstrate the importance of exploring application-level trade-offs in the communication-intensive graph traversal phase to adapt the algorithm to data parallel execution on gpus . on 3.1 hours of speech data set , we achieve more than 11 × speedup compared to a highly optimized sequential implementation on intel core i7 without sacrificing accuracy .
we present a correlated bigram lsa approach for unsupervised lm adaptation for automatic speech recognition . the correlated bigram lsa approach is trained using efficient variational em and smoothed using the proposed fractional kneser-ney smoothing which handles fractional counts . we address the scalability issue to large training corpora via bootstrapping of bigram lsa from unigram lsa . for lm adaptation , unigram and bigram lsa are integrated into the background n-gram lm via marginal adaptation and linear interpolation respectively . experimental results on the mandarin rt04 test set show that applying unigram and bigram lsa together yields 6 % -- 8 % relative perplexity reduction and 2.5 % relative character error rate reduction which is statistically significant compared to applying only unigram lsa . on the large-scale evaluation on arabic , word error rate reduction from bigram lsa is statistically significant compared to the unadapted baseline .
computational narrative is a complex and interesting domain for exploring ai techniques that algo-rithmically analyze , understand , and most importantly , generate stories . this paper studies the importance of domain knowledge in story generation , and particularly in analogy-based story generation . based on the construct of knowledge container in case-based reasoning , we present a theoretical framework for incorporating domain knowledge in analogy-based story generation . we complement the framework with empirical results in our existing system riu .
we propose a new blind source separation approach that models the inherent signal dependencies such as those observed in speech signals in order to solve the problem of separating convolved sources . the frequency domain methods for the convolved mixture problem require a solution to the well-known permutation problem . our blind source separation approach is based on assuming a vector representation of the source signal where its multidimensional joint densities are non-spherical . spherical distributions may be adequate for signals that exhibit uniform dependencies across frequencies but in case of speech signals we can observe stronger dependencies for neighboring frequency bins and almost no dependency for frequency bins that are far apart . the non-spherical joint density model takes into account this phenomenon . for the separation of convolved sources , the proposed blind source separation approach demonstrates consistent performance over previous methods and improved performance over the spherical joint density representations .
the increasing complexity of summarization systems makes llr difficult to analyze exactly which modules make a difference in performance . we carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization : raw frequency -lrb- word probability -rrb- and log-likelihood ratio . we demonstrate that the advantages of log-likelihood ratio come from its known dis-tributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input . we also find that llr is more suitable for query-focused summarization since , unlike raw frequency , llr is more sensitive to the integration of the information need defined by the user .
formula compilation by generation of prime implicates or implicants finds a wide range of applications in ai . recent work on formula compilation by prime implicate/implicant generation often assumes a conjunctive/disjunctive normal form representation . however , in many settings propositional formulae are naturally expressed in non-clausal form . despite a large body of work on compilation of non-clausal formulae , in practice existing approaches can only be applied to fairly small formulae , containing at most a few hundred variables . this paper describes two novel approaches for the compilation of non-clausal formu-lae either with prime implicants or implicates , that is based on propositional satisfiability solving . these novel algorithms also find application when computing all prime implicates of a cnf formula . the proposed approach is shown to allow the compilation of non-clausal formulae of size significantly larger than existing approaches .
although ` tracking-by-detection ' is a popular approach when reliable object detectors are available , missed detections remain a difficult hurdle to overcome . we present a hybrid stochastic/deterministic optimization scheme that uses rjmcmc to perform stochastic search over the space of detection configurations , interleaved with deterministic computation of the optimal multi-frame data association for each proposed detection hypothesis . since object trajectories do not need to be estimated directly by the sampler , our approach is more efficient than traditional mcmcda techniques . moreover , our holistic formulation is able to generate longer , more reliable trajectories than baseline tracking-by-detection approaches in challenging multi-target scenarios .
we propose a kernel-based method for finding matching between instances across different domains , such as multilingual documents and images with annotations . each instance is assumed to be represented as a multiset of features , e.g. , a bag-of-words representation for documents . the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains can not be directly measured . to overcome this difficulty , the proposed kernel-based method embeds all the features of different domains in a shared latent space , and regards each instance as a distribution of its own features in the shared latent space . to represent the distributions efficiently and nonparametrically , we employ the framework of the kernel embeddings of distributions . the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart . in our experiments , we show that the proposed kernel-based method can achieve high performance on finding correspondence between multilingual wikipedia articles , between documents and tags , and between images and tags .
in this paper we discuss why a simple network topology inference algorithm based on network co-occurrence measurements and a markov random walk model for routing enables perfect topology reconstruction , despite the seeming model mismatch to real network routing .
our goal is to improve the training and prediction time of nyström method , which is a widely-used technique for generating low-rank kernel matrix approximations . when applying the nyström approximation for large-scale applications , both training and prediction time is dominated by computing kernel values between a data point and all landmark points . with m landmark points , this computation requires θ -lrb- md -rrb- time -lrb- flops -rrb- , where d is the input dimension . in this paper , we propose the use of a family of fast transforms to generate structured landmark points for nyström approximation . by exploiting fast transforms , e.g. , haar transform and hadamard transform , our modified nyström method requires only θ -lrb- m -rrb- or θ -lrb- m log d -rrb- time to compute the kernel values between a given data point and m landmark points . this improvement in time complexity can significantly speed up kernel approximation and benefit prediction speed in kernel machines . for instance , on the webspam data -lrb- more than 300,000 data points -rrb- , our proposed algorithm enables kernel svm prediction to deliver 98 % accuracy and the resulting prediction time is 1000 times faster than lib-svm and only 10 times slower than linear svm prediction -lrb- which yields only 91 % accuracy -rrb- .
high dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters , each with its own structure , e.g. , sum of low rank and sparse matrices , sum of sparse and rotated sparse vectors , etc. . in this paper , we consider general superposition models which allow sum of any number of component parameters , and each component structure can be characterized by any norm . we present a simple estimator for such models , give a geometric condition under which the components can be accurately estimated , characterize sample complexity of the estimator , and give high probability non-asymptotic bounds on the componentwise estimation error . we use tools from empirical processes and generic chaining for the statistical analysis , and our results , which substantially generalize prior work on superposition models , are in terms of gaussian widths of suitable sets .
the similarities between unvoiced speech sounds and turbulent breath sounds were used to detect change in sound characteristics caused by narrowing of the upper airway -lrb- ua -rrb- , similar to that occurring in obstructive sleep apnea . in 18 awake subjects , ua resistance -lrb- r au -rrb- , an index of ua narrowing , was measured simultaneously with breath sounds recording . linear prediction coding was applied on turbulent inspiratory sounds drawn from low and high r au conditions and k-means was used to cluster the resulting coefficients . the resulting 2 clusters were tested for agreement with the underlying r au status . distinct clusters were formed when r ua increased relatively high but not in cases with lower rise in r ua -lrb- p < 0.01 for all indicators . -rrb- this is the first work to show the utility of ua resistance -lrb- r au -rrb- in breath sounds analysis confirmed by an objective indicator or ua narrowing .
we present a methodology for the real time alignment of music signals using sequential montecarlo inference techniques . the alignment problem is formulated as the state tracking of a dynamical system , and differs from traditional hidden markov model-dynamic time warping based systems in that the hidden state is continuous rather than discrete . the major contribution of this paper is addressing both problems of audio-to-score and audio-to-audio alignment within the same framework in a real time setting . performances of the proposed methodology on both problems are then evaluated and discussed .
this paper presents a new geometric relation between a solid bounded by a smooth surface and its silhouette in images formed under weak perspective projection . the relation has the potential to be used for recognizing complex 3-d objects from a single image . objects are mod-eled by showing them to a camera without any knowledge of their motion . the main idea is to consider the dual of the 3-d surface and the family of dual curves of the silhouettes over all viewing directions . occluding contours correspond to planar slices of the dual surface . we introduce an affine-invariant representation of this surface that can constructed from a sequence of images and allows an object to be recognized from arbitrary viewing directions . we illustrate the proposed object representation scheme through synthetic examples and image contours detected in real images .
in this paper we report our attempts to facilitate the creation of mixed-initiative spoken dialogue systems for both novice and experienced developers of human language technology . our efforts have resulted in the creation of a utility called speech-builder , which allows developers to specify linguistic information about their domains , and rapidly create spoken dialogue interfaces to them . speech-builder has been used to create domains providing access to structured information contained in a relational database , as well as to provide human language interfaces to control or transaction-based applications .
matrices that collect the image coordinates of point features tracked through video -- one column per feature -- have often low rank , either exactly or approximately . this observation has led to many matrix factorization methods for 3d reconstruction , motion segmentation , or regularization of feature trajectories . however , temporary occlusions , image noise , and variations in lighting , pose , or object geometry often confound trackers . a feature that reappears after a temporary tracking failure -- whatever the cause -- is regarded as a new feature by typical tracking systems , resulting in very sparse matrices with many columns and rendering factorization problematic . we propose a method to simultaneously factor and compact such a matrix factorization methods by merging groups of columns that correspond to the same feature into single columns . this combination of compaction and factorization makes trackers more resilient to changes in appearance and short occlusions . preliminary experiments show that imputation of missing matrix entries -- and therefore matrix factorization -- becomes significantly more reliable as a result . clean column merging also required us to develop a history-sensitive feature reinitialization method we call feature snapping that aligns merged feature trajectory segments precisely to each other .
in this paper , we reconstruct signals from underdetermined linear measurements where the componentwise gains of the measurement system are unknown a priori . the reconstruction is performed through an adaptation of the message-passing algorithm called adaptive gamp that enables joint gain calibration and signal estimation . to evaluate our adaptive gamp , we apply adaptive gamp to the problem of sparse recovery and compare adaptive gamp against an 1-based approach . we numerically show that adaptive gamp yields excellent results even for a moderate amount of data . adaptive gamp approaches the performance of oracle gamp where the gains are perfectly known asymptot-ically .
spike sorting relies on the ability to establish the temporal occurrence of action potentials and their relation to specific neurons . neural information is intrinsically compressible and as such suitable for sparse sampling . potentially , this should allow for the use of multi-channel recordings , which is particularly advantageous to improve spike sorting . in this paper we propose a novel algorithm capable of sampling neural data at sub-nyquist rates , yielding the same performance for spike sorting as traditional schemes .
top-down visual saliency is an important module of visual attention . in this work , we propose a novel top-down saliency model that jointly learns a conditional random field and a visual dictionary . the proposed top-down saliency model incorporates a layered structure from top to bottom : conditional random field , sparse coding and image patches . by using sparse codes as intermediate layer , we learn a conditional random field in a supervised manner with the structured output of crf layer , and meanwhile learn the crf parameters with sparse coding as features . for efficient and effective joint learning , we develop a max-margin approach via a stochastic gradient descent algorithm . experimental results on the graz-02 and pascal voc datasets show that our top-down saliency model performs favorably against the state-of-the-art top-down saliency methods for target object localization and the dictionary update significantly improves the performance of our top-down saliency model . in addition , we demonstrate the merits of the proposed top-down saliency model by applying top-down saliency model to human fixation prediction .
we derive new bounds for the mixing parameter , $ , within the cross-correlation constant modulus algirithm for blind source separation and equalization in non-ideal multiuser environments . channel undermodelling and noise are considered when the complex sources are circularly symmetric . these tighter bounds are obtained by surface topography of the error performance surface of the cc-cma algorithm , and replace earlier work which suggested that ¥ § ¦ © ¨ the validity of the bounds is con rmed by simulation studies .
this paper presents a performance analysis of likelihood ratio test - based and generalized likelihood ratio test -lrb- glrt -rrb- - based array receivers for the detection of a known signal corrupted by a potentially noncircular interference . studying the distribution of the statistics associated with the likelihood ratio test and glrt , expressions of the probability of detection -lrb- p d -rrb- and false alarm -lrb- p fa -rrb- are given . in particular , an exact closed-form expression of p d and p fa are given for two lrt-based receivers and asymptotic -lrb- with respect to the data length -rrb- closed-form expression are given for p d and p fa for four glrt-based receivers . finally illustrative examples are presented in order to strengthen the obtained results .
recently , a number of over-complete dictionaries such as wavelets , wave packets , cosine packets etc. have been proposed . signal decomposition on such over-complete dictionaries is not unique . this non-uniqueness provides us with the opportunity to adapt the signal representation to the signal . the adaptation is based on sparsity , resolution and stability of the signal representation . the computational complexity of the signal representation is of primary concern . we propose a new approach for identifying the sparsest representation of a given signal in terms of a given over-complete dictionary . we assume that the data vector can be exactly represented in terms of a known number of vectors .
monolingual monolingual alignment is the task of pair-ing semantically similar units from two pieces of text . we report a top-performing supervised aligner that operates on short text snippets . we employ a large feature set to -lrb- 1 -rrb- encode similarities among semantic units -lrb- words and named entities -rrb- in context , and -lrb- 2 -rrb- address cooperation and competition for monolingual alignment among units in the same snippet . these features are deployed in a two-stage logistic regression framework for monolingual alignment . on two benchmark data sets , our supervised aligner achieves f 1 scores of 92.1 % and 88.5 % , with statistically significant error reductions of 4.8 % and 7.3 % over the previous best supervised aligner . supervised aligner produces top results in extrinsic evaluation as well .
this paper presents a novel method to enhance the performance of traditional speaker adaptation algorithm using discriminative adaptation procedure based on a novel confidence measure and non-linear weighting . regardless of the distribution of the adaptation data , traditional model adaptation methods incorporate the adaptation data undiscriminatingly . when the data size is small and the parameter tying is extensive , adaptation based on outliers can be detrimental . a way to discriminate the contribution of each data in the adaptation is to incorporate a confidence measure based on likelihood . we evaluate and compare the performances of the proposed weighted smap which controls the contribution of each data by sigmoid weight-ing using a novel confidence measure . the effectiveness of the proposed algorithm is experimentally verified by adapting native speaker models to nonnative speaker environment using tidigit .
random forests have been shown to perform very well in propositional learning . forf is an upgrade of random forests for relational data . in this paper we investigate shortcomings of forf and propose an alternative algorithm , r 4 f , for generating random forests over relational data . r 4 f employs randomly generated relational rules as fully self-contained boolean tests inside each node in a tree and thus can be viewed as an instance of dynamic propositionalization . the implementation of r 4 f allows for the simultaneous or parallel growth of all the branches of all the trees in the ensemble in an efficient shared , but still single-threaded way . experiments favorably compare r 4 f to both forf and the combination of static proposition-alization together with standard random forests . various strategies for tree initialization and splitting of nodes , as well as resulting ensemble size , diversity , and computational complexity of r 4 f are also investigated .
a semi-continuous segmental probability model , which can be considered as a special form of continuous mixture segmental probability model with continuous output probability density functions sharing in a mixture gaussian density codebook , is proposed in this paper . the amount of training data required , as well as the computational complexity of the semi-continuous segmental probability model -lsb- 2 -rsb- , can be significantly reduced in comparison with the continuous segmental probability model . parameters of the vector quantization codebook and segmental probability model can be mutually optimized to achieve an optimal model/codebook combination , which leads to a unified modeling approach to vector quantization and segmental probability modeling of speech signals . the experimental results show that the recognition accuracy of the semi-continuous segmental probability model is higher than the semi-continuous hidden markov model and continuous segmental probability model .
a basic reasoning problem in dynamic systems is the projection problem : determine if a formula holds after a sequence of actions has been performed . in this paper , we propose a tractable 1 solution to the projection problem in the presence of incomplete first-order knowledge and context-dependent actions . our solution is based on a type of progression , that is , we progress the initial knowledge base wrt the action sequence and answer the query against the resulting knowledge base . the form of reasoning we propose is always logically sound and is also logically complete when the query is in a certain normal form and the agent has complete knowledge about the context of any context-dependent actions .
recently , deep neural networks have shown promise as an acoustic model for statistical parametric speech synthesis . their ability to learn complex mappings from linguistic features to acoustic model has advanced the naturalness of synthesis speech significantly . however , because dnn parameter estimation methods typically attempt to minimise the mean squared error of each individual frame in the training data , the dynamic and continuous nature of speech parameters is neglected . in this paper , we propose a training criterion that minimises speech parameter trajectory errors , and so takes dynamic constraints from a wide acoustic context into account during training . we combine this novel training criterion with our previously proposed stacked bottleneck features , which provide wide linguistic context . both objective and subjective evaluation results confirm the effectiveness of the proposed training criterion for improving model accuracy and naturalness of synthesised speech .
this paper presents a novel integrated background model for video surveillance . our integrated background model uses a primal sketch representation for image appearance and 3d scene geometry to capture the ground plane and major surfaces in the scene . the integrated background model divides the background image into three types of regions -- flat , sketchable and textured . the three types of regions are modeled respectively by mixture of gaussians , image primitives and lbp histograms . we calibrate the camera and recover important planes such as ground , horizontal surfaces , walls , stairs in the 3d scene , and use geometric information to predict the sizes and locations of foreground blobs to further reduce false alarms . compared with the state-of-the-art background modeling methods , our integrated background model is more effective , especially for indoor scenes where shadows , highlights and reflections of moving objects and camera exposure adjusting usually cause problems . experiment results demonstrate that our integrated background model improves the performance of background/foreground separation at pixel level , and the integrated video surveillance system at the object and tra-jectory level .
decentralized physics-based field estimation in clustered sensor networks requires the exchange of state vectors between neighboring clusters . we reduce the communication overhead between clusters by using a differential encoding of state vectors that exploits the spatio-temporal field dependencies . this encoding involves a kalman prediction step that builds on the state-space equations governing the field 's spatio-temporal evolution . the kalman prediction step keeps the computational complexity low . simulation results for an acoustic field demonstrate the approach .
this paper presents a novel quantization based watermarking scheme . watermark embedding is performed through modulating the normalized correlation between the host vector and a random vector with dither modulation . the watermarked signal is derived to provide the modulated normalized correlation in the sense of minimizing the embedding distortion . the proposed quantization based watermarking scheme is theoretically invariant to valumet-ric scaling and can resist stronger noise than the well-known spread transform dither modulation . numerical simulations on real images show that quantization based watermarking scheme achieves the good imperceptibility and strong robustness against a wide range of attacks .
in order to select a good hypothesis language -lrb- or model -rrb- from a collection of possible models , one has to assess the generalization performance of the hypothesis which is returned by a learner that is bound to use some particular model . this paper deals with a new and very eecient way of assessing this generalization performance . we present a new analysis which characterizes the expected generalization error of the hypothesis with least training error in terms of the distribution of error rates of the hypotheses in the model . this distribution can be estimated very eeciently from the data which immediately leads to an eecient model selection algorithm . the analysis predicts learning curves with a very high precision and thus contributes to a better understanding of why and when over-tting occurs . we present empirical studies -lrb- controlled experiments on boolean decision trees and a large-scale text categorization problem -rrb- which show that the eecient model selection algorithm leads to error rates which are often as low as those obtained by 10-fold cross validation -lrb- sometimes even superior -rrb- . however , the eecient model selection algorithm is much more eecient -lrb- because the learner does not have to be invoked at all -rrb- and thus solves model selection problems with as many as thousand relevant attributes and 12,000 examples .
the estimation of surface emissivity and temperature from thermal hyperspectral data is a challenge . methods that estimate the temperature and emissivity on a pixel composed by one single material exist . however , the estimation of the temperature on a mixed pixel , i.e. a pixel composed by more than one material , is more complex and has scarcely been investigated in the literature . this paper addresses this issue by proposing an estimator which linearizes the black body law around the mean temperature of each material . the performance of this estimator is studied using simulated data with different hyperspectral sensor configurations and under various noise conditions . the obtained results are encouraging and show an accuracy on the estimated temperature of 0.5 k while using high spectral resolution sensor .
this paper presents a novel approach for concatenative speech synthesis . this approach enables reduction of the dataset size of a concatenative text-to-speech system , namely the ibm trainable speech synthesis system , by more than an order of magnitude . a spectral acoustic feature based speech representation is used for computing a cost function during segment selection as well as for speech generation . initial results indicate that even with a dataset size of a few megabytes it is possible to achieve quality which is significantly higher than existing small footprint formant based synthesizers .
we propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available , which is referred to as query weighting . query weighting is a key step in ranking model adaptation . as the learning object of ranking algorithms is divided by query instances , we argue that it 's more reasonable to conduct importance weighting at query level than document level . we present two query weighting schemes . the first compresses the query into a query feature vector , which aggregates all document instances in the same query , and then conducts query weighting based on the query feature vector . this query weighting schemes can efficiently estimate query importance by compressing query data , but the potential risk is information loss resulted from the compression . the second measures the similarity between the source query and each target query , and then combines these fine-grained similarity values for its importance estimation . adaptation experiments on letor3 .0 data set demonstrate that query weighting significantly outperforms document instance weighting methods .
recently there has been an increasing amount of work in the area of automatic genre classification of music in audio format . such systems can be used as a way to evaluate features describing musical content as well as a way to structure large collections of music . however the evaluation and comparison of genre classification systems is hindered by the subjective perception of genre definitions by users . in this work we describe a set of experiments in automatic musical genre classification . an important contribution of this work is the comparison of the automatic results with human genre classification on the same dataset . the results show that , although there is significant room for improvement , genre classification is inherently subjective and therefore perfect results can not be expected from either automatic algorithms or human annotation . the experiments also show that the use of features derived from an auditory model have similar performance with features based on mel-frequency cepstral coefficients .
in this paper , we propose a speaking rate compensation method using frame period and frame length adaptation . our speaking rate compensation method decodes an input utterance using several sets of frame period and frame length parameters for speech analysis . then , this speaking rate compensation method selects the best set with the highest score which consists of the acoustic likelihood normalized by frame period , language likelihood and insertion penalty . furthermore , we apply this speaking rate compensation method to the training of the acoustic model . we calculate the acoustic likelihood for each frame period and frame length using viterbi alignment and select the best one for each training utterance . the proposed speaking rate compensation applied to both the acoustic model creation process and decoding process resulted in accuracy improvement of 2.9 % -lrb- absolute -rrb- for spontaneous lecture speech recognition task .
in this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection . we show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing . empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .
we consider the problem of time difference of arrival estimation for cyclostationary signals in additive white gaussian noise . classical approaches to the problem either ignore the cyclostationarity and use ordinary cross-correlations , or exploit the cyclostationarity by using cyclic cross-correlations , or combine these approaches into a mul-ticycle approach . despite contradicting claims in the literature regarding the performance-ranking of these approaches , there has been almost no analytical comparative performance study . we propose to regard the estimated -lrb- ordinary or cyclic -rrb- correlations as the '' front-end '' data , and based on their asymptotically gaussian distribution , to compute the asymptotic cramér-rao bounds for the various combinations -lrb- ordinary/single-cycle/multi-cycle -rrb- . using our cyclic-correlations-based crb -lrb- termed '' crbcrb '' -rrb- , we can bound the performance of any -lrb- unbiased -rrb- estimator which exploits a given set of correlations . moreover , we propose an approximate maximum likelihood estimator -lrb- with respect to the correlations -rrb- , and show that approximate maximum likelihood estimator attains our crbcrb asymptotically in simulations , outperforming the competitors .
we consider a special type of multi-label learning where class assignments of training examples are incomplete . as an example , an instance whose true class assignment is -lrb- c 1 , c 2 , c 3 -rrb- is only assigned to class c 1 when it is used as a training sample . we refer to this problem as multi-label learning with incomplete class assignment . incompletely labeled data is frequently encountered when the number of classes is very large -lrb- hundreds as in mir flickr dataset -rrb- or when there is a large ambiguity between classes -lrb- e.g. , jet vs plane -rrb- . in both cases , it is difficult for users to provide complete class assignments for objects . we propose a ranking based multi-label learning framework that explicitly addresses the challenge of learning from incompletely labeled data by exploiting the group lasso technique to combine the ranking errors . we present a learning algorithm that is empirically shown to be efficient for solving the related optimization problem . our empirical study shows that the proposed ranking based multi-label learning framework is more effective than the state-of-the-art algorithms for multi-label learning in dealing with incompletely labeled data .
in statistical analysis of video sequences for speech recognition , and more generally activity recognition , it is natural to treat temporal evolutions of features as trajec-tories on riemannian manifolds . however , different evolution patterns result in arbitrary parameterizations of these trajectories . we investigate a recent framework from statistics literature -lsb- 15 -rsb- that handles this nuisance variability using a cost function/distance for temporal registration and statistical summarization & modeling of trajectories . it is based on a mathematical representation of trajectories , termed transported square-root vector field , and the l 2 norm on the space of transported square-root vector field . we apply this framework to the problem of speech recognition using both audio and visual components . in each case , we extract features , form trajectories on corresponding manifolds , and compute parametrization-invariant distances using transported square-root vector field for speech classification . on the transported square-root vector field the classification performance under metric increases significantly , by nearly 100 % under both modalities and for all choices of features . we obtained speaker-dependent classification rate of 70 % and 96 % for visual and audio components , respectively .
this paper presents a cyclostationary approach to the non-data-aided phase and timing estimation problem for staggered modulations . the problem is addressed under the low-snr unconditional maximum likelihood framework , and modulations such as offset quadrature amplitude modulation and minimum shift keying are considered . in this sense , it is found that not only the timing parameter but also the phase , can be jointly obtained from the asymptotic uml cost function based on the spectral line generation after a second-order non-linearity .
in this paper , we introduce a novel pitch tracking database including ground truth signals obtained from a laryn-gograph . the pitch tracking database , referenced as pitch tracking database , consists of 2342 phonetically rich sentences taken from the timit corpus . each sentence was at least recorded once by a male and a female native speaker . in total , the pitch tracking database contains 4720 recordings from 10 male and 10 female speakers . furthermore , we evaluated two multipitch tracking systems on a subset of speakers to provide a benchmark for further research activities . the pitch tracking database can be downloaded at
authorship attribution -lrb- authorship attribution -rrb- aims to identify the authors of a set of documents . traditional studies in this area often assume that there are a large set of labeled documents available for training . however , in the real life , it is often difficult or expensive to collect a large set of labeled data . for example , in the online review domain , most reviewers -lrb- authors -rrb- only write a few reviews , which are not enough to serve as the training data for accurate classification . in this paper , we present a novel three-view tri-training method to iteratively identify authors of unlabeled data to augment the training set . the key idea is to first represent each document in three distinct views , and then perform tri-training to exploit the large amount of un-labeled documents . starting from 10 training documents per author , we systematically evaluate the effectiveness of the proposed three-view tri-training method for authorship attribution . experimental results show that the proposed three-view tri-training method outperforms the state-of-the-art semi-supervised method cng+svm and other baselines .
conflict escalation in multi-party conversations refers to an increase in the intensity of conflict during conversations . here we study annotation and detection of conflict escalation in broadcast political debates towards a machine-mediated conflict management system . in this regard , we label conflict escalation using crowd-sourced annotations and predict it with automatically extracted conversational and prosodic features . in particular , to annotate the conflict escalation we deploy two different strategies , i.e. , indirect inference and direct assessment ; the direct assessment method refers to a way that annotators watch and compare two consecutive clips during the annotation process , while the indirect inference method indicates that each clip is independently annotated with respect to the level of conflict then the level conflict escalation is inferred by comparing annotations of two consecutive clips . empirical results with 792 pairs of consecutive clips in classifying three types of conflict escalation , i.e. , escalation , de-escalation , and constant , show that labels from direct assessment yield higher classification performance -lrb- 45.3 % unweighted accuracy -lrb- ua -rrb- -rrb- than the one from indirect inference -lrb- 39.7 % ua -rrb- , although the annotations from both methods are highly correlated -lrb- ρ = 0.74 in continuous values and 63 % agreement in ternary classes -rrb- .
many traditional ai algorithms fail to scale as the size of state space increases exponentially with the number of features . one way to reduce computation in such scenarios is to reduce the problem size by grouping symmetric states together and then running the algorithm on the reduced problem . the focus of this work is to exploit symmetry in problems of sequential decision making and probabilistic inference . our recent work-asap-uct defines new state-action pair symmetries in markov decision processes . we also apply these state-action pair symmetries in monte carlo tree search framework . in probabilistic inference , we expand the notion of unconditional symmetries to contextual symmetries and apply unconditional symmetries in markov chain monte carlo methods . in future , we plan to explore interesting links in symmetry exploitation in different problems and aim to develop a generic symmetry based framework .
using data-driven techniques and ultrasound data , it is possible to learn models that reconstruct the tongue shape of a speaker with submillimetric accuracy given the location of 3 -- 4 fleshpoints , and to adapt these models to a new speaker for which little data is available . in practice , tongue contours extracted from ultrasound imaging are often incomplete because of shadowing , noise and other factors . we extend these models to deal with missing data during learning and adaptation , and show that submillimetric accuracy can still be achieved even with relatively large amounts of missing data .
we address the problem of compression for wireless sensor networks , where each of the sensors has limited power , and acquires data that should be sent to a central node . the final goal is to have a reconstructed version of the sampled field at the central node , with the sensors spending as little energy as possible . we propose a distributed compression algorithm for multihop , distributed sensor networks based on the lifting factorization of the wavelet transform that exploits the natural data flow in the network to aggregate data by computing partial wavelet coefficients that are refined as the data flows towards the central node . a key result of our work is that by performing partial computations we greatly reduce unnecessary transmission , significantly reducing the overall energy consumption .
most existing speech and audio coders were developed to meet a single purpose of delivering the best quality possible under fixed constraints in bit-rate , computational complexity , and algorithmic delay . recent expansion in network communications demands the additional capability to cope with the packet-lossy nature associated with these network communications . the problem lies within the research area of multiple description coding -lrb- mdc -rrb- . in this report we investigate its essence , state its inherent limitations and tradeoffs , and propose a novel method to design efficient mdc systems for speech .
many design tasks involve the creation of new objects in the context of an existing scene . existing work in computer vision only provides partial support for such tasks . on the one hand , multi-view stereo algorithms allow the reconstruction of real-world scenes , while on the other hand algorithms for line-drawing interpretation do not take context into account . our work combines the strength of these two domains to interpret line drawings of imaginary objects drawn over photographs of an existing scene . the main challenge we face is to identify the existing 3d structure that correlates with the line drawing while also allowing the creation of new structure that is not present in the real world . we propose a labeling algorithm to tackle this problem , where some of the labels capture dominant orientations of the real scene while a free label allows the discovery of new orientations in the imaginary scene . we illustrate our labeling algorithm by interpreting line drawings for urban planing , home remodeling , furniture design and cultural heritage .
the total variability factor space in speaker verification system architecture based on factor analysis has greatly improved speaker recognition performances . carrying out channel compensation in a low dimensional total factor space , rather than in the gmm supervector space , allows for the application of new techniques . we propose here new intersession compensation and scoring methods . furthermore , this new approach contributes to a better understanding of the session variability characteristics in the total factor space .
in this paper , we proposed a method to realize the recently developed keyword-aware grammar for lvcsr-based keyword search using weight finite-state automata . the approach creates a compact and deterministic grammar wfsa by inserting keyword paths to an existing n-gram wfsa . tested on the evalpart1 data of the iarpa babel openkws13 vietnamese and openkws14 tamil limited-language pack tasks , the experimental results indicate the proposed keyword-aware framework achieves significant improvement , with about 50 % relative actual term weighted value -lrb- atwv -rrb- enhancement for both languages . comparisons between the keyword-aware grammar and our previously proposed n-gram lm based approximation approach for the grammar also show that the kws performances of these two realizations are complementary .
we propose a family of supervised dimensionality reduction algorithms that combine feature extraction -lrb- dimensionality reduction -rrb- with learning a predictive model in a unified optimization framework , using data - and class-appropriate generalized linear models , and handling both classification and regression problems . our supervised dimensionality reduction algorithms uses simple closed-form update rules and is provably convergent . promising empirical results are demonstrated on a variety of high-dimensional datasets .
various information sources naturally contains new words that appear in a daily basis and which are not present in the vocabulary of the speech recognition system but are important for applications such as closed-captioning or information dissemination . to be recognized , those words need to be included in the vocabulary and the language model parameters updated . in this context , we propose a new method that allows including new words in the vocabulary even if no well suited training data is available , as is the case of archived documents , and without the need of lm retraining . it uses morpho-syntatic information about an in-domain corpus and part-of-speech word classes to define a new lm unigram distribution associated to the updated vocabulary . experiments were carried out for a european portuguese broadcast news transcription system . results showed a relative reduction of 4 % in word error rate , with 78 % of the occurrences of those newly included words being correctly recognized .
the objective of this paper is to design an embedding method mapping local features describing image -lrb- e.g. sift -rrb- to a higher dimensional representation used for image retrieval problem . by investigating the relationship between the linear approximation of a nonlinear function in high dimensional space and state-of-the-art feature representation used in image retrieval , i.e. , vlad , we first introduce a new approach for the approximation . the embedded vectors resulted by the function approximation process are then aggregated to form a single representation used in the image retrieval framework . the evaluation shows that our embedding method gives a performance boost over the state of the art in image retrieval , as demonstrated by our experiments on the standard public image retrieval benchmarks .
with the rapid development of telecommunication techniques and digital devices , it is quite easy to copy , modify and republish videos in digital format , resulting in large volume of duplicate videos on the web in recent years . in this paper we mainly investigate the problem of detecting excessive content duplication , so as to facilitate video search and intelligence propriety protection . a real-time detection method is hence proposed , which first selects videos ' representative frames and then reduces each to a 64 bit hash code . then the similarity of any two videos can be estimated by the proportion of their similar hash codes . the experiments demonstrate that our real-time detection method is both efficient and effective in terms of real-time applications .
the fact that image data samples lie on a manifold has been successfully exploited in many learning and inference problems . in this paper we leverage the specific structure of data in order to improve recognition accuracies in general recognition tasks . in particular we propose a novel framework that allows to embed manifold priors into sparse representation-based classification approaches . we also show that manifold constraints can be transferred from the data to the optimized variables if these are linearly correlated . using this new insight , we define an efficient alternating direction method of multipliers that can consistently integrate the manifold constraints during the optimization process . this is based on the property that we can recast the problem as the projection over the mani-fold via a linear embedding method based on the geodesic distance . the proposed approach is successfully applied on face , digit , action and objects recognition showing a consistently increase on performance when compared to the state of the art .
we present in this paper a novel algorithm for single channel speech enhancement . it is based on a subspace approach in the bark domain and an optimal subspace selection by the minimum description length criterion . the processing in the bark domain allows us to take into account in an optimal manner the masking properties of the human auditory system . the subspace selection provided by the mdl criterion overcomes the limitations encountered with other selection criteria , like the overes-timation of the signal -- plus -- noise subspace or the need for empirical parameters . together , the resulting mdl-subspace approach in the bark domain provides maximum noise reduction while minimizing signal distortions . the performance of our algorithm is assessed in white and colored noise . it shows that our algorithm provides high performance for a large scale of input signal-to-noise ratio .
the knowledge compilation map introduced by darwiche and marquis takes advantage of a number of concepts -lrb- mainly queries , transformations , expressiveness , and succinctness -rrb- to compare the relative adequacy of representation languages to some ai problems . however , the framework is limited to the comparison of languages that are interpreted in a homogeneous way -lrb- formulae are interpreted as boolean functions -rrb- . this prevents one from comparing , on a formal basis , languages that are close in essence , such as obdd , mdd , and add . to fill the gap , we present a generalized framework into which comparing formally heterogeneous representation languages becomes feasible . in particular , we explain how the key notions of queries and transformations , expressiveness , and succinctness can be lifted to the generalized setting .
this paper presents two versions of an algorithm for bandwidth extension of speech signals . we focus on the generation of the spectral envelope and compare the performance of two different approaches -- neural networks versus codebooks -- in terms of objective and subjective distortion measures .
in this paper , we present a novel approach for power control in a multiuser data transmission system . the main objective of our approach is to distribute the overall transmission power among the lines in a way , such that a certain ratio between the bit rate of the lines is achieved . given the maximum transmission power per line and the ratio of the bit rates both the transmission power and the achievable bit rate for each line are found by an iterative optimization . we show the impact of our novel power control in an experimental section , where the approach is used for finding the transmission power for a ten-line vdsl-2 system with various bit rate distributions .
we combine the replica approach from statistical physics with a varia-tional approach to analyze learning curves analytically . we apply the replica approach to gaussian process regression . as a main result we derive ap-proximative relations between empirical error measures , the generalization error and the posterior variance .
in this paper , an approach to continuous speech recognition based on a two-layer lexical tree is proposed . the search network is maintained by the two-layer lexical tree , in which the first layer reflects the word net and the phone net while the second layer the dynamic programming . because the acoustic information is tied in the second layer , the memory cost is so small that it has the ability to process some complicated applications , such as the use of crossword context-dependent triphone models , the chinese fuzzy syllable mapping and the pronunciation modeling . the search algorithm based on the two-layer lexical tree is also proposed , which is derived from the token-passing algorithm . finally , an implementation of the two-layer lexical tree using the crossword context-dependent triphone models is presented , and the experimental results show that the highly efficient decoding can be achieved without too much memory cost .
in this paper , a new algorithm for robust adaptive beamform-ing is developed . the basic idea of the proposed algorithm is to estimate the difference between the actual and presumed steering vectors and to use this difference to correct the erroneous presumed steering vector . the estimation process is performed iteratively where a quadratic convex optimization problem is solved at each iteration . unlike other robust beam-forming techniques , our algorithm does not assume that the norm of the mismatch vector is upper bounded , and hence it does not suffer from the negative effects of over/under estimation of the upper bound . simulation results show the effectiveness of the proposed algorithm .
this paper considers a markovian dynamical game theoretic setting for distributed transmission control in a wireless sensor network . the available spectrum bandwidth is modeled as a markov chain . a distributed algorithm named correlated q-learning algorithm is proposed to obtain the correlated equilibrium policies of the wireless sensor network . this correlated q-learning algorithm has the decentralized feature and is easily implementable in a real wireless sensor network . numerical example is also provided to verify the performances of the proposed correlated q-learning algorithm .
it has recently been shown that a multi-channel linear prediction can effectively achieve blind speech dereverberation based on maximum-likelihood estimation . this approach can estimate and cancel unknown reverberation processes from only a few seconds of observation . however , one problem with this approach is that speech distortion may increase if we iterate the dereverberation more than once based on itakura-saito distance minimization to further reduce the reverberation . to overcome this problem , we introduce speech log-spectral priors into this approach , and refor-mulate it based on maximum a posteriori estimation . two types of priors are introduced , a gaussian mixture model -lrb- gmm -rrb- of speech log spectra , and a gmm of speech mel-frequency cepstral coefficients . in the formulation , we also propose a new versatile technique to integrate such speech log-spectral priors with the is distance minimization in a computationally efficient manner . preliminary experiments show the effectiveness of the proposed approach .
we present a pp-attachment disambiguation method based on a gigantic volume of unambiguous examples extracted from raw corpus . the unambiguous examples are utilized to acquire precise lexical preferences for pp-attachment disambiguation . attachment decisions are made by a machine learning method that optimizes the use of the lexical preferences . our experiments indicate that the precise lexical preferences work effectively .
state transition matrices as used in standard hmm decoders have two widely perceived limitations . one is that the implicit geometric state duration distributions which they model do not accurately reflect true duration distributions . the other is that they impose no hard limit on maximum duration with the result that state transition probabilities often have little influence when combined with acoustic probabilities , which are of a different order of magnitude . explicit duration models were developed in the past to address the first problem . these were not widely taken up because their performance advantage in clean speech recognition was often not sufficiently great to offset the extra complexity which they introduced . however , duration models have much greater potential when applied to noisy speech recognition . in explicit duration model paper we present a simple and generic form of explicit duration model and show that explicit duration model leads to strong performance improvements when applied to connected digit recognition in noise .
the hard thresholding pursuit is a class of truncated gradient descent methods for finding sparse solutions of ℓ 0-constrained loss minimization problems . the htp-style methods have been shown to have strong approximation guarantee and impressive numerical performance in high dimensional statistical learning applications . however , the current theoretical treatment of these htp-style methods has traditionally been restricted to the analysis of parameter estimation consistency . it remains an open problem to analyze the support recovery performance -lrb- a.k.a. , sparsistency -rrb- of this type of htp-style methods for recovering the global minimizer of the original np-hard problem . in this paper , we bridge this gap by showing , for the first time , that exact recovery of the global sparse minimizer is possible for htp-style methods under restricted strong condition number bounding conditions . we further show that htp-style methods are able to recover the support of certain relaxed sparse solutions without assuming bounded restricted strong condition number . numerical results on simulated data confirms our theoretical predictions .
there are many complex combinatorial problems which involve searching for an undirected graph satisfying a certain property . these combinatorial problems are often highly challenging because of the large number of isomorphic representations of a possible solution . in this paper we introduce novel , effective and compact , symmetry breaking constraints for undirected graph search . while incomplete , isomorphic representations prove highly beneficial in pruning the search for a graph representation . we illustrate the application of symmetry breaking in graph representation to resolve several open instances in extremal graph theory .
extracted keyphrases can enhance numerous applications ranging from search to tracking the evolution of scientific discourse . we present schbase , a hierarchical database of keyphrases extracted from large collections of scientific literature . schbase relies on a tendency of scientists to generate new abbreviations that '' extend '' existing forms as a form of signaling novelty . we demonstrate how these keyphrases/concepts can be extracted , and their viability as a database in relation to existing collections . we further show how keyphrases can be placed into a semantically-meaningful '' phylogenetic '' structure and describe key features of this structure .
in this paper , we explore the potential of geo-social media to construct location-based interest profiles to uncover the hidden relationships among disparate locations . through an investigation of millions of geo-tagged tweets , we construct a per-city interest model based on fourteen high-level categories -lrb- e.g. , technology , art , sports -rrb- . these interest models support the discovery of related locations that are connected based on these categorical perspectives -lrb- e.g. , college towns or vacation spots -rrb- but perhaps not on the individual tweet level . we then connect these city-based interest models to underlying demographic data . by building mul-tivariate multiple linear regression and neural network models we show how a location 's interest profile may be estimated based purely on its demo-graphics features .
shortage of manually sense-tagged data is an obstacle to supervised word sense dis-ambiguation methods . in this paper we investigate a label propagation based semi-supervised learning algorithm for wsd , which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption : similar examples should have similar labels . our experimental results on benchmark corpora indicate that label propagation based semi-supervised learning algorithm consistently out-performs wsd when only very few labeled examples are available , and its performance is also better than monolingual bootstrapping , and comparable to bilingual bootstrapping .
finding correspondences between feature points is one of the most relevant problems in the whole set of visual tasks . in this paper we address the problem of matching a feature vector -lrb- or a matrix -rrb- to a given subspace . given any vector base of such a subspace , we observe a linear combination of its elements with all entries swapped by an unknown permutation . we prove that such a computation-ally hard integer problem is uniquely solved in a convex set resulting from relaxing the original problem . also , if noise is present , based on this result , we provide a robust estimate recurring to a linear programming-based algorithm . we use structure-from-motion and object recognition as motivating examples .
one of the key challenges in human action recognition from video sequences is how to model an human action sufficiently . therefore , in this paper we propose a novel motion-based representation called motion context , which is insensitive to the scale and direction of an human action , by employing image representation techniques . a motion context captures the distribution of the motion words -lrb- mws -rrb- over relative locations in a local region of the motion image -lrb- mi -rrb- around a reference point and thus summarizes the local motion information in a rich 3d mc descriptor . in this way , any human action can be represented as a 3d descriptor by summing up all the mc descriptors of this human action . for action recognition , we propose 4 different recognition configurations : mw + plsa , mw+svm , motion context + w 3-plsa -lrb- a new direct graphical model by extending plsa -rrb- , and mc+svm . we test our approach on two human action video datasets from kth and weizmann institute of science -lrb- wis -rrb- and our performances are quite promising . for the motion context , the proposed motion-based representation achieves the highest performance using the proposed w 3-plsa . for the motion context , the best performance of the proposed motion context is comparable to the state of the art .
in this paper we propose a novel , effective , and efficient utterance verification technology for access control in the interactive voice response systems . the key of our utterance verification technology is to construct a context-free grammar by using the secret answer to a question and a word n-gram based filler model . the utterance verification technology provides rich alternatives to the secret answer and can potentially improve the accuracy of the interactive voice response systems . utterance verification technology can also absorb carrier words used by callers and thus can improve the robustness . we also propose using a predictor based on the best alternative to calculate the confidence . we show detailed experimental results on a tough uv test set that contains 930 positive and 930 negative cases and discuss types of questions that are suitable for the interactive voice response systems . we demonstrate that our utterance verification technology can achieve a 2.14 % equal error rate on average and 0.8 % false accept rate if the false reject rate is 2.6 % and above . this is a 49 % equal error rate compared with the approaches using acoustic fillers , and a 72 % equal error rate compared with the posterior probability based confidence measurement .
in this paper , we propose a real-time 3d hand pose estimation algorithm using the randomized decision forest framework . our real-time 3d hand pose estimation algorithm takes a depth image as input and generates a set of skeletal joints as output . previous decision forest-based methods often give labels to all points in a point cloud at a very early stage and vote for the joint locations . by contrast , our real-time 3d hand pose estimation algorithm only tracks a set of more flexible virtual landmark points , named segmentation index points , before reaching the final decision at a leaf node . roughly speaking , an segmentation index points represents the cen-troid of a subset of skeletal joints , which are to be located at the leaves of the branch expanded from the segmentation index points . inspired by recent latent regression forest-based hand pose estimation framework -lrb- tang et al. 2014 -rrb- , we integrate segmentation index points into the framework with several important improvements : first , we devise a new forest growing strategy , whose decision is made using a randomized feature guided by segmentation index points . second , we speed-up the forest growing strategy since only segmentation index points , not the skeletal joints , are estimated at non-leaf nodes . third , the experimental results on public benchmark datasets show clearly the advantage of the proposed real-time 3d hand pose estimation algorithm over previous state-of-the-art methods , and our real-time 3d hand pose estimation algorithm runs at 55.5 fps on a normal cpu without parallelism .
we present a manifold learning approach to dimension-ality reduction that explicitly models the manifold learning approach as a mapping from low to high dimensional space . the manifold learning approach is represented as a parametrized surface represented by a set of parameters that are defined on the input samples . the manifold learning approach also provides a natural mapping from high to low dimensional space , and a concatenation of these two natural mapping induces a projection operator onto the manifold learning approach . the explicit projection operator allows for a clearly defined objective function in terms of projection distance and reconstruction error . a formulation of the natural mapping in terms of kernel regression permits a direct optimization of the objective function and the extremal points converge to principal surfaces as the number of data to learn from increases . principal surfaces have the desirable property that they , informally speaking , pass through the middle of a distribution . we provide a proof on the convergence to principal surfaces and illustrate the effectiveness of the proposed manifold learning approach on synthetic and real data sets .
this discussion paper proposes to generalize the notion of independent component analysis to the notion of multidimen-sional independent component analysis . we start from the ica or blind source separation -lrb- bss -rrb- model and show that independent component analysis can be uniquely identified provided independent component analysis is properly parameterized in terms of one-dimensional subspaces . from this standpoint , the bss/ica model is generalized to multidimensional components . we discuss how independent component analysis can be adapted to multidimen-sional independent component analysis . the relevance of these ideas is illustrated by a mica decomposition of ecg signals .
iterative shrinkage of sparse and redundant representations are at the heart of many state of the art denoising and decon-volution algorithms . they assume the signal is well approximated by a few elements from an overcomplete basis of a linear space . if one instead selects the elements from a nonlinear manifold it is possible to more efficiently represent piecewise polynomial signals . this suggests that image restoration algorithms based around nonlinear transformations could provide better results for this class of signals . this paper uses iterative shrinkage ideas and a nonlinear quadtree decomposition to develop image restoration algorithms suitable for piecewise polynomial images .
spectral graph partitioning spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science . some notable studies have been carried out regarding the behavior of these spectral graph partitioning methods for infinitely large sample size -lrb- von luxburg et al. , 2008 ; rohe et al. , 2011 -rrb- , which provide sufficient confidence to practitioners about the effectiveness of these spectral graph partitioning methods . on the other hand , recent developments in computer science have led to a plethora of applications , where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs . in this paper , we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting . we develop a planted partition model or stochastic blockmodel for such problems using higher order tensors , present a spectral technique suited for the purpose and study its large sample behavior . the analysis reveals that the planted partition model is consistent for m-uniform hypergraphs for larger values of m , and also the rate of convergence improves for increasing m . our result provides the first theoretical evidence that establishes the importance of m-way affinities .
letizia is a user interface agent that assists a user browsing the world wide web . as the user operates a conventional web browser such as netscape , the agent tracks user behavior and attempts to anticipate items of interest by doing concurrent , autonomous exploration of links from the user 's current position . the agent automates a browsing strategy consisting of a best-first search augmented by heuristics inferring user interest from browsing behavior .
this paper proposes a systematic strategy for the automated implementation of mixed constraint-and input-output-based representations of signal processing systems . examples of the systematic strategy are provided in synthesizing algorithms derived from signal-flow graphs having delay-free loops , as well as in performing automated system inversion . an algorithm that follows the systematic strategy , and which has been deployed online as part of an edx course , is discussed in greater focus . sensitivity analysis of systems designed using the algorithm is provided .
resolving ambiguity in the process of query translation is crucial to cross-language information retrieval when only a bilingual dictionary is available . in this paper we propose a novel approach for query translation disam-biguation , named '' spectral query translation model '' . the proposed approach views the problem of query translation disambiguation as a graph partitioning problem . for a given query , a weighted graph is first created for all possible translations of query words based on the co-occurrence statistics of the translation words . the best translation of the query is then determined by the most strongly connected component within the weighted graph . the proposed approach distinguishes from previous approaches in that the translations of all query words are estimated simultaneously . furthermore , translation probabilities are introduced in the proposed approach to capture the uncertainty in translating queries . empirical studies with trec datasets have shown that the spectral query translation model achieves a relative 20 % -50 % improvement in cross-language information retrieval , compared to other approaches that also exploit word co-occurrence statistics for query translation dis-ambiguation .
in this paper , we introduce a similarity metric for curved shapes that can be described , distinctively , by ordered points . the proposed similarity metric represents a given curve as a point in the deformation space , the direct product of rigid transformation matrices , such that the successive action of the matrices on a fixed starting point reconstructs the full curve . in general , both open and closed curves are represented in the deformation space modulo shape orientation and orientation preserving diffeomorphisms . the use of direct product lie groups to represent curved shapes led to an explicit formula for geodesic curves and the formulation of a similarity metric between shapes by the l 2-norm on the lie algebra . additionally , invariance to reparametrization or estimation of point correspondence between shapes is performed as an intermediate step for computing geodesics . furthermore , since there is no computation of differential quantities on the curves , our direct product lie groups is more robust to local perturbations and needs no pre-smoothing . we compare our similarity metric with the elastic shape metric defined through the square root velocity mapping , and other shape matching approaches .
it has been shown in several recent publications that application of vocal tract normalization is a successful method for improving the accuracy of speaker independent recognisers . we argue that vocal tract normalization can be implemented in the filterbank domain and propose a model to achieve this . we show how the model can be implemented directly in the mfcc domain , where vocal tract normalization may be viewed as a constrained version of maximum likelihood linear regression . the parameter estimates produced by the model are in accord with our ideas about how vocal tract normalization should operate to perform vocal tract normalization . recognition results on a phoneme recognition task are presented which show a small improvement in accuracy .
the national archives of singapore keeps a large volume of historical handwritten documents . one common problem with the archives is that over the years , ink sipped through the pages of these documents such that characters on the reverse side become visible and interfere with the characters on the front side . this paper addresses this problem and develops a novel algorithm to extract clear textual images from the interference . we achieve this by mapping images from both sides of a page such that interfering strokes seen on the front side are matched with the strokes originating from the reverse side so as to achieve a cancellation effect . the resultant image is further subjected to an improved canny edge detection to eliminate remaining background interference . experimental results have confirmed the validity of our proposed method .
a qualitative approach to visually-guided navigation based on the computation of optical ow eld is presented . the qualitative approach is based on the use of two cameras mounted on a mobile robot and with the optical axis directed in opposite directions such that the two visual elds do not overlap -lrb- divergent stereo -rrb- ; range computation is based on the computation of the apparent image speed on images acquired during robot 's motion . an example of reeex-type control of motion , driven by diierential estimation of the ow eld measured by the two eyes , is presented . in particular it is shown how a diicult task like navigating through a funneled corridor with obstacles , is possible without the need for metric depth estimation .
advanced e-applications require comprehensive knowledge about their users ' preferences in order to provide accurate personalized services . in this paper , we propose to learn users ' preferences to product brands from their implicit feedbacks such as their searching and browsing behaviors in user web browsing log data . the user brand preference learning problem is challenge since -lrb- 1 -rrb- the users ' implicit feedbacks are extremely sparse in various product domains ; and -lrb- 2 -rrb- we can only observe positive feedbacks from users ' behaviors . in this paper , we propose a latent factor model to collaboratively mine users ' brand preferences across multiple domains simultaneously . by collective learning , the learning processes in all the domains are mutually enhanced and hence the problem of data scarcity in each single domain can be effectively addressed . on the other hand , we learn our latent factor model with an adaption of the bayesian personalized ranking optimization criterion which is a general learning framework for collaborative filtering from implicit feedbacks . experiments with both synthetic and real world datasets show that our proposed latent factor model significantly outperforms the baselines .
the complexity of existing planners is bounded by the length of the resulting plan , a fact that limits planning to domains with relatively short solutions . we present a novel planning algorithm that uses the causal graph of a domain to decompose it into sub-problems and stores subproblem plans in memory as macros . in many domains , the resulting plan can be expressed using relatively few macros , making it possible to generate exponential length plans in polynomial time . we show that our planning algorithm is complete , and that there exist special cases for which it is optimal and polynomial . experimental results demonstrate the potential of using macros to solve planning domains with long solution plans .
we present a technique for two-stream processing of speech signals for emotion detection . the first stream recognises emotion from acoustic features while the second stream recognises emotion from the semantics of the conversation . a probabilistic measure is derived for each of the individual streams and the emotion category from the two streams is recognised . the output of the two streams is combined to generate a score for a particular emotion category . the confidence level of each stream is used to weigh the scores from the two streams while generating the final score . this technique is extremely significant for call-center data that have some semantics associated with the speech . the proposed technique is evaluated on the ldc corpus and on the real-word call-center data . experiments suggest that use of a two-stream process provides better results than the existing techniques of extracting emotion only from acoustic features .
we present a particle filter-based target tracking algorithm for flir imagery . a dual foreground and background model is proposed for target representation which supports robust and accurate target tracking and size estimation . a novel online feature selection technique is introduced that is able to adaptively select the optimal feature to maximize the tracking confidence . moreover , a coupled particle filtering approach is developed for joint target tracking and feature selection in an unified bayesian estimation framework . the experimental results show that the proposed particle filter-based target tracking algorithm can accurately track poorly-visible targets in flir imagery even with strong ego-motion . the tracking confidence performance is improved when compared to the tracker with a foreground-based target model and without online feature selection .
current and future radio telescopes , in particular the square kilometre array , are envisaged to produce large images -lrb- > 10 8 pixels -rrb- with over 60 db dynamic range . this poses a number of image reconstruction and technological challenges , which will require novel approaches to image reconstruction and design of data processing systems . in this paper , we sketch the limitations of current algorithms by extrapolating their computational requirements to future radio telescopes as well as by discussing their imaging limitations . we discuss a number of potential research directions to cope with these challenges .
discovering and segmenting objects in videos is a challenging task due to large variations of objects in appearances , deformed shapes and cluttered backgrounds . in this paper , we propose to segment objects and understand their visual semantics from a collection of videos that link to each other , which we refer to as semantic co-segmentation . without any prior knowledge on videos , we first extract semantic objects and utilize a tracking-based approach to generate multiple object-like tracklets across the video . each tracklet maintains temporally connected segments and is associated with a predicted category . to exploit rich information from other videos , we collect tracklets that are assigned to the same category from all videos , and co-select tracklets that belong to true objects by solving a submodular function . this function accounts for object properties such as appearances , shapes and motions , and hence facilitates the co-segmentation process . experiments on three video object segmentation datasets show that the proposed algorithm performs favorably against the other state-of-the-art methods .
magnetic resonant coupling -lrb- mrc -rrb- is an efficient method for realizing the near-field wireless power transfer . the use of multiple transmitters -lrb- txs -rrb- each with one coil can be applied to enhance the wpt performance by focusing the magnetic fields from all tx coils in a beam toward the receiver coil , termed as '' magnetic beamforming '' . in this paper , we study the optimal magnetic beamforming for an mrc-wpt system with multiple txs and a single rx . we formulate an optimization problem to jointly design the currents flowing through different txs so as to minimize the total power drawn from their voltage sources , subject to the minimum power required by the rx load as well as the txs ' constraints on the peak voltage and current . for the special case of identical tx resistances and neglecting all txs ' constraints on the peak voltage and current , we show that the optimal current magnitude of each magnetic resonant coupling is proportional to the mutual inductance between its tx coil and the rx coil . in general , the problem is a non-convex quadratically constrained quadratic programming -lrb- qcqp -rrb- problem , which is reformulated as a semidefinite programming problem . we show that its semidefinite relaxation is tight . numerical results show that magnetic beamforming significantly enhances the deliverable power as well as the wpt efficiency over the uncoordinated benchmark scheme of equal current allocation .
variable-length codes -lrb- variable-length codes -rrb- are widely used in media transmission . compared to fixed-length codes , variable-length codes can represent the same message with a lower bit rate , thus having a better compression performance . but inevitably , variable-length codes are very sensitive to transmission errors . in this work , based on the trellis representation for variable-length codes and the bcjr algorithm , we present a variable-length soft-decision decoder utilizing bit-wise channel reliability information and achieving a better error robustness in contrast to hard-decision decoding . given the application of variable-length codes in audio coding showing both source correlation and variable block lengths , a strong dependency of performance is observed for both . therefore , we point out tradeoffs of -lrb- soft-decision -rrb- decoded flcs and variable-length codes depending on quantization bit rate , source correlation , and block length . we find that variable-length codes over awgn channels are only recommended for very low source correlation in combination with very short block lengths and soft-decision decoding .
this paper addresses the problem of learning control policies in very high dimensional state spaces . we propose a linear dimensionality reduction algorithm that discovers predictive projections : projections in which accurate predictions of future states can be made using simple nearest neighbor style learning . the goal of this work is to extend the reach of existing reinforcement learning algorithms to domains where they would otherwise be inap-plicable without extensive engineering of features . the linear dimensionality reduction algorithm is demonstrated on a synthetic pendulum balancing domain , as well as on a robot domain requiring visually guided control .
this paper described our handheld two-way speech translation system for english and iraqi . the focus is on developing a field usable handheld device for speech-to-speech translation . the computation and memory limitations on the handheld impose critical constraints on the asr , smt , and tts components . in this paper we discuss our approaches to optimize these components for the handheld device and present performance numbers from the evaluations that were an integral part of the project . since one major aspect of the transtac program is to build fieldable systems , we spent significant effort on developing an intuitive interface that minimizes the training time for users but also provides useful information such as back translations for translation quality feedback .
automatically produced texts -lrb- e.g. translations or summaries -rrb- are usually evaluated with n-gram based measures such as bleu or rouge , while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes . in this paper we first present an in-depth analysis of the state of the art in order to clarify this issue . after this , we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human-produced references satisfies . these properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process . in addition , the greater the heterogeneity of the measures -lrb- which is measurable -rrb- the higher their combined reliability . these results support the use of heterogeneous measures in order to consolidate text evaluation results .
hybrid intelligence systems combine machine and human intelligence to overcome the shortcomings of existing ai systems . this paper reviews recent research efforts towards developing hybrid intelligence systems focusing on reasoning methods for optimizing access to human intelligence and on gaining comprehensive understanding of humans as helpers of ai systems . it concludes by discussing short and long term research directions .
complexity theory of circuits strongly suggests that deep architectures can be much more efficient -lrb- sometimes exponentially -rrb- than shallow architectures , in terms of computational elements required to represent some functions . deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions . however , until recently it was not clear how to train such deep architectures , since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions . hin-ton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for deep belief networks , a generative model with many layers of hidden causal variables . in the context of the above optimization problem , we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task . our experiments also confirm the hypothesis that the greedy layer-wise unsu-pervised training strategy mostly helps the optimization , by initializing weights in a region near a good local minimum , giving rise to internal distributed representations that are high-level abstractions of the input , bringing better generalization .
one of the most important crowdsourcing topics is to study the effective quality control methods so as to reduce the cost and to guarantee the quality of task processing . as an effective approach , iterative improvement workflow is known to choose the best result from multiple workflows . however , for complex crowdsourcing tasks that consists of a certain number of subtasks under some specific constraints , but can not be split into subtasks to be crowdsourced , the approach merely considers the best workflow without integrating the contributions of all workflows , which potentially results in extra costs for more iterations . in this paper , we propose an assembly model to integrate the best output of subtasks from different workflows . moreover , we devise an efficient iterative method based on pomdp to improve the quality of assembled output . empirical studies confirms the superiority of our proposed assembly model .
we propose a distributed multiresolution representation of sensor network data so that large-scale summaries are readily available by querying a small fraction of sensor nodes , anywhere in the network , and small-scale details are available by querying a larger number of sensors , locally in the region of interest . a global querier -lrb- such as a mobile collector or unmanned aerial vehicle -rrb- can obtain a lossy to lossless representation of the network data , according to the desired resolution . a local querier -lrb- such as a sensor node -rrb- can also obtain either large-scale trends or local details , by querying its immediate neighborhood . we want the encoding to be robust to arbitrary , even time-varying , wireless communication con-nectivity graphs . thus we want to avoid cluster heads or de-terministic hierarchies that are not robust to single points of failure . we propose a randomized encoding which enables both robustness , and distributed computation that does not require long distance coordination or awareness of network connectivity at individual sensors . our distributed encoding algorithm operates on local neighborhoods of the communication graph .
in this paper we introduce maximum variance correction , which finds large-scale feasible solutions to maximum variance unfolding by post-processing embed-dings from any manifold learning algorithm . maximum variance correction increases the scale of mvu embeddings by several orders of magnitude and is naturally parallel . this unprecedented scala-bility opens up new avenues of applications for manifold learning , in particular the use of mvu embeddings as effective heuristics to speed-up a * search . we demonstrate unmatched reductions in search time across several non-trivial a * benchmark search problems and bridge the gap between the man-ifold learning literature and one of its most promising high impact applications .
an approach is proposed for partial tying of states of tied-mixture hidden markov models . to facilitate tying at the sub-state level , the state emission probabilities are constructed in two stages , or equivalently , are viewed as a '' mixture of mixtures of gaussians . '' this paradigm allows , and is complemented with , an optimization technique to seek the best complexity-accuracy tradeoff solution , which jointly exploits gaussian density sharing and sub-state tying . experimental results on the e-set show that the classification error rate is reduced by over 20 % compared to standard gaussian sharing and whole-state tying . the approach is then embedded within the recently developed procedure of combined parameter training and reduction technique . experiments with the overall technique show that the error rate is further reduced by 8 % .
a novel subspace-based speech enhancement scheme based on a criterion of audible noise reduction is considered . masking properties of the human auditory system is used to define the audible noise quantity in the eigen-domain . subsequently , an audible noise reduction scheme is developed based on a signal subspace technique . we derive the eigen-decomposition of the estimated speech autocorrelation matrix with the assumption of white noise and outline the implementation of our proposed audible noise reduction scheme . we further extend the audible noise reduction scheme to the colored noise case . simulation results show that our proposed audible noise reduction scheme outperforms many existing sub-space methods in terms of segmental signal-to-noise ratio , perceptual evaluation of speech quality and informal listening tests .
this paper presents a constraint model for the interpretation of multilinear representations of speech utterances which can provide important fine-grained information for speech recognition applications . the constraint model uses explicit structural constraints specifying overlap and precedence relations between features in both the phonological and the phonetic domains in order to recognise well-formed syllable structures . in the phonological domain , these constraints together form a complete phonotactic description of the language , while in the phonetic domain , the constraints define the internal structure of phonologial features based on phonetic realisations . the constraints are enhanced by a constraint relaxation procedure to cater for underspecified input and allows output representations to be extrapolated based on the phonetic and phonological information contained in the constraints and the rankings which have been assigned to them . this constraint relaxation procedure thus addresses issues of robustness in speech recognition .
this paper provides an alternating optimization algorithm for large-scale matrix rank minimization problems and its parallel implementation on gpu . the matrix rank minimization problem has a lot of important applications in signal processing , and several useful algorithms have been proposed . however most algorithms can not be applied to a large-scale problem because of high computational cost . this paper proposes a null space based algorithm , which provides a low-rank solution without computing inverse matrix nor singular value decomposition . the null space based algorithm can be parallelized easily without any approximation and can be applied to a large-scale problem . numerical examples show that the null space based algorithm provides a low-rank solution efficiently and can be speed up by parallel gpu computing .
we describe a system to learn an object template from a video stream , and localize and track the corresponding object in live video . the template is decomposed into a number of local descriptors , thus enabling detection and low-level tracking in spite of partial occlusion . each local descriptor aggregates contrast invariant statistics -lrb- normalized intensity and gradient orientation -rrb- across scales , in a way that enables matching under significant scale variations . low-level tracking during the training video sequence enables capturing object-specific variability due to the shape of the object , which is encapsulated in the descriptor . salient locations on both the template and the target image are used as hypotheses to expedite matching .
we study the demodulation problem in time division multiple access -lrb- tdma -rrb- wireless asynchronous transfer mode -lrb- atm -rrb- networks , where rician flat fading channels are considered . a linear interpolation with decision feedback combined with a modified version of the self-organizing-map demodulator is proposed for such a system . we obtain the training sequence by exploiting medium access control and data link control protocols such that a semi-blind adaptive demodulator is implemented . simulation results show that lidf-som obtains 0.4 − 1.0 db gain over rician fading channels as compared to lidf alone .
in this paper , we propose a maximum likelihood -lrb- ml -rrb- based frame selection approach . a fixed frame rate adopted in most state-of-the-art speech recognition systems can face some problems , such as accidentally meeting noisy frames , assigning the same importance to each frame , and pitch asynchronous representation . as an attempt to avoid those problems , our approach selects reliable frames from a fine resolution along the time axis . in a phoneme recognition task , we show that significant improvements are achieved with the frame selection approach comparing to a system with a fixed frame rate .
in this paper we contrast a traditional approach to semantic parsing for natural language understanding applications in which a single parser captures a whole application domain , with an alternative approach consisting of a collection of smaller parsers , each able to handle only a portion of the domain . we implement this topic-specific parsing strategy by fragmenting the training corpus into subject specific subsets and developing from each subset a corresponding subject parser . we demonstrate this procedure on the darpa communicator task , and we observe that given an appropriate smoothing mechanism to overcome data sparseness , the set of subject-specific parsers performs as effectively -lrb- in accuracy terms -rrb- as the original parser . we present experiments both under supervised and unsupervised subject selection modes .
we present a speaker clustering method for conversational speech recordings that contain short utterances from multiple speakers . the proposed speaker clustering method represents a speech segment with a vector of vq code frequencies and uses a cosine between two vectors as their similarity measure . the clustering is performed by a spectral clustering algorithm with cluster number estimation based on an eigen structure of the similarity matrix . we conducted experiments on five test sets with different utterance length distributions to compare the proposed speaker clustering method with the conventional approach based on a hierarchical agglomerative clustering using bic stopping criterion . the results show that the proposed speaker clustering method significantly outperforms the conventional one in speaker diarization error rate and purity metrics .
we present a method for computing the 3d motion of articulated models from 2d correspondences . an iterative batch algorithm is proposed which estimates the maximum aposteriori trajectory based on the 2d measurements subject to a number of constraints . these include -lrb- i -rrb- kinematic constraints based on a 3d kinematic model , -lrb- ii -rrb- joint angle limits , -lrb- iii -rrb- dynamic smoothing and -lrb- iv -rrb- 3d key frames which can be specified the user . the framework handles any variation in the number of constraints as well as partial or missing data . this method is shown to obtain favorable reconstruction results on a number of complex human motion sequences .
we explore the use of adaptation transforms employed in speech recognition systems as features for speaker recognition . this approach is attractive because , unlike standard frame-based cepstral speaker recognition models , it normalizes for the choice of spoken words in text-independent speaker verification . affine transforms are computed for the gaussian means of the acoustic models used in a recognizer , using maximum likelihood linear regression . the high-dimensional vectors formed by the transform coefficients are then modeled as speaker features using support vector machines . the resulting speech recognition systems is competitive , and in some cases significantly more accurate , than state-of-the-art cepstral gaussian mixture and svm systems . further improvements are obtained by combining baseline and mllr-based systems .
human action and role recognition play an important part in complex event understanding . state-of-the-art methods learn action and role models from detailed spatio temporal annotations , which requires extensive human effort . in this work , we propose a method to learn such models based on natural language descriptions of the training videos , which are easier to collect and scale with the number of actions and roles . there are two challenges with using this form of weak supervision : first , these descriptions only provide a high-level summary and often do not directly mention the actions and roles occurring in a video . second , natural language descriptions do not provide spa-tio temporal annotations of actions and roles . to tackle these challenges , we introduce a topic-based semantic re-latedness measure between a video description and an action and role label , and incorporate topic-based semantic re-latedness measure into a posterior regularization objective . our event recognition system based on these action and role models matches the state-of-the-art method on the trecvid-med11 event kit , despite weaker supervision .
we extend the centering model for the resolution of intia-sentential anaphora and specify how to handle complex sentences . an empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence .
we present and study a distributed optimization algorithm by employing a stochas-tic dual coordinate ascent method . stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochas-tic gradient descent methods in optimizing regularized loss minimization problems . it still lacks of efforts in studying stochastic dual coordinate ascent methods in a distributed framework . we make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network , with an analysis of the tradeoff between computation and communication . we verify our analysis by experiments on real data sets . moreover , we compare the proposed distributed optimization algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing svms in the same distributed framework , and observe competitive performances .
this paper proposes tracking forecast memories as a novel method for implementing re-randomization and de-correlation of stochastic bit streams in stochastic channel de-coders . we show that tracking forecast memories are able to achieve decoding performance similar to that of the previous methods in the literature -lrb- i.e. , edge memories or ems -rrb- , but they exhibit much lower hardware complexity . tracking forecast memories significantly reduce the area requirements of asic implementations of stochastic de-coders .
we describe a family of embedding algorithms that are based on nonparametric estimates of mutual information . using parzen window estimates of the distribution in the joint -lrb- input , embedding -rrb- - space , we derive a mi-based objective function for dimensionality reduction that can be optimized directly with respect to a set of latent data representatives . various types of supervision signal can be introduced within the framework by replacing plain mi with several forms of conditional mi . examples of the semi - -lrb- un -rrb- supervised algorithms that we obtain this way are a new model for manifold alignment , and a new type of embedding method that performs ` conditional dimensionality reduction ' .
we introduce a new method that characterizes typical local image features -lrb- e.g. , sift -lsb- 9 -rsb- , phase feature -lsb- 3 -rsb- -rrb- in terms of their distinctiveness , detectability , and robustness to image deformations . this is useful for the task of classifying local image features in terms of those three properties . the importance of this classifying local image features for a recognition task using local features is as follows : a -rrb- reduce the recognition time due to a smaller number of features present in the test image and in the database of model features ; b -rrb- improve the recognition time since only the most useful features for the recognition task are kept in the model database ; and c -rrb- increase the scalability of the recognition task given the smaller number of features per model . a discriminant classifier is trained to select well behaved feature points . a regression network is then trained to provide quantitative models of the detection distributions for each selected feature point . it is important to note that both the classifier and the regression network use image data alone as their input . experimental results show that the use of these trained regression network not only improves the performance of our recognition task , but regression network also significantly reduces the computation time for the recognition task .
in this paper we propose a combined scheme of linear prediction analysis for feature extraction along with linear projection methods for feature reduction followed by known pattern recognition methods on the purpose of discriminating between normal and pathological voice samples . two different cases of speech under vocal fold pathology are examined : vocal fold paralysis and vocal fold edema . three known classifiers are tested and compared in both cases , namely the fisher linear discriminant , the ã-nearest neighbor classifier , and the nearest mean classifier . the performance of each classifier is evaluated in terms of the probabilities of false alarm and detection or the receiver operating characteristic . the datasets used are part of a database of disordered speech developed by massachusetts eye and ear infirmary . the experimental results indicate that vocal fold paralysis and edema can easily be detected by any of the aforementioned classifiers .
this paper describes a technique to ghosts cancellation without a ghost cancellation reference signal . the existing vertical edges in tv image are used to estimate the channel characteristics . with the estimating results , the coefficients of the equalizer are updated . in this paper , a method to speed up the convergence is given . the equalizer structure for cancelling all kinds of ghosts is discussed too . a new improved solution is achieved by interpreting the tv synchronisation signal as a genuine edge .
vector-based information retrieval methods such as the vector space model , latent semantic indexing , and the generalized vector space model represent both queries and documents by high-dimensional vectors learned from analyzing a training corpus of text . vector space model scales well to large collections , but can not represent term -- term correlations , which prevents vector space model from being used in translingual retrieval . vector space model and lsi can represent term -- term correlations , but do not scale well to very large retrieval collections . we present a novel method we call approximate dimension equalization -lrb- ade -rrb- that combines ideas from vector space model , lsi , and vector space model to produce a method that performs well on large collections , scales well computationally , and can represent term -- term correlations . we compare the performance of ade to the other methods on both large and small collections of both mono-lingual and bilingual text . ade outperforms all other methods on large bilingual collections , and performs close to the best in all other cases .
the smart vision chip has a large potential for application in general purpose high speed image processing systems . in order to fabricate smart vision chips including photo detector compactly , we have proposed the application of three dimensional lsi technology for smart vision chips . three dimensional lsi technology has great potential to realize new neuromorphic systems inspired by not only the biological function but also the biological structure . in this paper , we describe our three dimensional lsi technology for smart vision chips and the design of smart vision chips .
the lack of training data is a common challenge in many machine learning problems , which is often tackled by semi-supervised learning methods or transfer learning methods . the former requires unlabeled images from the same distribution as the labeled ones and the latter leverages labeled images from related homogenous tasks . however , these restrictions often can not be satisfied . to address this , we propose a novel robust and discriminative self-taught learning approach to utilize any unlabeled data without the above restrictions . our new robust and discriminative self-taught learning approach employs a robust loss function to learn the dictionary , and enforces the structured sparse regularization to automatically select the optimal dictionary basis vectors and incorporate the supervision information contained in the labeled data . we derive an efficient iterative algorithm to solve the optimization problem and rigorously prove its convergence . promising results in extensive experiments have validated the proposed robust and discriminative self-taught learning approach .
how can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks ? this paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model . the clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the stochastic recurrent neural networks 's posterior distribution . by retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path , like a state space model , we improve the state of the art results on the blizzard and timit speech modeling data sets by a large margin , while achieving comparable performances to competing methods on polyphonic music modeling .
convolutional neural networks have proved very successful in image recognition , thanks to their tolerance to small translations . convolutional neural networks have recently been applied to speech recognition as well , using a spectral representation as input . however , in this case the translations along the two axes -- time and frequency -- should be handled quite differently . so far , most authors have focused on convolution along the frequency axis , which offers invariance to speaker and speaking style variations . other researchers have developed a different network architecture that applies time-domain convolution in order to process a longer time-span of input in a hierarchical manner . these two convolutional neural networks have different background motivations , and both offer significant gains over a standard fully connected network . here we show that the two network architecture can be readily combined , like their advantages . with the combined model we report an error rate of 16.7 % on the timit phone recognition task , a new record on this dataset .
this paper presents a status quo of an ongoing research study of collocations -- an essential linguistic phenomenon having a wide spectrum of applications in the field of natural language processing . the core of the work is an empirical evaluation of a comprehensive list of automatic collocation extraction methods using precision-recall measures and a proposal of a new approach integrating multiple basic methods and statistical classification . we demonstrate that combining multiple independent techniques leads to a significant performance improvement in comparison with individual basic methods .
the behavior of a complex system often depends on parameters whose values are unknown in advance . to operate effectively , an autonomous agent must actively gather information on the parameter values while progressing towards its goal . we call this problem parameter elicitation . partially observable markov decision processes provide a principled framework for such uncertainty planning tasks , but they suffer from high computational complexity . however , partially observable markov decision processes for parameter elicitation often possess special structural properties , specifically , factorization and symmetry . this work identifies these properties and exploits partially observable markov decision processes for efficient solution through a factored belief representation . the experimental results show that our new partially observable markov decision processes outperform sarsop and momdp , two of the fastest general-purpose partially observable markov decision processes available , and can handle significantly larger problems .
kernel embedding of distributions has led to many recent advances in machine learning . however , latent and low rank structures prevalent in real world distributions have rarely been taken into account in this setting . furthermore , no prior work in kernel embedding literature has addressed the issue of robust embedding when the latent and low rank information are misspecified . in this paper , we propose a hierarchical low rank decomposition of kernels embeddings which can exploit such low rank structures in data while being robust to model misspeci-fication . we also illustrate with empirical evidence that the estimated low rank embeddings lead to improved performance in density estimation .
in this paper , we propose a new deep hashing approach to learn compact binary codes for large scale visual search . unlike most existing binary codes learning methods which seek a single linear projection to map each sample into a binary vector , we develop a deep neural network to seek multiple hierarchical non-linear transformations to learn these binary codes , so that the nonlinear relationship of samples can be well exploited . our deep hashing approach is learned under three constraints at the top layer of the deep neural network : 1 -rrb- the loss between the original real-valued feature descrip-tor and the learned binary vector is minimized , 2 -rrb- the binary codes distribute evenly on each bit , and 3 -rrb- different bits are as independent as possible . to further improve the dis-criminative power of the learned binary codes , we extend dh into supervised dh by including one discrimi-native term into the objective function of dh which simultaneously maximizes the inter-class variations and minimizes the intra-class variations of the learned binary codes . experimental results show the superiority of the proposed deep hashing approach over the state-of-the-arts .
we present a novel method for the discovery and statistical representation of motion patterns in a scene observed by a static camera . related methods involving learning of patterns of activity rely on trajectories obtained from object detection and tracking systems , which are unreliable in complex scenes of crowded motion . we propose a gaussian mixture model model representation of salient patterns of optical flow , and present an algorithm for learning these patterns from dense optical flow in a hierarchical , unsupervised fashion . using low level cues of noisy optical flow , k-means is employed to initialize a gaussian mixture model for temporally segmented clips of video . the components of this gaussian mixture model are then filtered and instances of motion patterns are computed using a simple motion model , by linking components across space and time . motion patterns are then initialized and membership of instances in different motion patterns is established by using kl divergence between gaussian mixture model distributions of pattern instances . finally , a pixel level representation of motion patterns is proposed by deriving conditional expectation of optical flow . results of extensive experiments are presented for multiple surveillance sequences containing numerous patterns involving both pedestrian and vehicular traffic .
in the form of topic discussions , users interact with each other to share knowledge and exchange information in online forums . modeling the evolution of topic discussion reveals how information propagates on internet and can thus help understand sociological phenomena and improve the performance of applications such as recommendation systems . in this paper , we argue that a user 's participation in topic discussions is motivated by either her friends or her own preferences . inspired by the theory of information flow , we propose dynamic topic discussion models by mining influential relationships between users and individual preferences . reply relations of users are exploited to construct the fundamental influential social network . the property of discussed topics and time lapse factor are also considered in our dynamic topic discussion models . furthermore , we propose a novel measure called participationrank to rank users according to how important they are in the social network and to what extent they prefer to participate in the discussion of a certain topic . the experiments show our model can simulate the evolution of topic discussions well and predict the tendency of user 's participation accurately .
an efficient probabilistic approach to collaborative filtering with implicit feedback , based on modelling the user 's item selection process . • tree-structured distributions over items for scalability . • a principled and efficient algorithm for learning effective item trees from data . • a fix for the standard evaluation protocol for implicit feedback models , addressing its unrealistic assumptions .
many problems have a huge state space and no good heuristic to order moves so as to guide the search toward the best positions . random games can be used to score positions and evaluate their interest . random games can also be improved using random games to choose a move to try at each step of a game . nested monte-carlo search addresses the problem of guiding the search toward better states when there is no available heuristic . nested monte-carlo search uses nested levels of random games in order to guide the search . the nested monte-carlo search is studied theoretically on simple abstract problems and applied successfully to three different games : morpion solitaire , samegame and 16x16 sudoku .
good sparse approximations are essential for practical inference in gaussian processes as the computational cost of exact methods is prohibitive for large datasets . the fully independent training conditional and the variational free energy approximations are two recent popular methods . despite superficial similarities , these approximations have surprisingly different theoretical properties and behave differently in practice . we thoroughly investigate the two methods for regression both analytically and through illustrative examples , and draw conclusions to guide practical application .
we present a novel approach to non-rigid structure from motion -lrb- nrsfm -rrb- from an orthographic video sequence , based on a new interpretation of the problem . existing approaches assume the object shape space is well-modeled by a linear subspace . our approach only assumes that small neighborhoods of shapes are well-modeled with a linear subspace . this constrains the shapes to belong to a man-ifold of dimensionality equal to the number of degrees of freedom of the object . after showing that the problem is still overconstrained , we present a solution composed of a novel initialization algorithm , followed by a robust extension of the locally smooth manifold learning algorithm tailored to the nrsfm problem . we finally present some test cases where the linear basis method fails -lrb- and is actually not meant to work -rrb- while the proposed approach is successful .
we present a novel framework , called grab -lrb- graphical models with overlapping blocks -rrb- , to capture densely connected components in a network estimate . grab takes as input a data matrix of p variables and n samples and jointly learns both a network of the p variables and densely connected groups of variables -lrb- called ` blocks ' -rrb- . grab has four major novelties as compared to existing network estimation methods : 1 -rrb- it does not require blocks to be given a priori . 2 -rrb- blocks can overlap . 3 -rrb- it can jointly learn a network structure and overlapping blocks . 4 -rrb- it solves a joint optimization problem with the block coordinate descent method that is convex in each step . we show that grab reveals the underlying network structure substantially better than four state-of-the-art competitors on synthetic data . when applied to cancer gene expression data , grab outperforms its competitors in revealing known functional gene sets and potentially novel cancer driver genes .
we consider stochastic multi-armed bandits where the expected reward is a unimodal function over partially ordered arms . this important class of problems has been recently inves-the set of arms is either discrete , in which case arms correspond to the vertices of a finite graph whose structure represents similarity in rewards , or continuous , in which case arms belong to a bounded interval . for discrete unimodal bandits , we derive asymptotic lower bounds for the regret achieved under any algorithm , and propose osub , an algorithm whose regret matches this lower bound . our algorithm optimally exploits the unimodal structure of the problem , and surprisingly , its asymptotic regret does not depend on the number of arms . we also provide a regret upper bound for osub in non-stationary environments where the expected rewards smoothly evolve over time . the analytical results are supported by numerical experiments showing that osub performs significantly better than the state-of-the-art algorithms . for continuous sets of arms , we provide a brief discussion . we show that combining an appropriate discretization of the set of arms with the ucb algorithm yields an order-optimal regret , and in practice , outperforms recently proposed algorithms designed to exploit the unimodal structure .
we propose a generalized gaussian process model , which is a unifying framework that encompasses many existing gaussian process models , such as gp regression , classification , and counting . in the generalized gaussian process model , the observation likelihood of the gaussian process models is itself parameterized using the exponential family distribution . by deriving approximate inference algorithms for the generalized gp model , we are able to easily apply the same generalized gaussian process model to all other gaussian process models . novel gaussian process models are created by changing the parameterization of the likelihood function , which greatly simplifies their creation for task-specific output domains . we also derive a closed-form efficient taylor approximation for inference on the generalized gaussian process model , and draw interesting connections with other model-specific closed-form approximations . finally , using the generalized gaussian process model , we create several new gaussian process models and show their efficacy in building task-specific gp models for computer vision .
many clustering algorithms only find one clustering solution . however , data can often be grouped and interpreted in many different ways . this is particularly true in the high-dimensional setting where different subspaces reveal different possible groupings of the data . instead of committing to one clustering solution , here we introduce a novel method that can provide several non-redundant clustering solutions to the user . our approach simultaneously learns non-redundant subspaces that provide multiple views and finds a clustering solution in each view . we achieve this by augmenting a spectral clustering objective function to incorporate dimensionality reduction and multiple views and to penalize for redundancy between the views .
this paper addresses view-invariant object detection and pose estimation from a single image . while recent work fo-cuses on object-centered representations of point-based object features , we revisit the viewer-centered framework , and use image contours as basic features . given training examples of arbitrary views of an object , we learn a sparse object model in terms of a few view-dependent shape templates . the sparse object model are jointly used for detecting object occurrences and estimating their 3d poses in a new image . instrumental to this is our new mid-level feature , called bag of boundaries , aimed at lifting from individual edges toward their more informative summaries for identifying object boundaries amidst the background clutter . in inference , bobs are placed on deformable grids both in the image and the sparse object model , and then matched . this is formulated as a convex optimization problem that accommodates invariance to non-rigid , locally affine shape deformations . evaluation on benchmark datasets demonstrates our competitive results relative to the state of the art .
to judge how much a pair of words -lrb- or texts -rrb- are semantically related is a cognitive process . however , previous algorithms for computing semantic relatedness are largely based on co-occurrences within textual windows , and do not actively leverage cognitive human perceptions of relatedness . to bridge this perceptional gap , we propose to utilize free association as signals to capture such human perceptions . however , free association , being manually evaluated , has limited lexical coverage and is inherently sparse . we propose to expand lexical coverage and overcome sparseness by constructing an association network of terms and concepts that combines signals from free association norms and five types of co-occurrences extracted from the rich structures of wikipedia . our evaluation results validate that simple algorithms on this network give competitive results in computing semantic re-latedness between words and between short texts .
a number of computer vision problems such as human age estimation , crowd density estimation and body/face pose -lrb- view angle -rrb- estimation can be formulated as a regression problem by learning a mapping function between a high dimensional vector-formed feature input and a scalar-valued output . such a regression problem is made difficult due to sparse and imbalanced training data and large feature variations caused by both uncertain viewing conditions and intrinsic ambiguities between observable visual features and the scalar values to be estimated . encouraged by the recent success in using attributes for solving computer vision problems with sparse training data , this paper introduces a novel cumulative attribute concept for learning a regression model when only sparse and imbalanced data are available . more precisely , low-level visual features extracted from sparse and imbalanced image samples are mapped onto a cumulative attribute space where each dimension has clearly defined semantic interpretation -lrb- a label -rrb- that captures how the scalar output value -lrb- e.g. age , people count -rrb- changes continuously and cumulatively . extensive experiments show that our cumulative attribute concept gains notable advantage on accuracy for both age estimation and crowd counting when compared against conventional regression models , especially when the labelled training data is sparse with imbalanced sampling .
the paper presents a system that recognizes humans interacting with objects . we delineate a new framework that integrates object recognition , motion estimation , and semantic-level recognition for the reliable recognition of hierarchical human-object interactions . the framework is designed to integrate recognition decisions made by each component , and to probabilistically compensate for the failure of the components with the use of the decisions made by the other components . as a result , human-object interactions in an airport-like environment , such as ' a person carrying a baggage ' , ' a person leaving his/her baggage ' , or ' a person snatching another 's baggage ' , are recognized . the experimental results show that not only the performance of the final activity recognition is superior to that of previous approaches , but also the accuracy of the object recognition and the motion estimation increases using feedback from the semantic layer . several real examples illustrate the superior performance in recognition and semantic description of occurring events .
in online display advertising , state-of-the-art click through rate -lrb- ctr -rrb- prediction algorithms rely heavily on historical information , and they work poorly on growing number of new ads without any historical information . this is known as the the cold start problem . for image ads , current state-of-the-art systems use handcrafted image features such as multimedia features and sift features to capture the attractiveness of ads . however , these handcrafted features are task dependent , inflexible and heuristic . in order to tackle the cold start problem in image display ads , we propose a new feature learning architecture to learn the most dis-criminative image features directly from raw pixels and user feedback in the target task . the proposed feature learning architecture is flexible and does not depend on human heuristic . extensive experiments on a real world dataset with 47 billion records show that our feature learning architecture outperforms existing handcrafted features significantly , and feature learning architecture can extract discrimina-tive and meaningful features .
the idea of '' nugget pyramids '' has recently been introduced as a refinement to the nugget-based methodology used to evaluate answers to complex questions in the trec qa tracks . this paper examines data from the 2006 evaluation , the first large-scale deployment of the nugget pyramids scheme . we show that this method of combining judgments of nugget importance from multiple assessors increases the stability and dis-criminative power of the evaluation while introducing only a small additional burden in terms of manual assessment . we also consider an alternative method for combining assessor opinions , which yields a distinction similar to micro-and macro-averaging in the context of classification tasks . while the two approaches differ in terms of underlying assumptions , their results are nevertheless highly correlated .
this paper focuses on the automatic extraction of beat structure from a musical piece . a novel statistical approach to modeling beat sequences based on the application of hidden markov models is introduced . the resulting beat labels are obtained by running the viterbi decoder and subsequent lattice rescoring . for the observation vectors we propose a new feature set that is based on the impulsive and harmonic components of the reassigned spectrogram . different components of observation vectors have been investigated for their efficiency . the main advantage of the proposed approach is the absence of imposed deterministic rules . all the parameters are learned from the training data , and the experimental results show the efficiency of the proposed schema .
this paper deals with the problem of automatic target classi - ¿ cation or through-the-wall radar imaging . the proposed scheme considers stationary objects in enclosed structures and works on the sar image rather than the raw data . it comprises segmentation , feature extraction based on su-perquadrics , and classi ¿ cation . we present a recursive splitting tree to obtain optimum parameters for feature extraction . support vector machines and nearest neighbor classi ¿ ers are then applied to successfully classify among different indoor targets . the classi ¿ cation methods are tested and evaluated using real data generated from synthetic aperture through-the-wall radar imaging experiments .
in this paper we describe research on summarizing conversations in the meetings and emails domains . we introduce a conversation summarization system that works in multiple domains utilizing general conversational features , and compare our results with domain-dependent systems for meeting and email data . we find that by treating meetings and emails as conversations with general conversational features in common , we can achieve competitive results with state-of-the-art systems that rely on more domain-specific features .
the blind beamforming method for constant modulus signals based on relevance vector machine is proposed . the proposed blind beamforming method is obtained by incorporating the constant modulus algorithm - like error function into the conventional blind beamforming method . the blind beamforming method formulates the parameters of beamfomer by exploiting a probabilistic bayesian learning procedure with assumption of gaussian prior for parameters . the simulation results show that the proposed blind beamforming method can restore the desired signals with crowded interference signals .
binaural presentation of x.y sound is usually performed using virtual audio principles -- that is , by attempting to virtually reproduce the setup of the x+y loudspeakers in the reference room conſg-uration . the computational cost of such binaural presentation of x.y sound is linear in the number of channels in the x.y setup . we present a novel scheme that computes , offline , a spatio-temporal representation of the sound ſeld in the listening area and store it as a multipole expansion . during head-tracked playback , the binaural signal is obtained by evaluating the multipole expansion at the ear position corresponding to the current user pose , resulting in a ſxed playback cost . the representation is further extended to incorporate individualized hrtfs at no additional cost . simulation results are presented .
in the tandem approach to modeling the acoustic signal , a neural-net preprocessor is first discriminatively trained to estimate posterior probabilities across a phone set . these are then used as feature inputs for a conventional hidden markov model -lrb- hmm -rrb- based speech recognizer , which relearns the associations to sub-word units . in this paper , we apply the tandem approach to the data provided for the first speech in noisy environments -lrb- spine1 -rrb- evaluation conducted by the naval research laboratory -lrb- nrl -rrb- in august 2000 . in our previous experience with the etsi aurora noisy digits -lrb- a small-vocabulary , high-noise task -rrb- the tandem approach achieved error-rate reductions of over 50 % relative to the hmm baseline . for spine1 , a larger task involving more spontaneous speech , we find that , when context-independent models are used , the tandem approach continue to result in large reductions in word-error rates relative to those achieved by systems using standard mfc or plp features . however , these improvements do not carry over to context-dependent models . this may be attributable to several factors which are discussed in the paper .
this paper introduces a variational formulation for image denoising based on a quadratic function over variational formulation of variable bandwidth . these variational formulation are scale adaptive and reflect spatial and photometric similarities between pixels . the bandwidth of the variational formulation is observation-dependent towards improving the accuracy of the reconstruction process and is constrained to be locally smooth . we analyze the evolution of the noise model form the raw space to the rgb one , by propagating noise model over the image formation process . the experimental results demonstrate that the use of a variable bandwidth approach and an image intensity dependent noise variance ensures better restoration quality .
the following two important features of human experts ' knowledge are not realized by most evolutionary algorithms : one is that it is various and the other is that the amount of knowledge , including infrequently used knowledge , is large . to imitate these features , we introduce an activation value for individuals and a new variation-making operator , splitting , both of which are inspired by ecological systems . this algorithm is applied to the game of go and a large amount of knowledge evaluated as appropriate by a human expert is acquired . various kinds of go knowledge may be acquired such as patterns , sequences of moves , and go maxims , part of which has already been realized .
we propose a semi-supervised approach to solve the task of facial expression recognition in 2d face images using recent ideas in deep learning for handling the factors of variation present in data . an emotion classification algorithm should be both robust to -lrb- 1 -rrb- remaining variations due to the pose of the face in the image after centering and alignment , -lrb- 2 -rrb- the identity or morphology of the face . in order to achieve this invariance , we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both -lrb- 1 -rrb- and -lrb- 2 -rrb- . we address -lrb- 1 -rrb- by using a multi-scale contractive convolutional network in order to obtain invariance to translations of the facial traits in the image . using the feature representation produced by the multi-scale contractive convolutional network , we train a contrac-tive discriminative analysis feature extractor , a novel variant of the contractive auto-encoder , designed to learn a representation separating out the emotion-related factors from the others -lrb- which mostly capture the subject identity , and what is left of pose after the multi-scale contractive convolutional network -rrb- . this semi-supervised approach beats the state-of-the-art on a recently proposed dataset for facial expression recognition , the toronto face database , moving the state-of-art accuracy from 82.4 % to 85.0 % , while the multi-scale contractive convolutional network and cda improve accuracy of a standard contractive auto-encoder by 8 % .
in this paper , a kernel-based invariant subspace detection method is proposed for small target detection of hyperspectral images . the kernel-based invariant subspace detection method combines kernel principal component analysis and linear mixture model . the kernel principal component analysis is used to describe each pixel in the hyperspectral images as mixture of target , background and noise . the kernel principal component analysis is used to build subspaces of target and background . a generalized likelihood ratio test is used to detect whether each pixel in hyperspectral image includes target . the numerical experiments are performed on aviris hyperspectral data with 126 bands . the experimental results show the effectiveness of the proposed kernel-based invariant subspace detection method and prove that this kernel-based invariant subspace detection method can commendably overcome spectral variability in the hyperspectral target detection , and kernel-based invariant subspace detection method has good ability to separate target from background .
while image registration has been studied in different areas of computer vision , aligning images depicting different scenes remains a challenging problem , closer to recognition than to image matching . analogous to optical flow , where an image is aligned to its temporally adjacent frame , we propose sift flow , a method to align an image to its neighbors in a large image collection consisting of a variety of scenes . for a query image , histogram intersection on a bag-of-visual-words representation is used to find the set of nearest neighbors in the database . the sift flow algorithm then consists of matching densely sampled sift flow between the two images , while preserving spatial discontinu-ities . the use of sift flow allows robust matching across different scene/object appearances and the discontinuity-preserving spatial model allows matching of objects located at different parts of the scene . experiments show that the proposed approach is able to robustly align complicated scenes with large spatial distortions . we collect a large database of videos and apply the sift flow algorithm to two applications : -lrb- i -rrb- motion field prediction from a single static image and -lrb- ii -rrb- motion synthesis via transfer of moving objects .
as the popularity of community question answering increases , spam-ming activities also picked up in numbers and variety . on cqa sites , spammers often pretend to ask questions , and select answers which were published by their partners or themselves as the best answers . these fake best answers can not be easily detected by neither existing methods nor common users . in this paper , we address the issue of detecting spammers on cqa sites . we formulate the task as an optimization problem . social information is incorporated by adding graph regulariza-tion constraints to the text-based predic-tor . to evaluate the proposed approach , we crawled a data set from a cqa portal . experimental results demonstrate that the proposed method can achieve better performance than some state-of-the-art methods .
a new characteristic waveform decomposition method based on wavelets is proposed for the waveform interpolation paradigm . in waveform interpolation paradigm , pitch-cycle waveforms are filtered in the evolution domain to decompose the signal into two waveform surfaces , one characterising voiced speech and a second representing unvoiced speech . the slow roll-off of fir filters leads , however , to a significant interrelationship between the decomposed surfaces . here we present the pitch synchronous wavelet transform as an alternative decomposition mechanism . filtering is again performed in the evolutionary waveform domain , producing characteristic surfaces at several resolutions . this multi-scale characterisation leads to more flexible quantisation of parameters , especially at higher rates than waveform interpolation paradigm 's 2.4 kb/s . fir filters are replaced in the wavelet filter bank by causal , stable iir filters which achieve significant delay reductions over their fir counterparts . furthermore , iir filters track the dynamic aspects of the evolutionary surfaces faster , overcoming problems existing in the current wi decomposition .
we describe a monte carlo method for model-space noise adaptation of gaussian mixture models . this monte carlo method combines a single-gaussian noise model with the gmm speech model to produce an adapted monte carlo method . monte carlo method is similar to parallel model combination or model-space joint , except that monte carlo method applies to spliced and projected mfcc features rather than to mfcc plus dynamic features . we demonstrate the necessity of re-estimating the noise using both the silence and speech frames rather than just estimating monte carlo method from silence frames , and obtain improvements on a matched test set without added noise using a system that includes all standard adaptation techniques .
we propose a novel real-time adaptative localization approach for multiple sources using a circular array , in order to suppress the lo-calization ambiguities faced with linear arrays , and assuming a weak sound source sparsity which is derived from blind source separation methods . our proposed real-time adaptative localization approach performs very well both in simulations and in real conditions at 50 % real-time .
we present an unsupervised method for word sense disambiguation that exploits translation correspondences in parallel corpora . the unsupervised method takes advantage of the fact that cross-language lexicalizations of the same concept tend to be consistent , preserving some core element of its semantics , and yet also variable , reeecting diier-ing translator preferences and the in-uence of context . working with parallel corpora introduces an extra complication for evaluation , since it is dif-cult to nd a corpus that is both sense tagged and parallel with another language ; therefore we use pseudo-translations , created by machine translation systems , in order to make possible the evaluation of the unsupervised method against a standard test set . the results demonstrate that word-level translation correspondences are a valuable source of information for sense disam-biguation .
when designing a device , the final product of the design process is usually considered to be a physical specification of a device . however , the design of the causal mechanism underlying the physical specification , i.e. how the device is intended to work to achieve its function , is a product just as important as the physical specification , if not more . capturing this knowledge of causal mechanism is necessary in order to understand the physical specification of the device as well as to evaluate and refine the specifications during the design process . despite the importance of such knowledge , existing design process do not support its explicit representation or manipulation . we describe a design support system under development in which knowledge of both the causal mechanism and the physical structure of a device being designed is explicitly represented and manipulated . the design support system allows the designer to provide functional specifications at various levels of abstraction in a language called cfrl -lrb- causal functional representation language -rrb- . the cfrl -lrb- causal functional representation language -rrb- acquired from the user enables the design support system to evaluate the physical specification as design support system is being developed in order to provide useful feedback to the designer . furthermore , functional specifications provide an important basis for recording the engineer 's design rationale .
convolutional neural networks -lrb- cnn -rrb- have recently shown outstanding image classification performance in the large-scale visual recognition challenge . the success of large-scale visual recognition challenge is attributed to their ability to learn rich mid-level image representations as opposed to hand-designed low-level features used in other image classification methods . learning cnns , however , amounts to estimating millions of parameters and requires a very large number of annotated image samples . this property currently prevents application of large-scale visual recognition challenge to problems with limited training data . in this work we show how image representations learned with large-scale visual recognition challenge on large-scale annotated datasets can be efficiently transferred to other visual recognition tasks with limited amount of training data . we design a method to reuse layers trained on the large-scale visual recognition challenge to compute mid-level image representation for images in the pascal voc dataset . we show that despite differences in image statistics and tasks in the two datasets , the image representations leads to significantly improved results for object and action classification , outperforming the current state of the art on pascal voc 2007 and 2012 datasets . we also show promising results for object and action classification .
applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels . we present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution . like convolutional neural networks , the proposed recurrent neural network model has a degree of translation invariance built-in , but the amount of computation recurrent neural network model performs can be controlled independently of the input image size . while the recurrent neural network model is non-differentiable , recurrent neural network model can be trained using reinforcement learning methods to learn task-specific policies . we evaluate our recurrent neural network model on several image classification tasks , where recurrent neural network model significantly outperforms a convolutional neural network baseline on cluttered images , and on a dynamic visual control problem , where recurrent neural network model learns to track a simple object without an explicit training signal for doing so .
this paper proposes new algorithms to compute the sense similarity between two units -lrb- words , phrases , rules , etc. -rrb- from parallel corpora . the sense similarity scores are computed by using the vector space model . we then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of statistical machine translation rule pairs . similarity scores are used as additional features of the translation model to improve statistical machine translation performance . significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system .
english and japanese are quite different languages both phoneti-cally and linguistically and it is often very difficult for japanese students to master english pronunciation . to help students improve their pronunciation proficiency , a japanese national project of '' advanced utilization of multimedia for education '' has started in 2000 and under this project , a large database of english words and sentences read by 200 japanese students was built mainly for call system development . this paper describes a corpus-based analysis and comparison of american english -lrb- ae -rrb- and japanese english by using wsj database and the new je database . here , hidden markov models , which are widely-used acoustic modeling techniques of speech recognition , were firstly made for individual phonemes in the two kinds of english , and then , a tree diagram was drawn for the entire phonemes of each hmm set . the analysis and comparison of the two trees showed many interesting characteristics of japanese english , some of which are well-known habits observed in je pronunciation . the authors consider that this study showed statistical differences between ae and japanese english in view of the entire phonemic system of english for the first time .
we test the hypothesis that adding information regarding the positions of electromagnetic articulograph sensors on the lips and jaw can improve the results of a typical acoustic-to-ema mapping system , based on support vector regression , that targets the tongue sensors . our initial motivation is to use such a acoustic-to-ema mapping system in the context of adding a tongue animation to a talking head built on the basis of concatenating bimodal acoustic-visual units . for completeness , we also train a acoustic-to-ema mapping system that maps only jaw and lip information to tongue information .
this paper proposes a novel approach to part-based tracking by replacing local matching of an appearance model by direct prediction of the displacement between local image patches and part locations . we propose to use cascaded regression with incremental learning to track generic objects without any prior knowledge of an object 's structure or appearance . we exploit the spatial constraints between parts by implicitly learning the shape and deformation parameters of the object in an online fashion . we integrate a multiple temporal scale motion model to initialise our cascaded regression search close to the target and to allow multiple temporal scale motion model to cope with occlusions . experimental results show that our tracker ranks first on the cvpr 2013 benchmark .
in this paper , we study the effect of the design parameters of a single-channel reverberation suppression algorithm on reverberation-robust speech recognition . at the same time , reverberation compensation at the speech recognizer is investigated . the analysis reveals that it is highly beneficial to attenuate only the reverberation tail after approximately 50 ms while coping with the early reflections and residual late-reverberation by training the speech recognizer on moderately reverber-ant data . it will be shown that the overall system at its optimum configuration yields a very promising recognition performance even in strongly reverberant environments . since the single-channel reverberation suppression algorithm is evidenced to significantly reduce the dependency on the training data , it allows for a very efficient training of acoustic models that are suitable for a wide range of reverberation conditions . finally , experiments with an '' ideal '' single-channel reverberation suppression algorithm are carried out to cross-check the inferred guidelines .
this paper presents the 169 permitted relations between two rectangles whose sides are parallel to the axes of some orthogonal basis in a 2-dimensional euclidean space . elaborating rectangle algebra just like interval algebra , it deenes the concept of convexity as well as the ones of weak preconvexity and strong precon-vexity . it introduces afterwards the fundamental operations of intersection , composition and inversion and demonstrates that the concept of weak preconvexity is preserved by the operation of composition whereas the concept of strong preconvexity is preserved by the operation of intersection . finally , tting the propagation techniques conceived to solve interval networks , it shows that the polynomial path-consistency algorithm is a decision method for the problem of proving the consistency of strongly precon-vex rectangle networks .
planned experiments are the gold standard in reliably comparing the causal effect of switching from a baseline policy to a new ad pricing policy . one critical shortcoming of classical experimental methods , however , is that they typically do not take into account the dynamic nature of response to policy changes . for instance , in an experiment where we seek to understand the effects of a new ad pricing policy on auction revenue , agents may adapt their bidding in response to the experimental pricing changes . thus , causal effects of the new ad pricing policy after such adaptation period , the long-term causal effects , are not captured by the classical methodology even though they clearly are more indicative of the value of the new ad pricing policy . here , we formalize a framework to define and estimate long-term causal effects of policy changes in multiagent economies . central to our approach is behavioral game theory , which we leverage to formulate the ignorability assumptions that are necessary for causal inference . under such ignorability assumptions we estimate long-term causal effects through a latent space approach , where a behavioral model of how agents act conditional on their latent behaviors is combined with a temporal model of how behaviors evolve over time .
pathfinding in uniform-cost grid environments is a problem commonly found in application areas such as robotics and video games . the state-of-the-art is dominated by hierarchical pathfinding algorithms which are fast and have small memory overheads but usually return suboptimal paths . in this paper we present a novel search strategy , specific to grids , which is fast , optimal and requires no memory overhead . our search strategy can be described as a macro operator which identifies and selectively expands only certain nodes in a grid map which we call jump points . intermediate nodes on a path connecting two jump points are never expanded . we prove that this search strategy always computes optimal solutions and then undertake a thorough empirical analysis , comparing our search strategy with related works from the literature . we find that searching with jump points can speed up a * by an order of magnitude and more and report significant improvement over the current state of the art .
in a recent paper joachims -lsb- 1 -rsb- presented svm-perf , a cutting plane method for training linear support vector machines -lrb- svms -rrb- which converges to an accurate solution in o -lrb- 1 / / 2 -rrb- iterations . by tightening the analysis , teo et al. -lsb- 2 -rsb- showed that o -lrb- 1 / / -rrb- iterations suffice . given the impressive convergence speed of cutting plane method on a number of practical problems , it was conjectured that these rates could be further improved . in this paper we disprove this conjecture . we present counter examples which are not only applicable for training linear svms with hinge loss , but also hold for support vector methods which optimize a multivari-ate performance score . however , surprisingly , these problems are not inherently hard . by exploiting the structure of the objective function we can devise an algorithm that converges in o -lrb- 1 / √ -rrb- iterations .
this paper investigates the recognition of group actions in meetings by modeling the joint behaviour of participants . many meeting actions , such as presentations , discussions and consensus , are characterised by similar or complementary behaviour across participants . recognising these meaningful actions is an important step towards the goal of providing effective browsing and sum-marisation of processed meetings . in this work , a corpus of meetings was collected in a room equipped with a number of microphones and cameras . the corpus of meetings was labeled in terms of a pre-defined set of meeting actions characterised by global behaviour . in experiments , audio and visual features for each participant are extracted from the raw data and the interaction of participants is modeled using hmm-based approaches . initial results on the corpus of meetings demonstrate the ability of the system to recognise the set of meeting actions .
we present an object detector coupled with pose estimation directly in a single compact and simple model , where the object detector shares extracted image features with the pose estimator . the output of the classification of each candidate window consists of both object score and likelihood map of poses . this object detector introduces negligible overhead during detection so that the object detector is still capable of real time operation . we evaluated the proposed object detector on the problem of vehicle detection . we used existing datasets with viewpoint/pose annotation -lrb- wcvp , 3d objects , kitti -rrb- . besides that , we collected a new traffic surveillance dataset cod20k which fills certain gaps of the existing datasets and we make object detector public . the experimental results show that the proposed object detector is comparable with state-of-the-art approaches in terms of accuracy , but object detector is considerably faster -- easily operating in real time -lrb- matlab with c++ code -rrb- . the source codes and the collected cod20k dataset are made public along with the paper .
when dialogue system developers tackle a new domain , much effort is required ; the development of different parts of the system usually proceeds independently . yet it may be profitable to coordinate development efforts between different modules . here , we focus our efforts on extending small amounts of language model training data by integrating semantic classes that were created for a natural language understanding module . by converting finite state parses of a training corpus into a probabilistic context free grammar and subsequently generating artificial data from the context free grammar , we can significantly reduce perplexity and asr word error for situations with little training data . experiments are presented using data from the atis and darpa communicator travel corpora .
we address the problem of similarity metric selection in pairwise affinity clustering . traditional techniques employ standard algebraic context-independent sample-distance measures , such as the euclidean distance . more recent context-dependent metric modifications employ the bottleneck principle to develop path-bottleneck or path-average distances and define similarities based on geodesics determined according to these metrics . this paper develops a principled context-adaptive similarity metric for pairs of feature vectors utilizing the probability density of all data . specifically , based on the postulate that euclidean distance is the canonical metric for data drawn from a unit-hypercube uniform density , a density-geodesic distance measure stemming from riemannian geometry of curved surfaces is derived . comparisons with alternative metrics demonstrate the superior properties such as robustness .
measuring the semantic meaning between words is an important issue because it is the basis for many applications , such as word sense disambiguation , document summarization , and so forth . although it has been explored for several decades , most of the studies focus on improving the effectiveness of the problem , i.e. , precision and recall . in this paper , we propose to address the efficiency issue , that given a collection of words , how to efficiently discover the top-k most semantic similar words to the query . this issue is very important for real applications yet the existing state-of-the-art strategies can not satisfy users with reasonable performance . efficient strategies on searching top-k semantic similar words are proposed . we provide an extensive comparative experimental evaluation demonstrating the advantages of the introduced strategies over the state-of-the-art approaches .
we study the problem of unsupervised domain adaptation , which aims to adapt classi-fiers trained on a labeled source domain to an unlabeled target domain . many existing approaches first learn domain-invariant features and then construct classifiers with them . we propose a novel approach that jointly learn the both . specifically , while the method identifies a feature space where data in the source and the target domains are similarly distributed , it also learns the feature space discriminatively , optimizing an information-theoretic metric as an proxy to the expected misclassification error on the target domain . we show how this optimization can be effectively carried out with simple gradient-based methods and how hyperparameters can be cross-validated without demanding any labeled data from the target domain . empirical studies on benchmark tasks of object recognition and sentiment analysis validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in classification accuracies .
we explore methods for incorporating prior knowledge about a problem at hand in support vector learning machines . we show that both invari-ances under group transfonnations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions .
in this paper , we present a methodology for precisely comparing the robustness of face recognition algorithms with respect to changes in pose angle and illumination angle . for this study , we have chosen four widely-used algorithms : two subspace analysis methods -lrb- principle component analysis and linear discriminant analysis -rrb- and two probabilistic learning methods -lrb- hidden markov models -lrb- hmm -rrb- and bayesian intra-personal classifier -rrb- . we compare the recognition robustness of these algorithms using a novel database -lrb- facepix -rrb- that captures face images with a wide range of pose angles and illumination angles . we propose a method for deriving a robustness measure for each of these algorithms , with respect to pose and illumination angle changes . the results of this comparison indicate that the subspace methods perform more robustly than the probabilistic learning methods in the presence of pose and illumination angle changes .
a method for localization , the act of recognizing the environment , is presented . the method is based on representing the scene as a set of 2 0 views and predicting the appearances of novel views by linear combinations of the model views . the method accurately approximates the appearance of scenes under weak perspective projection . analysis of this projection as well as experimental results demonstrate that in many cases this approximation is suficient t o accurately describe the scene . when weak perspective approximation is invalid , either a larger number of models can be acquired or an iterative solution t o account for the perspective distortions can be employed . the method has several advantages over other approaches . it uses relatively rich representations ; the representations are 2d rather than 9d ; and localiza-tion can be done f r o m only a single 2d view .
web services offer a unique opportunity to simplify application integration by defining common , web-based , platform-neutral , standards for publishing service descriptions to a registry , finding and invoking them -- not necessarily by the same parties . viewing software components as web services , the current solutions to web services composition based on business web services -lrb- using wsdl , bpel , soap etc. -rrb- or semantic web services -lrb- using ontologies , goal-directed reasoning etc. -rrb- are both piecemeal and insufficient for building practical applications . inspired by the work in ai planning on decoupling causal -lrb- planning -rrb- and resource reasoning -lrb- scheduling -rrb- , we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches . the solution is based on a novel two -- staged composition approach that addresses the information model-ing aspects of web services , provides support for contextual information while composing services , employs efficient de-coupling of functional and non-functional requirements , and leads to improved scalability and failure handling . a prototype of the solution has been implemented in the synthy service composition system and applied to a number of composition scenarios from the telecom domain . the application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for ai .
this paper examines projectively invariant local properties of smooth curves and surfaces . oriented projective differential geometry is proposed as a theoretical framework for establishing such invariants and describing the local shape of surfaces and their outlines . this theoretical framework is applied to two problems : a projective proof of koenderink 's famous characterization of convexities , concavities , and inflections of apparent contours ; and the determination of the relative orientation of rim tangents at frontier points .
clustering is a popular problem with many applications . we consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially , such as from a disk , and where we must use as little memory as possible . our algorithm is based on recent theoretical results , with significant improvements to make it practical . our approach greatly simplifies a recently developed algorithm , both in design and in analysis , and eliminates large constant factors in the approximation guarantee , the memory requirements , and the running time . we then incorporate approximate nearest neighbor search to compute k-means in o -lrb- nk -rrb- -lrb- where n is the number of data points ; note that computing the cost , given a solution , takes θ -lrb- nk -rrb- time -rrb- . we show that our algorithm compares favorably to existing algorithms-both theoretically and experimentally , thus providing state-of-the-art performance in both theory and practice .
object recognition and detection represent a relevant component in cognitive computer vision systems , such as in robot vision , intelligent video surveillance systems , or multi-modal interfaces . object identification from local information has recently been investigated with respect to its potential for robust recognition , e.g. , in case of partial object occlusions , scale variation , noise , and background clutter in detection tasks . this work contributes to this research by a thorough analysis of the discriminative power of local appearance patterns and by proposing to exploit local information content to model object representation and recognition . we identify discriminative regions in the object views from a posterior en-tropy measure , and then derive object models from selected discriminative local patterns . for recognition , we determine rapid attentive search for locations of high information content from learned decision trees . the recognition system is evaluated by various degrees of partial occlusion and gaus-sian image noise , resulting in highly robust recognition even in the presence of severe occlusion effects .
we propose a distributed implementation of the gaussian particle filter for use in a wireless sensor network . each sensor runs a local gpf that computes a global state estimate . the updating of the particle weights at each sensor uses the joint likelihood function , which is calculated in a distributed way , using only local communications , via the recently proposed likelihood consensus scheme . a significant reduction of the number of particles can be achieved by means of another likelihood consensus scheme . the performance of the proposed local gpf is demonstrated for a target tracking problem .
a ltering approach to underdetermined blind source separation with application to temporomandibular disorders this item was submitted to loughborough university 's institutional repository by the/an author . ltering approach to underdetermined blind source separation with application to tem-poromandibular disorders . however , permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists , or to reuse any copyrighted component of this work in other works must be obtained from the ieee . abstract this paper addresses the underdetermined blind source separation problem , using a filtering approach . we have developed an extension of the fastica algorithm which exploits the disparity in the kurtoses of the underlying sources to estimate the mixing matrix and thereafter the recovery of the sources is achieved by employing the 1-norm algorithm . also , we demonstrate how promising fastica algorithm can be to extract the sources , without utilizing the 1-norm algorithm . furthermore , we illustrate how this ltering approach is particularly suitable to the separation of the temporomandibular joint sounds , crucial in the diagnosis of temporomandibu-lar disorders .
this paper presents a method to incorporate knowledge from possibly imperfect models and domain theories into inductive learning of decision trees for classification the approach assumes that a model or domain theory reflects useful prior knowledge of th < task thus the default bias should accept the model s predictions as accurate even in the face of somewhat contradictory data which may be unrepresen-lative or noisy however our approach allows the svslem to abandon the model or domain theorv , or portions thereof in the fact of suffi-cientlv contradictory data in particular we use c4 5 to induce decision trees from data that ha \ t heen augmented b \ model or domain-theory-denvcd features ' we weakly bias the svslem to select model-derived features dur ing decision tree induction but this preference is not dogmatically applied our experiments vary imperfection in a model the representa tiveness of data and the veracitv with which modfl-demed feature are preferred 1 introduction when human expertise is nonexistent or very weak relative to a particular domain/task and when data is plentiful machine induction from data mav be the only reasonable approach to task automation in contrast , when expertise is strong , then encoding the expert s model or domain theory via traditional knowledge acquisition strategies ma > be the best approach in fact , this human expertise may stem from induction over a much larger data sample than is available at the time task automation is undertaken in many cases , however , conditions are indeterminate as to whether sole reliance on machine induction or human expertise is most appropriate human expertise may not be ` perfect and/or data may not be as plentiful as desired in cases where some data is available and human expertise is less than perfect an advantageous strategy may be to exploit both in an appropriate way there is a growing body of work that combines model-based or domain-theory knowledge with empirical learning from data clark and matwin -lsb- 1993 -rsb- assume that an analyst-specified model mediates empirical learning-the rules derived from a machine-induction system are ace3pted as long as they do not contradict the biases found in the model evans and fisher -lsb- 1994 -rsb- employ a similar strategy-a human analyst may specify weak rules -lrb- e g when printing-plant humidity is low , a certain kind of printing error known as banding is more likely , to occur -rrb- if inductively-derived rules indicate an opposite trend then the learning system s default strategy is to reject the ...
this paper presents an extension of chi-ang 's hierarchical phrase-based model , called head-driven hpb , which incorporates head information in translation rules to better capture syntax-driven information , as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space . experiments on chinese-english translation on four nist mt test sets show that the head-driven hpb significantly outperforms chiang 's model with average gains of 1.91 points absolute in bleu .
in recent years , the interest of investors has shifted to computerized asset allocation -lrb- portfolio management -rrb- to exploit the growing dynamics of the capital markets . in this paper , asset allocation is formalized as a markovian decision problem which can be optimized by applying dynamic programming or reinforcement learning based algorithms . using an artificial exchange rate , the asset allocation strategy optimized with reinforcement learning is shown to be equivalent to a policy computed by dynamic programming . the asset allocation strategy is then tested on the task to invest liquid capital in the german stock market . here , neural networks are used as value function approximators . the resulting asset allocation strategy is superior to a heuristic benchmark policy . this is a further example which demonstrates the applicability of neural network based reinforcement learning to a problem setting with a high dimensional state space .
this paper develops a rao-blackwellised particle filtering algorithm for blind system identification . the state space model under consideration uses a time-varying autoregressive model for the sources , and a time-varying finite impulse response model for the channel . the multi-sensor measurements result from the convolution of the sources with the channels in the presence of additive noise . a numerical approximation to the optimal bayesian solution for the nonlinear sequential state estimation problem is implemented using sequential monte carlo methods . the bayesian solution is applied to improve the efficiency of the particle filter by marginalizing out the ar and fir coefficients from the joint posterior distribution . simulation results are given to verify the performance of the proposed rao-blackwellised particle filtering algorithm .
this paper tackles an important aspect of the variational problems involving active contours , which has been largely overlooked so far : the optimization by gradient flows . classically , the definition of a gradient depends directly on the choice of an inner product structure . this consideration is largely absent from the active contours literature . most authors , overtly or covertly , assume that the space of admissible deformations is ruled by the canonical l 2 inner product . the classical gradient flows reported in the literature are relative to this particular choice . in this paper , we investigate the relevance of using other inner products , yielding other gradient descents , and some other minimizing flows not deriving from any inner product . in particular , we show how to induce different degrees of spatial coherence into the minimizing flow , in order to decrease the probability of getting trapped into irrelevant local minima . we show with some numerical experiments that the sensitivity of the active contours method to initial conditions , which seriously limits its applicability and its efficiency , is alleviated by our application-specific spatially coherent minimizing flows .
the inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory ; it is beneficial to be able to learn this function for adaptive control . a robotic manipulator will often need to be controlled while holding different loads in its end effector , giving rise to a multi-task learning problem . by placing independent gaussian process priors over the latent functions of the inverse dynamics , we obtain a multi-task gaussian process prior for handling multiple loads , where the inter-task similarity depends on the underlying inertial parameters . experiments demonstrate that this robotic manipulator is effective in sharing information among the various loads , and generally improves performance over either learning only on single tasks or pooling the data over all tasks .
strategy representation and reasoning for incomplete information concurrent games has recently received much attention in multi-agent system and ai communities . however , most of the logical frameworks are based on concrete game models , lack the abilities to reason about strategies explicitly or specify strategies procedurally , and ignore the issue of coordination within a coalition . in this paper , by a simple extension of a variant of multi-agent epistemic situation calculus with a strategy sort , we develop a general framework for strategy representation and reasoning for incomplete information concurrent games . based on golog , we propose a strategy programming language which can be conveniently used to specify collective strategies of coalitions at different granularities . we present a formalization of joint abilities of coalitions under commitments to strategy programs . different kinds of individual strategic abilities can be distinguished in our framework . both strategic abilities in atl and joint abilities of ghaderi et al. can be considered as joint abilities under special programs in our framework . we illustrate our work with a variant of levesque 's squirrels world .
we study the application of hidden markov models to learning information extractors for $ - ary relations from free text . we propose an approach to representing the grammatical structure of sentences in the states of the model . we also investigate using an objective function during hmm training which maximizes the ability of the learned models to identify the phrases of interest . we evaluate our methods by deriving extractors for two binary relations in biomedical domains . our experiments indicate that our approach learns more accurate models than several baseline approaches .
this paper addresses the issue of motion estimation on image sequences . the standard motion estimation used to compute the apparent motion of image irradiance patterns is an invariance brightness based hypothesis called the optical flow constraint . other equations can be used , in particular the extended optical flow constraint , which is a variant of the optical flow constraint , inspired by the fluid mechanic mass conservation principle . in this paper , we propose a physical interpretation of this extended optical flow constraint and a new model unifying the optical flow and the extended optical flow constraints . we present results obtained for synthetic and meteorological images .
the facial action coding system , -lrb- facs -rrb- , devised by ekman and friesen -lrb- 1978 -rrb- , provides an objective means for measuring the facial muscle contractions involved in a facial expression . in this paper , we approach automated facial expression analysis by detecting and classifying facial actions . we generated a database of over 1100 image sequences of 24 subjects performing over 150 distinct facial actions or action combinations . we compare three diierent approaches to classifying the facial actions in these images : holistic spatial analysis based on principal components of graylevel images ; explicit measurement of local image features such as wrinkles ; and template matching with motion ow elds . on a dataset containing six individual actions and 20 subjects , these methods had 89 % , 57 % , and 85 % performances respectively for generalization to novel subjects . when combined , performance improved to 92 % .
we develop vector-sensor array processing to estimate the angles-of-arrival aoas and time delays of mul-tipath channels in the space-time-polarization domain . a music-type algorithm for joint angle and delay estimation with a vector-sensor array is derived . potential applications include multipath channel estimation and mobile localization . simulation results show that the space-time-polarization parameterization of the multi-path channels results in improved accuracy and resolution performance .
this paper studies the use of glottal inverse filtering together with a biomechanical model of the vocal folds to simulate the glottal flow waveform . the glottal flow waveform is first estimated by inverse filtering the acoustic speech pressure signal of natural speech . the estimated glottal flow is used as a template in an optimization process which searches for a set of parameters for a deterministic vocal fold model such that the model output reproduces the estimated glottal flow . the results indicate that the method can reproduce the main deterministic components of the glottal flow signal with good accuracy .
in this paper we propose an algorithm that automatically detects clear scene cut locations from an mpeg-1 video bit streams coded with a gop structure of m = 1 , without b pictures . the algorithm detects scene cuts at p t ype pictures by monitoring the percentage of intra-macroblocks per p picture . while scene cuts at i pictures are detected by matching the macroblocks type of the two p pictures at the gop boundaries . a \ type matching parameter '' -lrb- tmp -rrb- is developed to estimate the matching degree between the macroblock types of two p pictures . it is shown that the method is able to identify the location of scene cuts in p and i pictures with a high success rate .
a wide range of properties and assumptions determine the most appropriate spatial matching model for an application , e.g. recognition , detection , registration , or large scale image retrieval . most notably , these include discrim-inative power , geometric invariance , rigidity constraints , mapping constraints , assumptions made on the underlying features or descriptors and , of course , computational complexity . having image retrieval in mind , we present a very simple model inspired by hough voting in the transformation space , where votes arise from single feature correspondences . a relaxed matching process allows for multiple matching surfaces or non-rigid objects under one-to-one mapping , yet is linear in the number of correspondences . we apply relaxed matching process to geometry re-ranking in a search engine , yielding superior performance with the same space requirements but a dramatic speed-up compared to the state of the art .
it is now readily accepted that automated algorithm configuration is a necessity for ensuring optimized performance of solvers on a particular problem domain . even the best developers who have carefully designed their solver are not always able to manually find the best parameter settings for it . yet , the opportunity for improving performance has been repeatedly demonstrated by configuration tools like paramils , smac , and gga . however , all these techniques currently assume a static environment , where demonstrative instances are procured beforehand , potentially unlimited time is provided to adequately search the parameter space , and the solver would never need to be retrained . this is not always the case in practice . the react system , proposed in 2014 , demonstrated that a solver could be configured during runtime as new instances arrive in a steady stream . this paper further develops that approach and shows how a ranking scheme , like trueskill , can further improve the configurator 's performance , making it able to quickly find good parameterizations without adding any overhead on the time needed to solve any new instance , and then continuously improve as new instances are evaluated . the enhancements to react system that we present enable us to even outperform existing static config-urators like smac in a non-dynamic setting .
in this paper , we investigate semi-blind channel estimation for multiple input multiple output -lrb- mimo -rrb- quasi-static flat fading channels when maximum ratio transmission is employed . we propose a closed-form semi-blind solution for estimating the optimum transmit and receive beamforming vectors of the channel matrix . employing matrix perturbation theory , we develop expressions for the mean squared error in the beamforming vector and average received snr of both the semi-blind and the conventional least squares estimation -lrb- closed-form semi-blind solution -rrb- schemes . it is found that the proposed estimation technique outperforms closed-form semi-blind solution for a wide range of training lengths and training snrs .
applications are increasingly expected to make smart decisions based on what humans consider basic commonsense . an often overlooked but essential form of commonsense involves comparisons , e.g. the fact that bears are typically more dangerous than dogs , that tables are heavier than chairs , or that ice is colder than water . in this paper , we first rely on open information extraction methods to obtain large amounts of comparisons from the web . we then develop a joint optimization model for cleaning and disambiguating this knowledge with respect to wordnet . this joint optimization model relies on integer linear programming and semantic coherence scores . experiments show that our joint optimization model outperforms strong baselines and allows us to obtain a large knowledge base of disambiguated commonsense assertions .
urdu is spoken by more than 100 million speakers . this paper summarizes the corpus and lexical resources being developed for urdu by the crulp , in pakistan .
this paper addresses the problem of image-based surface reconstruction . the main contribution is the computation of the exact derivative of the reprojection error functional . this allows its rigorous minimization via gradient descent surface evolution . the main difficulty has been to correctly take into account the visibility changes that occur when the surface moves . a geometric and analytical study of these changes is presented and used for the computation of derivative . our analysis shows the strong influence that the movement of the contour generators has on the reprojection error . as a consequence , during the proper minimization of the reprojection error , the contour generators of the surface are automatically moved to their correct location in the images . therefore , current methods adding additional silhouettes or apparent contour constraints to ensure this alignment can now be understood and justified by a single criterion : the reprojection error .
binary coding techniques , which compress originally high-dimensional data samples into short binary codes , are becoming increasingly popular due to their efficiency for information retrieval . lever-aging supervised information can dramatically enhance the coding quality , and hence improve search performance . there are few methods , however , that efficiently learn coding functions that optimize the precision at the top of the hamming distance ranking list while approximately preserving the geometric relationships between database examples . in this paper , we propose a novel supervised binary coding approach , namely fast structural binary coding , to optimize the precision at the top of a hamming distance ranking list and ensure that similar images can be returned as a whole . the key idea is to train disciplined coding functions by optimizing a lower bound of the area under the roc -lrb- receiver operating characteristic -rrb- curve -lrb- auc -rrb- and penalize this objective so that the geometric relationships between database examples in the original euclidean space are approximately preserved in the hamming space . to find such a coding function , we relax the original discrete optimization objective with a continuous surrogate , and then derive a stochastic gradient descent method to optimize the surrogate objective efficiently . empirical studies based upon two image datasets demonstrate that the proposed supervised binary coding approach achieve superior image search performance to the states-of-the-art .
despite the success of cnns , selecting the optimal architecture for a given task remains an open problem . instead of aiming to select a single optimal architecture , we propose a '' cnns '' that embeds an exponentially large number of architectures . the cnns consists of a 3d trellis that connects response maps at different layers , scales , and channels with a sparse homogeneous local connectivity pattern . the only hyper-parameters of a cnns are the number of channels and layers . while individual architectures can be recovered as paths , the cnns can in addition ensemble all embedded architectures together , sharing their weights where their paths overlap . parameters can be learned using standard methods based on back-propagation , at a cost that scales linearly in the fabric size . we present benchmark results competitive with the state of the art for image classification on mnist and cifar10 , and for semantic segmentation on the part labels dataset .
in the absence of hmms trained with speech collected in the target environment , one may use hmms trained with a large amount of speech collected in another recording condition -lrb- e.g. , quiet office , with high quality microphone -rrb- . however , this may result in poor performance because of the mismatch between the two acoustic conditions . we propose a linear regression-based model adaptation procedure to reduce such a mismatch . with some adaptation utterances collected for the target environment , the linear regression-based model adaptation procedure transforms the hmms trained in a quiet condition to maximize the likelihood of observing the adaptation utterances . the transformation must be designed to maintain speaker-independence of the hmm . our speaker-independent test results show that with this linear regression-based model adaptation procedure about 1 % digit error rate can be achieved for hands-free recognition , using target environment speech from only 20 speakers .
we propose a highly efficient framework for kernel multi-class models with a large and structured set of classes . kernel parameters are learned automatically by maximizing the cross-validation log likelihood , and predictive probabilities are estimated . we demonstrate our approach on large scale text classification tasks with hierarchical class structure , achieving state-of-the-art results in an order of magnitude less time than previous work .
feature combination techniques based on pca , lda and hlda are compared in experiments where limited amount of training data is available . success with feature combination can be quite dependent on proper estimation of statistics required by the used technique . insufficiency of training data is , therefore , an important problem , which has to be taken in to account in our experiments . besides of some standard approaches increasing robustness of statistic estimation , methods based on combination of lda and hlda are proposed . an improved recognition performance obtained using these methods is demonstrated in experiments .
multi-label learning is useful in visual object recognition when several objects are present in an image . conventional approaches implement multi-label learning as a set of binary classification problems , but multi-label learning suffer from im-balanced data distributions when the number of classes is large . in this paper , we address multi-label learning with many classes via a ranking approach , termed multi-label ranking . given a test image , the proposed scheme aims to order all the object classes such that the relevant classes are ranked higher than the irrelevant ones . we present an efficient algorithm for multi-label ranking based on the idea of block coordinate descent . the proposed algorithm is applied to visual object recognition . empirical results on the pascal voc 2006 and 2007 data sets show promising results in comparison to the state-of-the-art algorithms for multi-label learning .
distributional methods have proven to excel at capturing fuzzy , graded aspects of meaning -lrb- italy is more similar to spain than to germany -rrb- . in contrast , it is difficult to extract the values of more specific attributes of word referents from distribu-tional representations , attributes of the kind typically found in structured knowledge bases -lrb- italy has 60 million inhabitants -rrb- . in this paper , we pursue the hypothesis that distributional vectors also implicitly encode referential attributes . we show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy : when evaluated on the prediction of both categorical and numeric attributes of countries and cities , the supervised regression model consistently reduces baseline error by 30 % , and is not far from the upper bound . further analysis suggests that our supervised regression model is able to '' ob-jectify '' distributional representations for entities , anchoring distributional representations more firmly in the external world in measurable ways .
commonly used speech enhancement algorithms estimate the power spectral density of the noise to be removed , or make a decision about the presence of speech in a particular frame , and estimate the clean speech based on these . errors in a noise estimate or speech activity decision may result in undesirable artifacts , and some errors may be more damaging than others . robust bayesian analysis is used to analyze the sensitivity of algorithms to errors in noise estimates and improve signal-to-noise ratio while mitigating artifacts in the enhanced speech . the findings explain why some common heuristic changes to the wiener filter algorithm are effective . a standard wiener algorithm is used for comparison , objective quality measures are used to quantify improvement , and insights into the underlying mechanisms of heuristic methods are offered .
to better understand , search , and classify image and video information , many visual feature descriptors have been proposed to describe elementary visual characteristics , such as the shape , the color , the texture , etc. . how to integrate these heterogeneous visual features and identify the important ones from them for specific vision tasks has become an increasingly critical problem . in this paper , we propose a novel sparse multimodal learning approach to integrate such heterogeneous features by using the joint structured sparsity regularizations to learn the feature importance of for the vision tasks from both group-wise and individual point of views . a new optimization algorithm is also introduced to solve the non-smooth objective with rigorously proved global convergence . we applied our sparse multimodal learning approach to five broadly used object categorization and scene understanding image data sets for both single-label and multi-label image classification tasks . for each data set we integrate six different types of popularly used image features . compared to existing scene and object cat-egorization methods using either single modality or multi-modalities of features , our sparse multimodal learning approach always achieves better performances measured .
in this paper we introduce a novel dynamic programming algorithm called information retrieval-based dynamic time warping used to find non-linearly matching subsequences between two time series where matching start and end points are not known a priori . in this paper our dynamic programming algorithm is applied for audio matching within the query by example -lrb- qbe -rrb- spoken term detection -lrb- std -rrb- task , although dynamic programming algorithm is applicable to many other problems . the main advantages of the proposed dynamic programming algorithm in comparison to similar approaches are twofold . on the one hand , ir-dtw requires a much smaller memory footprint than standard dynamic time warping approaches . on the other hand , dynamic programming algorithm allows for the application of indexing techniques to the search collection for increased matching speed , which makes ir-dtw suitable for application in large scale implementations . we show through preliminary experimentation with a qbe-std task that the memory footprint is greatly reduced in comparison to a baseline subsequence-dtw implementation and that its matching accuracy is much better than that of pure diagonal matching and just slightly worse than that of s-dtw .
the discovery of non-linear causal relationship under additive non-gaussian noise models has attracted considerable attention recently because of their high flexibility . in this paper , we propose a novel causal inference algorithm called least-squares independence regression . least-squares independence regression learns the additive noise model through minimization of an estimator of the squared-loss mutual information between inputs and residuals . a notable advantage of least-squares independence regression over existing approaches is that tuning parameters such as the kernel width and the regularization parameter can be naturally optimized by cross-validation , allowing us to avoid overfitting in a data-dependent fashion . through experiments with real-world datasets , we show that least-squares independence regression compares favorably with the state-of-the-art causal inference method .
this paper is focused on the co-segmentation problem -lsb- 1 -rsb- -- where the objective is to segment a similar object from a pair of images . the background in the two images may be arbitrary ; therefore , simultaneous segmentation of both images must be performed with a requirement that the appearance of the two sets of foreground pixels in the respective images are consistent . existing approaches -lsb- 1 , 2 -rsb- cast this co-segmentation problem as a markov random field based segmen-tation of the image pair with a regularized difference of the two histograms -- assuming a gaussian prior on the foreground appearance -lsb- 1 -rsb- or by calculating the sum of squared differences -lsb- 2 -rsb- . both are interesting formulations but lead to difficult optimization problems , due to the presence of the second -lrb- histogram difference -rrb- term . the model proposed here bypasses measurement of the histogram differences in a direct fashion ; we show that this enables obtaining efficient solutions to the underlying optimization model . our new algorithm is similar to the existing methods in spirit , but differs substantially in that it can be solved to optimal-ity in polynomial time using a maximum flow procedure on an appropriately constructed graph . we discuss our ideas and present promising experimental results .
unit selection speech synthesis techniques lead the speech synthesis state of the art . automatic segmentation of databases is necessary in order to build new voices . they may contain errors and segmentation processes may introduce some more . quality systems require a significant effort to find and correct these segmentation errors . phonetic transcription is crucial and is one of the manually supervised tasks . the possibility to automatically remove incorrectly transcribed units from the inventory will help to make the process more automatic . here we present a new technique based on speech recognition confidence measures that reaches to remove 90 % of incorrectly transcribed units from a database . the cost for it is loosing only a 10 % of correctly transcribed units .
we present a simple yet effective unsu-pervised domain adaptation method that can be generally applied for different nlp tasks . our unsu-pervised domain adaptation method uses unlabeled target domain instances to induce a set of instance similarity features . these instance similarity features are then combined with the original instance similarity features to represent labeled source domain instances . using three nlp tasks , we show that our unsu-pervised domain adaptation method consistently out-performs a few baselines , including scl , an existing general unsupervised domain adaptation method widely used in nlp . more importantly , our unsu-pervised domain adaptation method is very easy to implement and incurs much less computational cost than scl .
in this paper , we investigate unsupervised acoustic model training approaches for dysarthric-speech recognition . these unsupervised acoustic model training approaches are first , frame-based gaussian posteriorgrams , obtained from vector quantization , second , so-called acoustic unit descriptors , which are hidden markov models of phone-like units , that are trained in an unsupervised fashion , and , third , posteriorgrams computed on the acoustic unit descriptors . experiments were carried out on a database collected from a home automation task and containing nine speakers , of which seven are considered to utter dysarthric speech . all unsupervised acoustic model training approaches delivered significantly better recognition rates than a speaker-independent phoneme recognition baseline , showing the suitability of unsupervised acoustic model training for dysarthric speech . while the acoustic unit descriptors led to the most compact representation of an utterance for the subsequent semantic inference stage , posteriorgram-based representations resulted in higher recognition rates , with the gaussian posteri-orgram achieving the highest slot filling f-score of 97.02 % .
in dmt-based communication systems where full-duplex transmission is required , digital echo cancellers are employed to cancel echo by means of adaptive filters . in order to reduce the computational complexity of these cancellers , the structure of the toeplitz matrix containing the transmitted signal is usually exploited to transform the time domain signals and perform the emulation and adaptive update in a more convenient domain -lrb- e.g. frequency domain -rrb- . in this paper , we consider a recently proposed dual transform domain echo canceller , which is based on the general decomposition of the data toeplitz matrix . a comprehensive comparative performance evaluation of the proposed dual transform domain echo canceller with the existing methods is provided . this evaluation includes the comparison of the convergence curves and computational cost of the dual transform domain echo canceller . the comparison shows that the proposed dual transform domain echo canceller achieves a faster convergence with a low error floor with no increase in the complexity .
the paper describes an approach to combining multiple classifiers in order to improve classification accuracy . since individual classifiers in the ensemble should somehow be uncorrelated to yield higher classification accuracy than a single classifier , we propose to train classifiers by minimizing the correlation between their classification errors . a simple combination strategy for three classifiers is then proposed and its achievable error rate is analyzed and compared to individual single classifier performance . the proposed approach has been evaluated on artificial data and a nasal/oral vowel classification task . theoretical analyses and experimental results illustrate the effectiveness of the proposed approach .
model checking probabilistic knowledge of memory-ful semantics is undecidable , even for a simple formula concerning the reachability of probabilistic knowledge of a single agent . this result suggests that the usual approach of tackling undecidable model checking problems , by finding syntactic restrictions over the logic language , may not suffice . in this paper , we propose to work with an additional restriction that agent 's knowledge concerns a special class of atomic propositions . a pspace-complete case is identified with this additional restriction , for a logic language combining ltl with limit-sure knowledge of a single agent .
we address the design and optimization of an energy-efficient lifting-based 2d transform for wireless sensor networks with irregular spatial sampling . the energy-efficient lifting-based 2d transform is designed to allow for unidirectional computation found in existing path-wise transforms , thereby eliminating costly backward transmissions often required by existing 2d transforms , while simultaneously achieving greater data decorrelation than those path-wise transforms . we also propose a framework for optimizing the energy-efficient lifting-based 2d transform via an extension of standard dynamic programming algorithms , where a selection is made among alternative coding schemes -lrb- e.g. , different number of levels in the wavelet decomposition -rrb- . a recursive dp formulation is provided and an algorithm is given that finds the minimum cost coding scheme assignment for our proposed energy-efficient lifting-based 2d transform .
map inference for general energy functions remains a challenging problem . while most efforts are channeled towards improving the linear programming based relaxation , this work is motivated by the quadratic programming relaxation . we propose a novel map inference that penalizes the kullback-leibler divergence between the lp pairwise auxiliary variables , and qp equivalent terms given by the product of the unar-ies . we develop two efficient algorithms based on variants of this relaxation . the algorithms minimize the non-convex objective using belief propagation and dual decomposition as building blocks . experiments on synthetic and real-world data show that the solutions returned by our algorithms substantially improve over the lp relaxation .
this paper presents a blind dereverberation method designed to recover the subband envelope of an original speech signal from its reverberant version . the problem is formulated as a blind deconvolution problem with non-negative constraints , regularized by the sparse nature of speech spectrograms . we derive an iterative algorithm for its optimization , which can be seen as a special case of the non-negative matrix factor deconvolution . we confirmed through experiments that the blind dereverberation method is fast and robust to speaker movement .
in this paper , we present a non-rigid quasi-dense matching method and its application to object recognition and segmentation . the non-rigid quasi-dense matching method is based on the match propagation algorithm which is here extended by using local image gradients for adapting the propagation to smooth non-rigid deformations of the imaged surfaces . the adaptation is based entirely on the local properties of the images and the non-rigid quasi-dense matching method can be hence used in non-rigid image registration where global geometric constraints are not available . our non-rigid quasi-dense matching method for object recognition and seg-mentation is directly built on the quasi-dense matching . the quasi-dense pixel matches between the non-rigid quasi-dense matching method and test images are grouped into geometrically consistent groups using a non-rigid quasi-dense matching method which utilizes the local affine transformation estimates obtained during the propagation . the number and quality of geometrically consistent matches is used as a recognition criterion and the location of the matching pixels directly provides the segmentation . the experiments demonstrate that our non-rigid quasi-dense matching method is able to deal with extensive background clutter , partial occlusion , large scale and viewpoint changes , and notable geometric deformations .
we present a novel negotiation protocol to facilitate energy exchange between off-grid homes that are equipped with renewable energy generation and electricity storage . our negotiation protocol imposes restrictions over negotiation such that negotiation protocol reduces the complex interdependent multi-issue negotiation to one where agents have a strategy profile in subgame perfect nash equilibrium . we show that our negotiation protocol is concurrent , scalable and ; under certain conditions ; leads to pareto-optimal outcomes .
continuous relaxations play an important role in discrete optimization , but have not seen much use in approximate probabilistic inference . here we show that a general form of the gaussian integral trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems . the continuous representation allows the use of gradient-based hamiltonian monte carlo for inference , results in new ways of estimating normalization constants -lrb- partition functions -rrb- , and in general opens up a number of new avenues for inference in difficult discrete systems . we demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems .
this paper describes an approach to detect out-of-vocabulary words in spontaneous speech using a language model built on semantic categories and a new type of generalized word models consisting of a mixture of specific and general acoustic units . we demonstrate the construction of the generalized word models as replacements for surnames in a german spontaneous travel planning task gsst -lsb- 1 -rsb- . we show that the use of our generalized word models improves recognition accuracy in cases where out-of-vocabulary words appear and does not lead to a degradation of the overall recognition accuracy . in our experiments we measured recall and precision rates of oov-detection which are close to their theoretic optimum . furthermore , we compared the effect of using crossword - triphones vs. using context-independent crossword models . we show that when using generalized word models with crossword - triphones , the expected number of consequential errors following an oov word can be reduced significantly by 37 % .
we present a new language pair agnostic approach to inducing bilingual vector spaces from non-parallel data without any other resource in a bootstrapping fashion . the paper systematically introduces and describes all key elements of the language pair agnostic approach : -lrb- 1 -rrb- starting point or seed lexicon , -lrb- 2 -rrb- the confidence estimation and selection of new dimensions of the space , and -lrb- 3 -rrb- convergence . we test the quality of the induced bilingual vector spaces , and analyze the influence of the different components of the language pair agnostic approach in the task of bilingual lexicon extraction for two language pairs . results reveal that , contrary to conclusions from prior work , the seeding of the language pair agnostic approach has a heavy impact on the quality of the learned lexicons . we also show that our language pair agnostic approach outperforms the best performing fully corpus-based ble methods on these test sets .
halftone visual cryptography -lrb- halftone visual cryptography -rrb- is a visual sharing scheme where a secret image is encoded into halftone shares taking meaningful visual information . in this paper , novel construction method of halftone visual cryptography based on an iterative halftoning method is proposed . the secret image is concurrently embedded into binary valued shares while these shares are halftoned by constrained iterative halftoning . the proposed construction method is able to generate halftone shares showing natural images with high image quality . reconstructed secret images , obtained by stacking qualified shares together , does not suffer from cross interference of share images . simulations are provided to show the effectiveness of our proposed construction method .
in this paper we study 2-d nonseparable filter banks that annihilate information along a certain discrete direction . this is done by having filters with directional vanishing moments . we study the approximation property of such filters and the design problem providing conditions for its solvability . in particular we completely characterize the solution and propose a design procedure utilizing the mapping technique . nonlinear approximation experiments with the contourlet transform indicate that compared with the traditional filters , the new filters designed with directional vanishing moments provide gains in snr and visual quality due to their short size .
transductive inference on graphs has been garner-ing increasing attention due to the connected nature of many real-life data sources , such as online social media and biological data -lrb- protein-protein interaction network , gene networks , etc. -rrb- . typically rela-tional information in the data is encoded as edges in a graph but often it is important to model multi-way interactions , such as in collaboration networks and reaction networks . in this work we model multi-way relations as hypergraphs and extend the dis-criminative random walk framework , originally proposed for transductive inference on single graphs , to the case of multiple hypergraphs . we use the extended drw framework for inference on multi-view , multi-relational data in a natural way , by representing attribute descriptions of the data also as hypergraphs . we further exploit the structure of hypergraphs to modify the random walk operator to take into account class imbalance in the data . this work is among very few approaches to explicitly address class imbalance in the in-network classification setting , using random walks . we compare our approach to methods proposed for inference on hypergraphs , and to methods proposed for multi-view data and show that empirically we achieve better performance . we also compare to methods specifically tailored for class-imbalanced data and show that our approach achieves comparable performance even on non-network data .
in this paper , we propose a novel mechanism for enriching the feature vector , for the task of sarcasm detection , with cogni-tive features extracted from eye-movement patterns of human readers . sarcasm detection has been a challenging research problem , and its importance for nlp applications such as review summarization , dialog systems and sentiment analysis is well recognized . sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds . this presence of incongruity-implicit or explicit-affects the way readers eyes move through the text . we observe the difference in the behaviour of the eye , while reading sarcastic and non sarcastic sentences . motivated by this observation , we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data . we perform statistical classification using the enhanced feature set so obtained . the linguistic and stylistic features improve sarcasm detection by 3.7 % -lrb- in terms of f-score -rrb- , over the performance of the best reported system .
continuous-wave time-of-flight -lrb- tof -rrb- range imaging has become a commercially viable technology with many applications in computer vision and graphics . however , the depth images obtained from tof cameras contain scene dependent errors due to multipath interference . specifically , multipath interference occurs when multiple optical reflections return to a single spatial location on the imaging sensor . many prior approaches to rectifying multipath interference rely on sparsity in optical reflections , which is an extreme simplification . in this paper , we correct multipath interference by combining the standard measurements from a tof camera with information from direct and global light transport . we report results on both simulated experiments and physical experiments -lrb- using the kinect sensor -rrb- . our results , evaluated against ground truth , demonstrate a quantitative improvement in depth accuracy .
latent variable models are powerful tools for probabilistic modeling , and have been successfully applied to various domains , such as speech analysis and bioinformatics . however , parameter learning algorithms for latent variable models have predominantly relied on local search heuris-tics such as expectation maximization . we propose a fast , local-minimum-free spectral algorithm for learning latent variable models with arbitrary tree topologies , and show that the joint distribution of the observed variables can be reconstructed from the marginals of triples of observed variables irrespective of the maximum degree of the tree . we demonstrate the performance of our local-minimum-free spectral algorithm on synthetic and real datasets ; for large training sizes , our local-minimum-free spectral algorithm performs comparable to or better than em while being orders of magnitude faster .
many image communication systems have constraints on bandwidth , power and time which prohibit transmission of uncompressed raw image data . compressed image formats , however , are extremely sensitive to bit errors which can seriously degrade the quality of the image at the receiver . a new list-based iterative trellis decoder is proposed which accepts feedback from a post-processor which can detect channel errors in the reconstructed image . experimental results are shown which indicate the new list-based iterative trellis decoder provides signiicant improvement over the standard viterbi decoder .
in http adaptive streaming applications multiple video clients sharing the same wireless channel may experience different video qualities as result of both different video content complexity and different channel conditions . this causes unfairness in the end-user video quality . in this paper , we propose a quality-fair adaptive streaming solution to deliver fair video quality to has clients competing for the same resources in an lte cell . in the quality-fair adaptive streaming solution the share of radio resource is optimized according to video content characteristics and channel condition . the proposed quality-fair adaptive streaming solution is compared with other state-of-the-art strategies and numerical results in terms of ssim quality metric shows that quality-fair adaptive streaming solution significantly improves the quality fairness among heterogeneous has users .
we introduce multiclass kernel projection machines , a new formalism that extends the kernel projection machine framework to the multiclass case . our multiclass kernel projection machines is based on the use of output codes and multiclass kernel projection machines implements a co-regularization scheme by simultaneously constraining the projection dimensions associated with the individual predictors that constitute the global classifier . in order to solve the optimization problem posed by our multiclass kernel projection machines , we propose an efficient dynamic programming approach . numerical simulations conducted on a few pattern recognition problems illustrate the soundness of our multiclass kernel projection machines .
we analyze the complexity of reasoning with cir-cumscribed low-complexity dls such as dl-lite and the el family , under suitable restrictions on the use of abnormality predicates . we prove that in circumscribed dl-lite r complexity drops from nexp np to the second level of the polynomial hierarchy . in el , reasoning remains exptime-hard , in general . however , by restricting the possible occurrences of existential restrictions , we obtain membership in σ p 2 and π p 2 for an extension of el .
the automatic prediction of the quality of a dialogue is useful to keep track of a spoken dialogue system 's performance and , if necessary , adapt its behaviour . classifiers and regression models have been suggested to make this prediction . the parameters of these models are learnt from a corpus of dialogues evaluated by users or experts . in this paper , we propose to model this task as an ordinal regression problem . we apply support vector machines for ordinal regression on a corpus of dialogues where each system-user exchange was given a rate on a scale of 1 to 5 by experts . compared to previous models proposed in the literature , the ordinal regression predictor has significantly better results according to the following evaluation metrics : cohen 's agreement rate with experts ratings , spearman 's rank correlation coefficient , and euclidean and manhattan errors .
this paper presents a novel approach to improve the named entity translation by combining a translit-eration approach with web mining , using web information as a source to complement transliteration , and using transliteration information to guide and enhance web mining . a maximum entropy model is employed to rank translation candidates by combining pronunciation similarity and bilingual con-textual co-occurrence . experimental results show that our approach effectively improves the precision and recall of the named entity translation by a large margin .
overview ● objective : create a pronunciation estimator that is robust and adaptable to new domains ● focus on japanese/chinese , where we must estimate word boundaries as well ● approach : pointwise prediction , which tags all word boundaries and pronunciations independently ● pointwise prediction : ● robust : relies on dictionaries less than previous methods ● adaptable : it can be learned from single annotated words , not full sentences ● evaluation on japanese pronunciation estimation shows improvement over traditional joint n-gram
in this paper , we discuss a computational approach to the cognitive task of social planning . first , we specify a class of planning problems that involve an agent who attempts to achieve its goals by altering other agents ' mental states . next , we describe sfps , a flexible problem solver that generates social plans of this sort , including ones that include deception and reasoning about other agents ' beliefs . we report the results for experiments on social scenarios that involve different levels of sophistication and that demonstrate both sfps 's capabilities and the sources of its power . finally , we discuss how our computational approach to social planning has been informed by earlier work in the area and propose directions for additional research on the topic .
low-rank matrix approximation is an integral component of tools such as principal component analysis , as well as is an important instrument used in applications like web search , text mining and computer vision , e.g. , face recognition . recently , randomized algorithms were proposed to effectively construct low rank approximations of large matrices . in this paper , we show how matrices from error correcting codes can be used to find such low rank approximations . the benefits of using these error correcting codes are the following : -lrb- i -rrb- they are easy to generate and they reduce randomness significantly . -lrb- ii -rrb- code matrices have low coherence and have a better chance of preserving the geometry of an entire subspace of vectors ; -lrb- iii -rrb- unlike fourier transforms or hadamard matrices , which require sampling o -lrb- k log k -rrb- columns for a rank-k approximation , the log factor is not necessary in the case of error correcting codes . -lrb- iv -rrb- under certain conditions , the approximation errors can be better and the singular values obtained can be more accurate , than those obtained using gaussian random matrices and other structured random matrices .
the bayesian paradigm provides a natural and effective means of exploiting prior knowledge concerning the time-frequency structure of sound signals such as speech and music -- something which has often been overlooked in traditional audio signal processing approaches . here , after constructing a bayesian model and prior distributions capable of taking into account the time-frequency characteristics of typical audio waveforms , we apply markov chain monte carlo methods in order to sample from the resultant posterior distribution of interest . we present speech enhancement results which compare favourably in objective terms with standard time-varying filtering techniques -lrb- and in several cases yield superior performance , both objectively and subjectively -rrb- ; moreover , in contrast to such methods , our results are obtained without an assumption of prior knowledge of the noise power .
the simulation of speech by means of speech synthesis involves , among other things , the ability to mimic typical delivery for different speech styles . this requires a realistic imitation of the manner in which speakers organize their information flow in time -lrb- i.e. , word grouping boundaries -rrb- , as well their speech rate with its variations . the originality of our model is grounded in two levels . first , it is assumed that the temporal component plays a dominant role in the simulation of speech rhythm , whereas in traditional language models , temporal issues are mostly put aside . second , the outcome of our temporal modeling , based on statistical analysis and qualitative parameters , results from the harmonization of various layers -lrb- segmental , syllabic , phrasal -rrb- . the benefit of a multidimensional model is the possibility of imposing subtle quantitative and qualitative effects at various levels , which is a key for respecting a specific language system as well as speech coherence and fluency for different speech styles .
to precisely model the time dependency of features , segmental unit input hmm with a dimensionality reduction method has been widely used for speech recognition . linear discriminant analysis and heteroscedastic discriminant analysis are popular approaches to reduce the dimen-sionality . we have proposed another dimensionality reduction method called power linear discriminant analysis to select the best dimensionality reduction method that yields the highest relative recognition performance . this dimensionality reduction method on the basis of trial and error requires much time to train hmms and to test the relative recognition performance for each dimensionality reduction method . in this paper we propose a performance comparison method without training or testing . we show that the proposed performance comparison method using the chernoff bound can rapidly and accurately evaluate the relative recognition performance .
the performance of speech recognition systems trained in quiet degrades significantly under noisy conditions . to address this problem , a weighted viterbi recognition algorithm that is a function of the snr of each speech frame is proposed . acoustic models trained on clean data , and the acoustic front-end features are kept unchanged in this weighted viterbi recognition algorithm . instead , a confidence/robustness factor is assigned to the output observation probability of each speech frame according to its snr estimate during the viterbi decoding stage . comparative experiments are conducted with weighted viterbi recognition with different front-end features such as mfcc , lpcc and plp . results show consistent improvements with all three feature vectors . for a reasonable size of adaptation data , weighted viterbi recognition algorithm outperforms environment adaptation using plp .
we present two robust detectors for nonstationary random signals that belong to p-point uncertainty classes , one being based on an estimator-correlator approach , the other using the deflection criterion . apart of stable performance , these robust detectors have the advantage of requiring only reduced prior knowledge . using local cosine bases , we provide an intuitive and highly efficient time-frequency implementation of these robust detectors along with an extension that permits signal-adaptive operation . simulation results illustrate the robustness of the proposed robust detectors .
the precision-recall curve is a widely used visual tool to evaluate the performance of scoring functions in regards to their capacities to discriminate between two populations . the purpose of this paper is to examine both theoretical and practical issues related to the statistical estimation of pr curves based on classification data . consistency and asymptotic normality of the empirical counterpart of the pr curve in sup norm are rigorously established . eventually , the issue of building confidence bands in the pr space is considered and a specific resampling procedure based on a smoothed and truncated version of the empirical distribution of the data is promoted . arguments of theoretical and computational nature are presented to explain why such a bootstrap is preferable to a `` naive '' bootstrap in this setup .
we investigate evaluation metrics for end-to-end dialogue systems where supervised labels , such as task completion , are not available . recent works in end-to-end dialogue systems have adopted metrics from machine translation and text sum-marization to compare a model 's generated response to a single target response . we show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains . we provide quantitative and qualitative results highlighting specific weaknesses in existing metrics , and provide recommendations for future development of better automatic evaluation metrics for end-to-end dialogue systems .
in social choice settings with strict preferences , random dictatorship rules were characterized by gibbard -lsb- 1977 -rsb- as the only randomized social choice functions that satisfy strategyproofness and ex post efficiency . in the more general domain with indifferences , rsd -lrb- random serial dictatorship -rrb- rules are the well-known and perhaps only known generalization of random dictatorship . we present a new generalization of random dictatorship for indifferences called maximal recursive rule as an alternative to rsd . we show that maximal recursive rule is polynomial-time computable , weakly strategyproof with respect to stochastic dominance , and , in some respects , outperforms rsd on efficiency .
this paper investigates a new approach for training discriminant classifiers when only a small set of labeled data is available together with a large set of unlabeled data . this algorithm optimizes the classification maximum likelihood of a set of labeled-unlabeled data , using a variant form of the classification expectation maximization algorithm . its originality is that it makes use of both un-labeled data and of a probabilistic misclassification model for these data . the parameters of the label-error model are learned together with the classifier parameters . we demonstrate the effectiveness of the approach on four data-sets and show the advantages of this method over a previously developed semi-supervised algorithm which does not consider imperfections in the labeling process .
analysis of personal audio recordings is a challenging and interesting subject . using contemporary speech and language processing techniques , it is possible to mine personal audio recordings for a wealth of information that can be used to measure a person 's engagement with their environment as well as other people . in this study , we propose an analysis system that uses personal audio recordings to automatically estimate the number of unique people and environments which encompass the total engagement within the recording . the proposed analysis system uses speech activity detection , speaker diarization and environmental sniffing techniques , and is evaluated on naturalistic audio streams from the prof-life-log corpus . we also report performance of the individual systems , and also present a combined analysis which reveals the interaction of the subject with both people and environment . hence , this study establishes the efficacy and novelty of using contemporary speech technology for life logging applications .
we propose two very effective high-level binary-class features to enhance model-based skin color detection . first we find that the log likelihood ratio of the testing data between skin and non-skin rgb models can be a good discriminative feature . we also find that namely the background-foreground correlation provides another complementary feature compared to the conventional low-level rgb feature . further improvement can be accomplished by bayesian model adaptation and feature fusion . by jointly considering both schemes of bayesian model adaptation and feature fusion , we attain the best system performance . experimental results show that the proposed high-level binary-class features improves the 68 % to 84 % baseline f1 scores to as high as almost 90 % in a wide range of lighting conditions .
tumor segmentation from mri data is an important but time consuming task performed manually by medical experts . automating this process is challenging due to the high diversity in appearance of tumor tissue , among different patients and , in many cases , similarity between tumor and normal tissue . one other challenge is how to make use of prior information about the appearance of normal brain . in this paper we propose a variational brain tumor seg-mentation algorithm that extends current approaches from texture segmentation by using a high dimensional feature set calculated from mri data and registered atlases . using manually segmented data we learn a statistical model for tumor and normal tissue . we show that using a conditional model to discriminate between normal and abnormal regions significantly improves the segmentation results compared to traditional generative models . validation is performed by testing the variational brain tumor seg-mentation algorithm on several cancer patient mri scans .
we propose a probabilistic transfer learning model that uses task-level features to control the task mixture selection in a hierarchical bayesian model . these task-level features , although rarely used in existing approaches , can provide additional information to probabilistic transfer learning model complex task distributions and allow effective transfer to new tasks especially when only limited number of data are available . to estimate the model parameters , we develop an empirical bayes method based on variational approximation techniques . our experiments on information retrieval show that the proposed probabilistic transfer learning model achieves significantly better performance compared with other transfer learning methods .
we design minimal temporal description logics that are capable of expressing various aspects of temporal conceptual data models and investigate their computational complexity . we show that , depending on the required types of temporal and atemporal constraints , the satisfiability problem for temporal knowledge bases in the resulting logics can be nlogspace - , np-and pspace-complete , as well as undecidable .
based on psychological attribution theory , this paper presents a domain-independent computational model to automate social causality and responsibility judgment according to an agent 's causal knowledge and observations of interaction . the proposed domain-independent computational model is also empirically validated via experimental study .
in this paper , we adopt general-sum stochas-tic games as a framework for multiagent reinforcement learning . our work extends previous work by littman on zero-sum stochas-tic games to a broader framework . we design a multiagent q-learning method under this framework , and prove that multiagent q-learning method converges to a nash equilibrium under speciied conditions . this multiagent q-learning method is useful for nding the optimal strategy when there exists a unique nash equilibrium in the game . when there exist multiple nash equilibria in the game , this multiagent q-learning method should be combined with other learning techniques to nd optimal strategies .
colorization aims at recovering the original color of a monochrome image from only a few color pixels . a state-of-the-art approach is based on matrix completion , which assumes that the target color image is low-rank . however , this low-rank assumption is often invalid on natural images . in this paper , we propose a patch-based approach that divides the image into patches and then imposes a low-rank structure only on groups of similar patches . each local matrix completion problem is solved by an accelerated version of alternating direction patch-based approach of multipliers -lrb- admm -rrb- , and each admm subproblem is solved efficiently by divide-and-conquer . experiments on a number of benchmark images demonstrate that the proposed patch-based approach outperforms existing approaches .
this paper introduces the rl-tops architecture for robot learning , a hybrid system combining teleo-reactive planning and reinforcement learning techniques . the aim of this rl-tops architecture is to speed up learning by decomposing complex tasks into hierarchies of simple behaviours which can be learnt more easily . behaviours learnt in this way can subsequently be re-used to solve a variety of problems , reducing the need to learn every new task from scratch . it is even possible to learn multiple behaviours simultaneously , thus making more eecient use of experience . we demonstrate these advantages in a simple simulated environment .
transduction is an inference principle that takes a training sample and aims at estimating the values of a function at given points contained in the so-called working sample as opposed to the whole of input space for induction . transduction provides a confidence measure on single predictions rather than classifiers-a feature particularly important for risk-sensitive applications . the possibly infinite number of functions is reduced to a finite number of equivalence classes on the working sample . a rigorous bayesian analysis reveals that for standard classification loss we can not benefit from considering more than one test point at a time . the probability of the label of a given test point is determined as the posterior measure of the corresponding subset of hypothesis space . we consider the pac setting of binary classification by linear discriminant functions -lrb- perceptrons -rrb- in kernel space such that the probability of labels is determined by the volume ratio in version space . we suggest to sample this region by an ergodic billiard . experimental results on real world data indicate that bayesian analysis compares favourably to the well-known support vector machine , in particular if the posterior probability of labellings is used as a confidence measure to exclude test points of low confidence .
it is well known that the lowest tone in mandarin , a language without contrastive phonation , often co-occurs with laryngeal-ization/creaky voice quality , and we provide evidence that this is also the case for the lowest tone in cantonese . however , the effects of laryngealization on f0 feature extraction for tonal recognition , as well as the potential of laryngealization as a feature for improving tonal recognition , have not been well-discussed in the literature . we give evidence from a corpora of tonal production data for cantonese and mandarin that laryngealization is prevalent and significantly disturbs the extraction of f0 features , and suggest that laryngealization may in fact be a feature that could improve tonal recognition .
current system combination methods usually use confusion networks to find consensus translations among different systems . requiring one-to-one mappings between the words in candidate translations , confusion networks have difficulty in handling more general situations in which several words are connected to another several words . instead , we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations . experiments show that our lattice-based system combination model achieves significant improvements over the state-of-the-art baseline system on chinese-to-english translation test sets .
in this paper we consider the problem of visual saliency modeling , including both human gaze prediction and salient object segmentation . the overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models . a deep learning model based on fully convolutional networks -lrb- fcns -rrb- is presented , which shows very favorable performance across a wide variety of benchmarks relative to existing proposals . we also demonstrate that the manner in which training data is selected , and ground truth treated is critical to resulting model behaviour . recent efforts have explored the relationship between human gaze and salient objects , and we also examine this point further in the context of fcns . close examination of the proposed and alternative deep learning model serves as a vehicle for identifying problems important to developing more comprehensive deep learning model going forward .
graph partitioning active contours -lrb- graph partitioning active contours -rrb- is a recently introduced approach that elegantly embeds the graph-based image segmentation problem within a continuous optimization framework . graph partitioning active contours can be used within parametric snake-based or implicit level set-based active contour continuous paradigms for image partitioning . however , graph partitioning active contours similar to many other graph-based approaches has quadratic memory requirements which severely limits the scalability of the algorithm to practical problem domains . an n xn image requires o -lrb- n -lrb- 4 -rrb- -rrb- computation and memory to create and store the full graph of pixel inter-relationships even before the start of the contour optimization process . for example , an 1024x1024 grayscale image needs over one terabyte of memory . approximations using tile/block-based or superpixel-based multiscale grouping of the pixels reduces this complexity by trading off accuracy . this paper describes a new algorithm that implements the exact gpac algorithm using a constant memory requirement of a few kilobytes , independent of image size .
by building acoustic phonetic models which explicitly represent as much knowledge of pronunciation in a small domain -lrb- the digits -rrb- as possible , we can create a recognition system which not only performs well but allows for meaningful error analysis and improvement . an hmm-based recognizer for the digits and a few associated words was constructed in accord with these principles . about 65 phonetic models were trained on 140 carefully labeled utterances , then iteratively trained on unlabeled data under orthographic supervision . the basic recognition system achieved less than 3 % word error rate on digit strings of unknown length from unseen test speakers , and 1.4 % on 7-digit strings of known length . this is competitive with word-based models using the same hmm engine and similar parameter settings . as an recognition system , recognition system allows meaningful analysis of errors and relatively straightforward means of improvement .
in this paper we investigate the face recognition problem via the overlapping energy histogram of the dct coefficients . particularly , we investigate some important issues relating to the recognition performance , such as the issue of selecting threshold and the number of bins . these parameter selection methods utilise information obtained from the training dataset . experimentation is conducted on the yale face database and results indicate that the proposed parameter selection methods perform well in selecting the threshold and number of bins . furthermore , we show that the proposed overlapping energy histogram approach outperforms the eigen-faces , 2dpca and energy histogram significantly .
there is considerable interest in techniques capable of identifying anomalies and unusual events in busy outdoor scenes , e.g. road junctions . many approaches achieve this by exploiting deviations in spatial appearance from some expected norm accumulated by a model over time . in this work we show that much can be gained from explicitly modelling temporal aspects in detail . specifically , many traffic junctions are regulated by lights controlled by a timing device of considerable precision , and it is in these situations that we advocate a model which learns periodic spatio-temporal patterns with a view to highlighting anomalous events such as broken-down vehicles , traffic accidents , or pedestrians jay-walking . more specifically , by estimating autocovariance of self-similarity , used previously in the context gait recognition , we characterize a scene by identifying a global fundamental period . as our model , we introduce a spatio-temporal grid of histograms built in accordance with some chosen feature . this model is then used to classify objects found in subsequent test data . in particular we demonstrate the effect of such characterization experimentally by monitoring the bounding box aspect ratio and optical flow field of objects detected on a road traffic junction , enabling our model to discriminate between people and cars sufficiently well to provide useful warnings of adverse behaviour in real time .
the iterative closest point algorithm -lsb- 2 -rsb- is a popular method for modeling 3d objects from range data . the classical iterative closest point algorithm rests on a rigid surface assumption . building on recent work on nonrigid object models -lsb- 5 ; 16 ; 9 -rsb- , this paper presents an iterative closest point algorithm capable of model-ing nonrigid objects , where individual scans may be subject to local deformations . we describe an integrated mathematical framework for simultaneously registering scans and recovering the surface configuration . to tackle the resulting high-dimensional optimization problems , we introduce a hierarchical method that first matches a coarse skeleton of scan points , then adapts local scan patches . the hierarchical method is implemented for a mobile robot capable of acquiring 3d models of objects .
markov random field is now ubiquitous in many formulations of various vision problems . recently , optimization of higher-order potentials became practical using higher-order graph cuts : the combination of i -rrb- the fusion move algorithm , ii -rrb- the reduction of higher-order binary energy minimization to first-order , and iii -rrb- the fusion move algorithm . in the fusion move , it is crucial for the success and efficiency of the optimization to provide proposals that fits the energies being optimized . for higher-order energies , it is even more so because they have richer class of null potentials . in this paper , we focus on the efficiency of the higher-order graph cuts and present a simple technique for generating proposal labelings that makes the algorithm much more efficient , which we empirically show using examples in stereo and image denoising .
this paper presents a bayesian technique aimed at classifying signals without prior training -lrb- clustering -rrb- . the bayesian technique consists of modelling the observed signals , known only through a finite set of samples corrupted by noise , as gaussian processes . as in many other bayesian clustering approaches , the clusters are defined thanks to a mixture model . in order to estimate the number of clusters , we assume a priori a countably infinite number of clusters , thanks to a dirichlet process model over the gaussian processes parameters . computations are performed thanks to a dedicated monte carlo markov chain algorithm , and results involving real signals -lrb- mrna expression profiles -rrb- are presented .
the reliability and accuracy of spike trains have been shown to depend on the nature of the stimulus that the neuron encodes . adding ion channel stochasticity to neuronal models results with a macroscopic behavior that replicates the input-dependent reliability and precision of real neurons . we calculate the amount of information that an ion channel based stochastic hodgkin-huxley -lrb- hh -rrb- neuron model can encode about a wide set of stimuli . we show that both the information rate and the information per spike of the ion channel stochasticity is similar to the values reported experimentally . moreover , the amount of information that the neuron encodes is correlated with the amplitude of fluctuations in the input , and less so with the average firing rate of the neuron . we also show that for the hh ion channel density , the information capacity is robust to changes in the density of ion channels in the membrane , whereas changing the ratio between the na + and k + ion channels has a considerable effect on the information that the neuron can encode . this suggests that neurons may maximize their information capacity by appropriately balancing the density of the different ion channels that underlies neuronal excitability .
we present a method for learning image representations using a two-layer sparse coding scheme at the pixel level . the rst layer encodes local patches of an image . after pooling within local regions , the rst layer codes are then passed to the second layer , which jointly encodes signals from the region . unlike traditional sparse coding methods that encode local patches independently , this approach accounts for high-order dependency among patterns in a local image neighborhood . we develop algorithms for data encoding and codebook learning , and show in experiments that the method leads to more invariant and discriminative image representations . the algorithm gives excellent results for handwritten digit recognition on mnist and object recognition on the caltech101 benchmark . this marks the rst time that such accuracies have been achieved using automatically learned features from the pixel level , rather than using hand-designed descriptors .
oral , head and neck cancer represents 3 % of all cancers in the united states and is the 6th most common cancer worldwide . depending on the tumor size , location and staging , patients are treated by radical surgery , radiology , chemotherapy or a combination of those treatments . as a result , their anatomical structures for speech are impaired and this leads to some negative impact on their speech intelligibility . as a part of the interspeech 2012 speaker trait pathology sub-challenge , this study explored the use of auditory-inspired spectro-temporal modulation features for automatic speech intelligibility assessment of those pathologic speech . the averaged spectro-temporal modulations of speech considered as either intelligible or non-intelligible in the challenge database were analyzed and it was found that the non-intelligible speech tends to have its modulation amplitude peaks shift towards a smaller rate and scale . based on svm and gmm , variants of spectro-temporal modulation features were tested on the speaker trait challenge problem and the resulting performances on both the development and the test datasets are comparable to the baseline performance .
this paper describes the design and implementation of a 16-bit fixed point dsp processor . the 16-bit fixed point dsp processor is intended as a platform for hardware accelerators and allows additional computational units and assembler instructions to be added . the i/o facilities can also be customized to the needs of a specific application . benchmarking has shown that the 16-bit fixed point dsp processor , without any hardware accelerators , has a performance comparable to single mac commercial dsp processors . the architecture has been successfully synthesized in a 0.13 m process , resulting in a net-list of about 23000 gates , and a clock frequency of 195 mhz , making the performance/gate count ratio very competitive . it is also small enough to integrate 100 heterogeneous processors on a chip for example for communication infrastructure applications . the complete design time , including architecture and instruction set planning , as-sembler , debugger , instruction set simulator , rtl code and complete verification was about half a person-year .
we report on a series of experiments with convolutional neural networks trained on top of pre-trained word vectors for sentence-level classification tasks . we show that a simple convolutional neural networks with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks . learning task-specific vectors through fine-tuning offers further gains in performance . we additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors . the cnn models discussed herein improve upon the state of the art on 4 out of 7 tasks , which include sentiment analysis and question classification .
models that acquire semantic representations from both linguistic and perceptual input are of interest to researchers in nlp because of the obvious parallels with human language learning . performance advantages of the multi-modal approach over language-only models have been clearly established when models are required to learn concrete noun concepts . however , such concepts are comparatively rare in everyday language . in this work , we present a new means of extending the scope of multi-modal approach to more commonly-occurring abstract lexical concepts via an approach that learns multi-modal embeddings . our architecture out-performs previous approaches in combining input from distinct modalities , and propagates perceptual information on concrete concepts to abstract concepts more effectively than alternatives . we discuss the implications of our results both for optimizing the performance of multi-modal approach and for theories of abstract conceptual representation .
in order to properly regulate iterated belief revision , darwiche and pearl -lrb- 1997 -rrb- model belief revision as revising epistemic states by propositions . an epistemic state in their sense consists of a belief set and a set of conditional beliefs . although the denotation of an epis-temic state can be indirectly captured by a total preorder on the set of worlds , it is unclear how to directly capture the structure in terms of the beliefs and conditional beliefs it contains . in this paper , we first provide an ax-iomatic characterisation for epistemic states by using nine rules about beliefs and conditional beliefs , and then argue that the last two rules are too strong and should be eliminated for characterising the belief state of an agent . we call a structure which satisfies the first seven rules a general epistemic state . to provide a semantical characterisation of geps , we introduce a mathematical structure called belief algebra , which is in essence a certain binary relation defined on the power set of worlds . we then establish a 1-1 correspondence between epistemic state and belief algebras , and show that total preorders on worlds are special cases of belief algebras . furthermore , using the notion of belief algebras , we extend the classical iterated belief revision rules of darwiche and pearl to our setting of general epistemic states .
the autoregressive model is a well-known technique to analyze time series . the yule-walker equations provide a straightforward connection between the ar model parameters and the covariance function of the process . in this paper , we propose a nonlinear extension of the autoregressive model using kernel machines . to this end , we explore the yule-walker equations in the feature space , and show that the model parameters can be estimated using the concept of expected kernels . finally , in order to predict once the autoregressive model identified , we solve a pre-image problem by getting back from the feature space to the input space . we also give new insights into the convex-ity of the pre-image problem . the relevance of the proposed method is evaluated on several time series .
as embedded systems grow increasingly complex , there is a pressing need for diagnosing and monitoring capabilities that estimate the system state robustly . this paper is based on approaches that address the problem of robustness by reasoning over declarative models of the physical plant , represented as a variant of factored hidden markov models , called probabilistic concurrent constraint automata . prior work on mode estimation of pccas is based on a best-first trajec-tory enumeration algorithm . two algorithms have since made improvements to the best-first trajec-tory enumeration algorithm : 1 -rrb- the best-first belief state update algorithm has improved the accuracy of best-first trajec-tory enumeration algorithm and 2 -rrb- the mexec algorithm has introduced a polynomial-time bounded algorithm using a smooth deterministic decomposable negation normal form -lrb- sd-dnnf -rrb- representation . this paper introduces a new dnnf-based belief state estimation algorithm that merges the polynomial time bound of the mexec algorithm with the accuracy of the best-first trajec-tory enumeration algorithm . this paper also presents an encoding of a dnnf-based belief state estimation algorithm as a cnf with probabilistic data , suitable for compilation into an sd-dnnf-based representation . the sd-dnnf-based representation supports computing k belief states from k previous belief states in the best-first trajec-tory enumeration algorithm .
this paper introduces an approach of creating face makeup upon a face image with another image as the style example . our approach is analogous to physical makeup , as we modify the color and skin detail while preserving the face structure . more precisely , we first decompose the two images into three layers : face structure layer , skin detail layer , and color layer . thereafter , we transfer information from each layer of one image to corresponding layer of the other image . one major advantage of the proposed method lies in that only one example image is required . this renders face makeup by example very convenient and practical . equally , this enables some additional interesting applications , such as applying makeup by a portraiture . the experiment results demonstrate the effectiveness of the proposed approach in faithfully transferring makeup .
most of the state-of-the-art sentiment classification methods are based on supervised learning algorithms which require large amounts of manually labeled data . however , the labeled resources are usually imbalanced in different languages . cross-lingual sentiment classification tackles the problem by adapting the sentiment resources in a resource-rich language to resource-poor languages . in this study , we propose an attention-based bilingual representation learning model which learns the distributed semantics of the documents in both the source and the target languages . in each language , we use long short term memory network to hierarchical attention mechanism the documents , which has been proved to be very effective for word sequences . meanwhile , we propose a hierarchical attention mechanism for the bilingual lstm network . the sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive . the proposed hierarchical attention mechanism achieves good results on a benchmark dataset using english as the source language and chinese as the target language .
two seemingly contradictory theories attempt to explain how humans move to intercept an airborne ball . one theory posits that humans predict the ball trajectory to optimally plan future actions ; the other claims that , instead of performing such complicated computations , humans employ heuristics to reactively choose appropriate actions based on immediate visual feedback . in this paper , we show that interception strategies appearing to be heuristics can be understood as computational solutions to the optimal control problem faced by a ball-catching agent acting under uncertainty . modeling catching as a continuous partially observable markov decision process and employing stochastic optimal control theory , we discover that the four main heuristics described in the literature are optimal solutions if the catcher has sufficient time to continuously visually track the ball . specifically , by varying model parameters such as noise , time to ground contact , and perceptual latency , we show that different strategies arise under different circumstances . the catcher 's policy switches between generating reactive and predictive behavior based on the ratio of system to observation noise and the ratio between reaction time and task duration . thus , we provide a rational account of human ball-catching behavior and a unifying explanation for seemingly contradictory theories of target interception on the basis of stochastic optimal control .
in this paper we explore a pos tagging application of neural architectures that can infer word representations from the raw character stream . it relies on two modelling stages that are jointly learnt : a convolutional network that infers a word representation directly from the character stream , followed by a prediction stage . models are evaluated on a pos and morphological tagging task for german . experimental results show that the convolutional network can infer meaningful word representations , while for the prediction stage , a well designed and structured strategy allows the convolutional network to outperform state-of-the-art results , without any feature engineering .
in this work we propose a technique to combine bottom-up segmentation , coming in the form of slic superpixels , with sliding window detectors , such as deformable part models -lrb- dpms -rrb- . the merit of our approach lies in ` cleaning up ' the low-level hog features by exploiting the spatial support of slic superpixels ; this can be understood as using segmentation to split the feature variation into object-specific and background changes . rather than committing to a single seg-mentation we use a large pool of slic superpixels and combine them in a scale - , position-and object-dependent manner to build soft segmentation masks . the segmentation masks can be computed fast enough to repeat this process over every candidate window , during training and detection , for both the root and part filters of dpms . we use these segmentation masks to construct enhanced , background-invariant features to train dpms . we test our approach on the pascal voc 2007 , outperforming the standard dpm in 17 out of 20 classes , yielding an average increase of 1.7 % ap . additionally , we demonstrate the robustness of this approach , extending it to dense sift descriptors for large displacement optical flow .
nowadays , almost all speaker-independent speech recognition systems use cdhmm with multivariate mixture gaussian as observation density to cover speaker variabilities . it has been shown that given sufficient training data , the more mixtures are used in the hmm observation density , the better the speaker-independent speech recognition systems 's perform . however , acoustic hmm with more gaussian densities is more complex and slows down recognition speed . another efficient way to handle speaker variation is to use speaker adaptation . yet , even though speaker adaptation of full multivariate mixture gaussian densities can increase recognition accuracy , it does not improve recognition speed . in this paper , we introduce a principal mixture speaker adaptation method which reduces hmm complexity by choosing only the principle mixtures corresponding to a particular speaker 's characteristics . we show that our principal mixture speaker adaptation method both improves recognition accuracy by 31.8 % when compared to si models , and reduces recognition speed by 30 % , when compared to full mixture sa models .
online camera recalibration is necessary for long-term deployment of computer vision systems . existing algorithms assume that the source of recalibration information is a set of features in a general 3d scene ; and that enough features are observed that the calibration problem is well-constrained . however , these assumptions are frequently invalid outside the laboratory . real-world scenes often lack texture , contain repeated texture , or are mostly planar , making calibration difficult or impossible . in this paper we consider the calibration of families of stereo cameras , where each camera is assumed to have parameters drawn from a common but unknown prior distribution . we show how estimation of this prior using a small-number of offline-calibrated cameras -lrb- e.g. from the same production line -rrb- allows online calibration of additional cameras using a small number of point correspondences ; and that using the estimated prior significantly increases the accuracy and robustness of stereo camera calibration .
the task of speaker diarization consists of answering the question '' who spoke when ? '' . the most commonly used approach to speaker diarization is agglomerative clustering of multiple initial clusters . even though the initial clustering is greatly modified by iterative cluster merging and possibly multiple resegmentations of the data , the initialization algorithm is a key module for system performance and robustness . in this paper we present a novel approach that obtains a desired initial number of clusters in three steps . it first computes possible speaker change points via a standard technique based on the bayesian information criterion . it then classifies the resulting segments into '' friend '' and '' enemy '' groups to finally creates an initial set of clusters for the system . we test this algorithm with the dataset used in the rt05s evaluation , where we show a 13 % diarization error rate relative improvement and a 2.5 % absolute cluster purity improvement with respect to our previous algorithm .
this study examines the ability of a semantic space model to represent the meaning of noun compounds such as '' information gathering '' or '' weather forecast '' . a new algorithm , comparison , is proposed for computing compound vectors from constituent word vectors , and compared with other algorithms -lrb- i.e. , predication and centroid -rrb- in terms of accuracy of multiple-choice synonym test and similarity judgment test . the result of both tests is that the comparison algorithm is , on the whole , superior to other algorithms , and in particular achieves the best performance when noun compounds have emergent meanings . furthermore , the comparison algorithm also works for novel noun compounds that do not occur in the corpus . these findings indicate that a semantic space model in general and the comparison algorithm in particular has sufficient ability to compute the meaning of noun compounds .
work in grammar induction should help shed light on the amount of syntactic structure that is discoverable from raw word or tag sequences . but since most current grammar induction algorithms produce unlabeled dependencies , it is difficult to analyze what types of constructions these grammar induction algorithms can or can not capture , and , therefore , to identify where additional supervision may be necessary . this paper provides an in-depth analysis of the errors made by unsupervised ccg parsers by evaluating unsupervised ccg parsers against the labeled dependencies in ccgbank , hinting at new research directions necessary for progress in grammar induction .
camera calibration is a primary crucial step in many computer vision tasks . in this paper we present a new neural approach for camera c alibration . unlike some existing neural approaches , our calibrating network can tell the perspective-projection-transformation matrix between the world 3d points and the corresponding 2d image pixels . starting from random initial weights , the net can specify the camera m o del parameters satisfying the orthogonality constraints on the rotational transformation . the neural approach is shown to solve four diierent types of calibration problems that are found in computer vision tasks . moreover , neural approach can be extended to the more diicult problem of calibrating cameras with automated active lenses . the validity and performance of our neural approach are tested with both synthetic data under different noise conditions and with real images . experiments have shown the accuracy and the eeciency of our neural approach .
this paper presents two novel regularization methods motivated in part by the geometric significance of biorthogonal bases in signal processing applications . these regularization methods , in particular , draw upon the structural relevance of orthogonality and biorthogonality principles and are presented from the perspectives of signal processing , convex programming , continuation methods and nonlinear projection operators . each method is specifically endowed with either a homotopy or tuning parameter to facilitate tradeoff analysis between accuracy and numerical stability . an example involving a basis comprised of real exponential signals illustrates the utility of the proposed regularization methods on an ill-conditioned inverse problem and the results are compared to standard regularization techniques from the signal processing literature .
while boltzmann machines have been successful at un-supervised learning and density modeling of images and speech data , boltzmann machines can be very sensitive to noise in the data . in this paper , we introduce a novel model , the robust boltz-mann machine -lrb- robm -rrb- , which allows boltzmann machines to be robust to corruptions . in the domain of visual recognition , the robm is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of gaussians over pixels . image denoising and in-painting correspond to posterior inference in the robm . our model is trained in an unsupervised fashion with un-labeled noisy data and can learn the spatial structure of the occluders . compared to standard algorithms , the robm is significantly better at recognition and denoising on several face databases .
introduction in this paper we propose an efficient adaptive support vector machine for time-varying data streams based on the martingale approach -lsb- 2 -rsb- and using adiabatic incremental learning -lsb- 1 -rsb- . when a new data point is observed , hypothesis testing decides whether any change has occurred . once a change is detected , historical information about previous data is removed from the memory . the adaptive support vector machine is a one-pass incre-mental algorithm that 1 . does not require a sliding window on the data stream , 2 . does not require monitoring the performance of the clas-sifier as data points are streaming , and 3 . works well for high dimensional , multi-class data streams .
we introduce structured prediction energy networks , a flexible framework for structured prediction . a deep architecture is used to define an energy function of candidate labels , and then predictions are produced by using back-propagation to iteratively optimize the energy with respect to the labels . this deep architecture captures dependencies between labels that would lead to intractable graphical models , and performs structure learning by automatically learning dis-criminative features of the structured output . one natural application of our technique is multi-label classification , which traditionally has required strict prior assumptions about the interactions between labels to ensure tractable learning and prediction problems . we are able to apply structured prediction energy networks to multi-label problems with substantially larger label sets than previous applications of structured prediction , while modeling high-order interactions using minimal structural assumptions . overall , deep learning provides remarkable tools for learning features of the inputs to a prediction problem , and this work extends these techniques to learning features of the outputs . our experiments provide impressive performance on a variety of benchmark multi-label classification tasks , demonstrate that our technique can be used to provide interpretable structure learning , and illuminate fundamental trade-offs between feed-forward and iterative structured prediction techniques .
the paper introduces new features for describing possible focus variation in a human/human conversation . the application considered is a real-life telephone customer care service . the purpose is to hypothesize the dominant theme of conversations between a casual customer calling . conversations are processed by an automatic speech recognition system that provides hypotheses used for extracting word frequency . features are extracted in different , broadly defined and partially overlapped , time segments . combinations of each feature in different segments are represented in a quaternion algebra framework . the advantage of the proposed features is made evident by the statistically significant improvements in theme classification accuracy .
this work concerns learning probabilistic models for ranking data in a heterogeneous population . the specific problem we study is learning the parameters of a mallows mixture model . despite being widely studied , current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima . we present the first polynomial time algorithm which provably learns the parameters of a mixture of two mallows models . a key component of our polynomial time algorithm is a novel use of tensor decomposition techniques to learn the top-k prefix in both the rankings . before this work , even the question of identifiability in the case of a mixture of two mallows models was unresolved .
since the present-day japanese use of voiced consonant mark had established in the meiji era , modern japanese literary text written in the meiji era often lacks compulsory voiced consonant marks . this deteriorates the performance of morphological analyzers using ordinary dictionary . in this paper , we propose an approach for automatic labeling of voiced consonant marks for modern literary japanese . we formulate the automatic labeling of voiced consonant marks into a binary classification problem . our point-wise prediction method uses as its feature set only surface information about the surrounding character strings . as a consequence , training corpus is easy to obtain and maintain because we can exploit a partially annotated corpus for learning . we compared our proposed method as a pre-processing step for morphological analysis with a dictionary-based approach , and confirmed that pointwise prediction out-performs dictionary-based approach by a large margin .
supervised dimensionality reduction has shown great advantages in finding predictive subspaces . previous methods rarely consider the popular maximum margin principle and are prone to overfitting to usually small training data , especially for those under the maximum likelihood framework . in this paper , we present a posterior-regularized bayesian approach to combine principal component analysis with the max-margin learning . based on the data augmentation idea for max-margin learning and the probabilistic interpretation of principal component analysis , our posterior-regularized bayesian approach can automatically infer the weight and penalty parameter of max-margin learning machine , while finding the most appropriate pca sub-space simultaneously under the bayesian framework . we develop a fast mean-field variational inference algorithm to approximate the posterior . experimental results on various classification tasks show that our posterior-regularized bayesian approach outperforms a number of competitors .
combining multiple classiiers is an eeective technique for improving accuracy . there are many general combining algorithms , such as bagging or error correcting output coding , that signiicantly improve classiiers like decision trees , rule learners , or neural networks . unfortunately , many combining methods do not improve the nearest neighbor classiier . in this paper , we present mfs , a combining algorithm designed to improve the accuracy of the nearest neighbor -lrb- nn -rrb- classiier . mfs combines multiple nn classiiers each using only a random subset of features . the experimental results are encouraging : on 25 datasets from the uci repository , mfs sig-niicantly improved upon the nn , k nearest neighbor -lrb- knn -rrb- , and nn classiiers with forward and backward selection of features . mfs was also robust to corruption by irrelevant features compared to the knn classiier . finally , we show that mfs is able to reduce both bias and variance components of error .
storyline detection from news articles aims at summarizing events described under a certain news topic and revealing how those events evolve over time . it is a difficult task because it requires first the detection of events from news articles published in different time periods and then the construction of storylines by linking events into coherent news stories . moreover , each storyline has different hierarchical structures which are dependent across epochs . existing approaches often ignore the dependency of hierarchical structures in storyline generation . in this paper , we propose an unsupervised bayesian model , called dynamic storyline detection model , to extract structured representations and evolution patterns of storylines . the proposed dynamic storyline detection model is evaluated on a large scale news corpus . experimental results show that our proposed dynamic storyline detection model outperforms several baseline approaches .
opinion leaders play an important role in influencing people 's beliefs , actions and behaviors . although a number of methods have been proposed for identifying influentials using secondary sources of information , the use of primary sources , such as surveys , is still favored in many domains . in this work we present a new active surveying method which combines secondary data with partial knowledge from primary sources to guide the information gathering process . we apply our proposed active surveying method to the problem of identifying key opinion leaders in the medical field , and show how we are able to accurately identify the opinion leaders while minimizing the amount of primary data required , which results in significant cost reduction in data acquisition without sacrificing its integrity .
deductive reasoning and inductive learning are the most common approaches for deriving knowledge . in real world applications when data is dynamic and incomplete , especially those exposed by sensors , reasoning is limited by dynamics of data while learning is biased by data incompleteness . therefore discovering consistent knowledge from incomplete and dynamic data is a challenging open problem . in our approach the semantics of data is captured through ontologies to empower learning -lrb- mining -rrb- with -lrb- description logics -rrb- reasoning . consistent knowledge discovery is achieved by applying generic , significative , representative association semantic rules . the experiments have shown scalable , accurate and consistent knowledge discovery with data from dublin .
-- we present the complex double gaussian distribution that describes the product of two independent , non-zero mean , complex gaussian random variables , a doubly-infinite summation of terms . this distribution is useful in a wide array of problems . we discuss its application to blind tr detection systems by deriving the neyman-pearson optimal detector when the channel is modeled as the product of two independent complex gaussian random variables , such as in a time reversal scenario . we show that near-optimal detection performance can be achieved with as few as 25 summation terms . theoretical analysis and monte carlo simulations illustrate our results .
deep neural networks -lrb- dnns -rrb- have been successfully applied to a variety of automatic speech recognition tasks , both in discriminative feature extraction and hybrid acoustic mod-eling scenarios . the development of improved loss functions and regularization approaches have resulted in consistent reductions in asr word error rates . this paper presents a manifold learning based regularization framework for dnn training . the associated techniques attempt to preserve the underlying low dimensional manifold based relationships amongst speech feature vectors as part of the optimization procedure for estimating network parameters . this is achieved by imposing manifold based locality preserving constraints on the outputs of the network . the techniques are presented in the context of a bottleneck dnn architecture for feature extraction in a tandem configuration . the asr word error rates obtained using these networks is evaluated on a speech-in-noise task and compared to that obtained using dnn-bottleneck networks trained without mani-fold constraints .
we present a large scale effort to build a commercial automatic speech recognition product for arabic . our goal is to support voice search , dictation , and voice control for the general arabic-speaking public , including support for multiple arabic dialects . we describe our commercial automatic speech recognition product and compare recognizers for five arabic dialects , with the potential to reach more than 125 million people in egypt , jordan , lebanon , saudi arabia , and the united arab emirates . we compare systems built on diacritized vs. non-diacritized text . we also conduct cross-dialect experiments , where we train on one dialect and test on the others . our average word error rate is 24.8 % for voice search .
the performance of large vocabulary speech recognizers often varies depending on the input speech and the quality of the trained models . the particular attributes that cause recognition errors are a research area that has not been well studied . this paper addresses this issue from a robustness perspective using a large amount of field data collected from natural language dialog services . in particular , we present a method for tracking time-varying or nonstationary extraneous events , such as music , background noise , etc. . we show that this measure is a better predictor of recognition errors than a standard measure of stationary signal-to-noise ratio . combining the two measures provides a data selection algorithm for detecting problematic speech .
this paper addresses the problem of the supervised signal classification , by using a hierarchical bayesian method . each signal is characterized by a set of parameters , the features , which are estimated from a set of learning signals . moreover , these parameters are distributed according to a class-specific posterior distribution which allows one to capture the variability of the features within the same class . within the hierarchical bayesian method , the feature extraction step and the learning step can be performed jointly . unfortunately , the estimation of the class-specific distribution parameters requires the computation of intractable multi-dimensional integrals . then a markov-chain monte carlo algorithm is used to sample the posterior distributions of the features over all the training signals of each class . an application to electrical transient classification for non-intrusive load monitoring is introduced . simulations over real-world electrical transients signals are driven and show the capacity of the proposed markov-chain monte carlo algorithm to discriminate two classes of transients .
digital fingerprinting is an effective method to identify users who might try to redistribute multimedia content , such as images and video . these digital fingerprinting are typically embedded into the content using watermarking techniques that are designed to be robust to a variety of attacks . a cheap and effective attack against such digital fingerprints is collusion , where several differently marked copies of the same content are averaged or combined to disrupt the underlying fingerprint . in this paper , we study the problem of designing fingerprints that can withstand collusion , yet trace colluders . since , in antipodal cdma-type watermarking , the correlation contributions only decrease where watermarks differ , by constructing binary code vectors where any subset of k or fewer of these vectors have unique overlap , we may identify groups of k or less colluders . our construction of such anti-collusion codes uses the theory of combinatorial designs , and for n users requires o -lrb- √ n -rrb- bits . further , we explore a block matrix structure for the anti-collusion codes that reduces the computational complexity for identifying colluders and improves the detection capability when colluders belong to the same subgroup .
we study an interesting and challenging problem , online streaming feature selection , in which the size of the feature set is unknown , and not all features are available for learning while leaving the number of observations constant . in this problem , the candidate features arrive one at a time , and the learner 's task is to select a '' best so far '' set of features from streaming features . standard feature selection methods can not perform well in this scenario . thus , we present a novel framework based on feature relevance . under this framework , a promising alternative method , online streaming feature selection -lrb- osfs -rrb- , is presented to online select strongly relevant and non-redundant features . in addition to osfs , a faster fast-osfs algorithm is proposed to further improve the selection efficiency . experimental results show that our algorithms achieve more compactness and better accuracy than existing streaming feature selection algorithms on various datasets .
the goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local -lrb- possibly nonsmooth -rrb- convex functions using only local computation and communication . we develop and analyze distributed algorithms based on dual averaging of subgradients , and provide sharp bounds on their convergence rates as a function of the network size and topology . our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure . we show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network . the sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks .
natural images are approximately scale invariant resulting in long range statistical regularities that typically obey a power law . for example , images have considerable regularity in their second order spatial correlations as measured by the power spectrum . processing images to remove these expected correlations is known as whitening an image . because the expected value of the power spectrum has a regular form -lrb- a power law -rrb- linear processing such as con-volution can be used to whiten an image . after whitening an image , higher order regularities that can not be removed with linear processing still exist in the form of correlations in the magnitude . in this paper it is shown that these correlations also obey a power law and a non-linear method is used to remove them , a process referred to as higher order whitening . the method is invertible demonstrating that while redundancy is removed no information is lost . experiments are given showing that after higher order whitening the coefficients can be severely quantized yet a good reconstruction is possible despite the nonlinearities .
computer lipreading is one of the great signal processing challenges . not only is the signal noisy , it is variable . however it is almost unknown to compare the performance with human lip-readers . partly this is because of the paucity of human lip-readers and partly because most automatic systems only handle data that are trivial and therefore not representative of human speech . here we generate a multiview dataset using connected words that can be analysed by an automatic systems , based on linear predictive trackers and active appearance models , and human lip-readers . the automatic systems we devise has a viseme accuracy of ≈ 46 % which is comparable to poor professional human lip-readers . however , unlike human lip-readers our automatic systems is good at guessing its fallibil-ity .
context is crucial for identifying argumentative relations in text , but many argument mining methods make little use of contex-tual features . this paper presents context-aware argumentative relation mining that uses context-aware argumentative relation mining extracted from writing topics as well as from windows of context sentences . experiments on student essays demonstrate that the proposed context-aware argumentative relation mining improve predictive performance in two argumentative relation classification tasks .
ensembles of randomized decision trees , usually referred to as random forests , are widely used for classification and regression tasks in machine learning and statistics . random forests achieve competitive predictive performance and are computationally efficient to train and test , making random forests excellent candidates for real-world prediction tasks . the most popular random forest variants -lrb- such as breiman 's random forest and extremely randomized trees -rrb- operate on batches of training data . online methods are now in greater demand . existing online random forests , however , require more training data than their batch counterpart to achieve comparable predictive performance . in this work , we use mondrian processes -lrb- roy and teh , 2009 -rrb- to construct ensembles of random decision trees we call mondrian forests . mondrian forests can be grown in an incremental/online fashion and remarkably , the distribution of online mondrian forests is the same as that of batch mondrian forests . mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically retrained batch random forests , while being more than an order of magnitude faster , thus representing a better computation vs accuracy tradeoff .
in this paper we present voicebuilder , a voicebuilder for automating the process of developing speech applications . our voicebuilder allows speech user interface specialists to introduce ui 's in two ways : a stand-alone gui application , and a web-based interface ; in which speech ui 's are stored in a markup language previously proposed called suiml -lsb- 1 -rsb- , supporting either system initiative or mixed initiative dialogue strategies . for automatic coding , we propose an algorithm based on a macro-processor that generates voicexml code by parsing suiml documents . this algorithm was designed to generate various kinds of code with a minimal initial effort . we performed experiments considering both system initiative and mixed initiative dialogue strategies with three different speech applications : auto-attendant , e-mail reader , and flight reservations . voicebuilder is very useful for building speech applications in new domains , requires no programming effort and could be incorporated into several voice toolkits .
given a set of 2d images , we propose a novel approach for the reconstruction of straight 3d line segments that represent the underlying geometry of static 3d objects in the scene . such an algorithm is especially useful for the automatic 3d reconstruction of man-made environments . the main contribution of our approach is the generation of an improved reconstruction of straight 3d line segments by imposing global topologi-cal constraints given by connections between neighbouring lines . additionally , our approach does not employ explicit line matching between views , thus making it more robust against image noise and partial occlusion . furthermore , we suggest a technique to merge independent reconstructions , that are generated from different base images , which also helps to remove outliers . the proposed algorithm is evaluated on synthetic and real scenes by comparison with ground truth .
what defines an action like '' kicking ball '' ? we argue that the true meaning of an action lies in the change or transformation an action brings to the environment . in this paper , we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens -lrb- precondition -rrb- to the state after the action -lrb- effect -rrb- . motivated by recent advancements of video representation using deep learning , we design a siamese network which models the action as a transformation on a high-level feature space . we show that our siamese network gives improvements on standard action recognition datasets including ucf101 and hmdb51 . more importantly , our siamese network is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new act dataset .
in this paper , we propose a novel nonparametric modeling technique , namely space kernel analysis , as a result of the definition of the space kernel . we analyze the uncertainty of space kernel analysis and show that space kernel analysis is subjected to the bias/variance dilemma . nevertheless , we demonstrate that , by a proper choice of the space kernel matrix , space kernel analysis is able to balance between the robustness and accuracy and hence outperforms other kernel-based learning methods . the cost function of space kernel analysis is derived , and it proves that space kernel analysis minimizes the weighted least squared cost function whose weight matrix is diagonal and determined by the space kernel matrix . the parallels between space kernel analysis and several other nonparametric modeling techniques are examined . study shows that the traditional kernel regression , general regression neural network , similarity based modeling and radial basis function network are examples of space kernel analysis with specified space kernel matrices .
the problem of representing and reasoning about two notions of time that are relevant in the context of knowledge bases is addressed . these are called historical time and belief time respectively . historical time denotes the time for which information models reality/belief time denotes the time lor which a belief is held -lrb- by an agent or a knowledge base -rrb- . we formalize an appropriate theory of time using logic as a metalanguage . we then present a metalogic program derived from this theory of time through fold/unfold transformations . the metalogic program enables the temporal reasoning required for knowledge base applications to be carried out efficiently . the metalogic program is directly implementable as a prolog program and hence the need for a more complex theorem prover is obviated . the metalogic program is applicable for such knowledge base applications as legislation and legal reasoning and in the context of multi-agent reasoning where an agent reasons about the beliefs of another agent .
during the last decade , speaker verification systems have shown significant progress and have reached a level of performance and accuracy that support their utilization in practical applications , including the forensic ones . this context emphasizes the importance of a deeper analysis of the speaker verification systems 's performance over basic error rate . in this paper , the influence of the speaker -lrb- his/her ` voice ' -rrb- on the performance is studied and the effect of the model -lrb- the training excerpt -rrb- is investigated . the experimental setup is based on an open source system and the experimental context of nist-sre 2008 . the results confirm that the lower performances are obtained from a reduced number of speakers . even more than speaker factor , speaker verification systems performances are shown to be highly dependant on the voice samples used to train speaker models .
in this paper , a class of algorithms for automatic classification of individual musical instrument sounds is presented . several perceptual features used in sound classification applications as well as mpeg-7 descriptors were measured for 300 sound recordings consisting of 6 different musical instrument classes . subsets of the feature set are selected using branch-and-bound search , obtaining the most suitable features for classification . a class of classifiers is developed based on the non-negative matrix factorization . the standard non-negative matrix factorization is examined as well as its modifications : the local , the sparse , and the discriminant nmf . the experimental results compare feature subsets of varying sizes alongside the various nmf algorithms . it has been found that a subset containing the mean and the variance of the first mel-frequency cepstral coefficient and the audiospectrumflatness descrip-tor along with the means of the audiospectrumenvelope and the audiospectrumspread descriptors when is fed to a standard non-negative matrix factorization yields an accuracy exceeding 95 % .
a near-real-time stereo matching technique is presented in this paper , which is based on the reliability-based dynamic programming algorithm we proposed earlier . the new algorithm can generate semi-dense disparity maps using only two dynamic programming passes , while our previous approach requires 20 ~ 30 passes . we also implement the algorithm on programmable graphics hardware , which further improves the processing speed . the experiments on the four middlebury stereo datasets show that the new algorithm can produce dense -lrb- > 85 % of the pixels -rrb- and reliable -lrb- error rate < 0.3 % -rrb- matches in near real-time -lrb- 0.05 ~ 0.1 sec -rrb- . if needed , it can also be used to generate dense disparity maps . based on the evaluation conducted by the middlebury stereo vision research website , the new algorithm is ranked between the variable window and the graph cuts approaches and currently is the most accurate dynamic programming based technique . when more than one reference images are available , the accuracy can be further improved with little extra computation time .
in this paper , we consider a multi-step version of the stochastic admm method with efficient guarantees for high-dimensional problems . we first analyze the simple setting , where the optimization problem consists of a loss function and a single regularizer -lrb- e.g. sparse optimization -rrb- , and then extend to the multi-block setting with multiple regularizers and multiple variables -lrb- e.g. matrix decomposition into sparse and low rank components -rrb- . for the sparse optimization problem , our stochastic admm method achieves the minimax rate of o -lrb- s log d/t -rrb- for s-sparse problems in d dimensions in t steps , and is thus , unimprovable by any stochastic admm method up to constant factors . for the sparse optimization problem with a general loss function , we analyze the multi-step admm with multiple blocks . we establish o rate and efficient scaling as the size of matrix grows . for natural noise models -lrb- e.g. independent noise -rrb- , our convergence rate is minimax-optimal . thus , we establish tight convergence guarantees for multi-block admm in high dimensions . experiments show that for both sparse optimization and matrix decomposition problems , our stochastic admm method outperforms the state-of-the-art methods .
a common practice in asr to add contextual information is to append consecutive feature frames in a single large feature vector . however , this increases the processing time in the acoustic modelling and may lead to poorly trained parameters . a possible solution is to use a linear discriminant analysis mapping to reduce the dimensionality of the feature , but this is not optimal , at least in the case where the lda classes are hmm-states . it is shown in this paper that the feature reduction problem is essentially a problem of approximating class posterior probabilities . these can be approximated using neural nets . some approaches using different choices for the classes and nn topology are presented and tested on the aurora 2000 digit task and on our in-car task . results on neural nets show a significant performance increase compared to neural nets , but none of the nn-based approaches outperforms neural nets on our in-car task .
this paper describes the ami transcription system for speech in meetings developed in collaboration by five research groups . the ami transcription system includes generic techniques such as discriminative and speaker adaptive training , vocal tract length normalisation , heteroscedastic linear discriminant analysis , maximum likelihood linear regression , and phone posterior based features , as well as techniques specifically designed for meeting data . these include segmentation and cross-talk suppression , beam-forming , domain adaptation , web-data collection , and channel adaptive training . the ami transcription system was improved by more than 20 % relative in word error rate compared to our previous ami transcription system and was usd in the nist rt '06 evaluations where it was found to yield competitive performance .
by its very nature dsp is a mathematically heavy topic and to fully understand it students need to understand the mathematical developments underlying dsp topics . however , relying solely on mathematical developments often clouds the true nature of the foundation of a result . it is likely that students who master the mathematics may still not truly grasp the key ideas of a topic . furthermore , teaching dsp topics by merely '' going through the mathematics '' deprives students of learning the art of discovery that will make them good researchers . this paper uses the topic of polyphase decimation and interpolation to illustrate how it is possible to maintain rigor yet teach using less mathematical approaches that show students how researchers think when developing new ideas .
this paper proposes a dynamic model supporting multimodal state space probability distributions and presents the application of the dynamic model in dealing with visual occlusions when tracking multiple objects jointly . for a set of hypotheses , multiple measurements are acquired at each time instant . the dynamic model switches among a set of hypothesized measurements during the propagation . two computationally efficient filtering algorithms are derived for online joint tracking . both the occlusion relationship and state of the objects are recursively estimated from the history of measurement data . the switching hypothesized measurements model is generally applicable to describe various dynamic processes with multiple alternative measurement methods .
the resilience of identity verification systems to subsampling and compression of human iris images is investigated for three high performance iris matching algorithms . for evaluation , 2156 images from 308 eyes are mapped into a rectangular format with 512 pixels circumferentially and 80 radially . for identity verification , the 48 rows nearest the pupil were taken and the images were subsampled by fourier domain processing . negligible degradation in verification is observed if at least 171 circumferential and 16 radial fourier coefficients are preserved , corresponding to sampling at 342 by 32 pixels . with compression by jpeg 2000 , improved performance is observed down to 0.3 bpp , attributed to noise reduction without significant loss of texture . to ensure that no algorithm is degraded , it is recommended that normalized iris images should be exchanged at 512 x 80 pixel resolution , compressed by jpeg 2000 to 0.5 bpp . this achieves a smaller file size than the proposed m1 biometric data interchange format .
automatic face photo-sketch recognition has important applications for law enforcement . recent research has fo-cused on transforming photos and sketches into the same modality for matching or developing advanced classification algorithms to reduce the modality gap between features extracted from photos and sketches . in this paper , we propose a new inter-modality face recognition approach by reducing the modality gap at the feature extraction stage . a new face descriptor based on coupled information-theoretic encoding is used to capture discriminative local face structures and to effectively match photos and sketches . guided by maximizing the mutual information between photos and sketches in the quantized feature spaces , the coupled encoding is achieved by the proposed coupled information-theoretic projection tree , which is extended to the random-ized forest to further boost the performance . we create the largest face sketch database including sketches of 1 , 194 people from the feret database . experiments on this large scale dataset show that our inter-modality face recognition approach significantly outper-forms the state-of-the-art methods .
this paper proposes a novel approach for multi-view multi-pose object detection using discriminative shape-based exemplars . the key idea underlying this method is motivated by numerous previous observations that manually clustering multi-view multi-pose training data into different categories and then combining the separately trained two-class classifiers greatly improved the multi-view multi-pose object detection performance . a novel computational framework is proposed to unify different processes of categorization , training individual classifier for each intra-class category , and training a strong classifier combining the individual classifiers . the individual processes employ a single objective function that is optimized using two nested adaboost loops . the outer adaboost loop is used to select discriminative exemplars and the inner adaboost is used to select discriminative features on the selected exemplars . the proposed approach replaces the manual time-consuming process of exemplar selection as well as addresses the problem of labeling ambiguity inherent in this process . also , our approach fully complies with the standard adaboost-based object detection framework in terms of real-time implementation . experiments on multi-view multi-pose people and vehicle data demonstrate the efficacy of the proposed approach .
in this paper we present a highly optimized implementation of gaussian mixture acoustic model evaluation algorithm . evaluation of these likelihoods is one of the most computationally intensive parts of automatics speech recognizers but gaussian mixture acoustic model evaluation algorithm can be well-parallelized and offloaded to gpu devices . our gaussian mixture acoustic model evaluation algorithm offers significant speed-up compared to the recently published approaches , since gaussian mixture acoustic model evaluation algorithm exploits the gpu devices better . all the recent implementations were programmed either in cuda or opencl gpu programming frameworks . we present results for both ; cuda as well as opencl . results suggest that even very large acoustic models can be utilized in real-time speech recognition engines on computers and laptops equipped with a low-end gpu . optimization of acoustic likelihoods computation on cuda enables to use the remaining gpu resources for offloading of other compute-intensive parts of lvcsr decoder . other possible use of the freed gpu resources is to evaluate several acoustic models at the same time and use fusion techniques or model selection techniques to improve the quality of resulting conditional likelihoods under diverse conditions .
pronunciation variations can be roughly classified into two types : a phone change or a sound change -lsb- 1 -rsb- -lsb- 2 -rsb- . a phone change happens when a canonical phone is produced as a different phone . such a change can be modeled by converting the baseform phone to a surfaceform phone . a sound change happens at a lower , phonetic or subphonetic level within a phone and it can not be modeled well by either the baseform or the surfaceform phone alone . we propose here to refine the acoustic models to cope with sound changes by -lrb- 1 -rrb- sharing the gaussian mixture components of hmm states in the baseform and the surfaceform models ; -lrb- 2 -rrb- adapting the mixture components of the acoustic models towards those of the surfaceform models ; -lrb- 3 -rrb- selectively reconstructing new acoustic models through sharing or adapting . the proposed pronunciation modeling algorithms are generic and can , in principle , be applied to different languages . specifically , pronunciation modeling algorithms were tested in a cantonese speech recognition database . relative word error rate reductions of 5.45 % , 2.53 % , and 3.04 % have been achieved using the three approaches , respectively .
in this paper we present a method for learning the structure of bayesian networks without making any assumptions on the probability distribution of the domain . this is mainly useful for continuous domains , where there is little guidance and many choices for the parametric distribution families to be used for the local conditional probabilities of the bayesian networks , and only a few have been examined analytically . we therefore focus on bn structure learning in continuous domains . we address the problem by developing a conditional independence test for continuous variables , which can be readily used by any existing independence-based bn structure learning algorithm . our test is non-parametric , making no assumptions on the distribution of the domain . we also provide an effective and computationally efficient method for calculating it from data . we demonstrate the learning of the structure of graphical models in continuous domains from real-world data , to our knowledge for the first time using independence-based methods and without distributional assumptions . we also experimentally show that our test compares favorably with existing statistical approaches which use prediscretization , and verify desirable properties such as statistical consistency .
a new integro-differential invariant for curves in 3d transformed by af ¿ ne group action is presented in this paper . the derivatives involved are of the ¿ rst order , and therefore this invariant is signi ¿ - cantly less sensitive to noise than classical af ¿ ne differential invari-ants , the simplest of which involves derivatives of order 5 . a classi ¿ - cation procedure based on characteristic curves of an object surface is considered using our proposed mixed invariants . substantiating examples are provided to verify ef ¿ ciency and discriminant power of the characteristic spatial curve based 3d object classi ¿ cation .
in this paper , we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features . rather than directly using treebank categories as in previous studies , we learn a set of linguistically-guided latent syntactic categories automatically from a source-side parsed , word-aligned parallel corpus , based on the hierarchical structure among phrase pairs as well as the syntactic structure of the source side . in our model , each x non-terminal in a scfg rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories . these feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the scfg rules that are applied to derive translations . our approach maintains the advantages of hierarchical phrase-based machine translation systems while at the same time naturally incorporates soft syntactic constraints .
in this paper we consider a digital subscriber line system with n orthogonal narrowband tones . each user has a limited power budget , and our goal is to determine the power allocation of each user that enables the ` user capacity ' of the digital subscriber line system to be approached . in this paper , we use ` user capacity ' to denote the maximum number of users that can be supported by the digital subscriber line system , provided that each user is guaranteed to have a data rate that lies within a prescribed range . finding a power allocation that enables this capacity to be approached directly can be quite cumbersome because power allocation involves solving a -lrb- non-convex -rrb- integer-program . in order to circumvent this difficulty , in this paper we propose an alternate approach that is based on exploiting the fairness and per-tone convexity of the harmonic mean-rate objective . using these features , we devise a computationally-efficient power allocation technique that enables the user capacity of the digital subscriber line system to be approached more closely than power allocation techniques that are more computationally demanding .
this paper presents experiments in automatically diagnosing primary progressive aphasia and two of its subtypes , semantic dementia and progressive nonfluent aphasia , from the acoustics of recorded narratives and textual analysis of the resultant transcripts . in order to train each of three types of classifier -lrb- na ¨ ıve bayes , support vector machine , random forest -rrb- , a large set of 81 available features must be reduced in size . two methods of feature selection are therefore compared -- one based on statistical significance and the other based on minimum-redundancy-maximum-relevance . after classifier optimization , ppa -lrb- or absence thereof -rrb- is correctly diagnosed across 87.4 % of conditions , and the two subtypes of ppa are correctly classified 75.6 % of the time .
we improve upon our measures relating feature vector distributions to speaker recognition performances for performance prediction and potential arbitrary data selection for speaker recognition , as described in -lsb- 1 -rsb- . in particular , we examine the means and variances of 11 features pertaining to nasality -lrb- each of which is denoted as a measure -rrb- , computing them on feature vectors of phones to determine which measures give good sr performance prediction of phones . we 've found that the combination of nasality measures give a 0.917 correlation with the equal error rates of phones on sre08 , exceeding the correlation of our previous best measure -lrb- mutual information -rrb- by 12.7 % . when implemented in our data-selection scheme -lrb- which does not require a speaker recognition to be run -rrb- , the nasality measures allow us to select data with combined equal error rates better than data selected via running a speaker recognition in certain cases , at a fortieth of the computational costs . the nasality measures also require a tenth of the computational costs to compute compared to our previous best measure .
in this paper we present two results on reducing the peak power of orthogonal frequency division multiplexing symbols via constellation extension . the first result is a derivation of the interior-point method algorithm needed to find the optimal distortion set , where the distortion is constrained by convex functions . next we optimize the parameters of a hybrid ce constraint set to minimize the constellation extension . numerical examples are provided to illustrate the findings .
we present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets . our probabilistic topic model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties . this probabilistic topic model directly enables discovery of highly rated or inconsistent properties of a product . our probabilistic topic model admits an efficient variational mean-field inference algorithm which can be paral-lelized and run on large snippet collections . we evaluate our probabilistic topic model on a large corpus of snippets from yelp reviews to assess property and attribute prediction . we demonstrate that probabilistic topic model outperforms applicable baselines by a considerable margin .
event anaphora resolution is an important event anaphora resolution for cascaded event template extraction and other nlp study . previous study only touched on event pronoun resolution . in this paper , we provide the first systematic study to resolve event noun phrases to their verbal mentions crossing long distances . our study shows various lexical , syntactic and positional features are needed for event noun phrase resolution and most of lexical , syntactic and positional features , such as morphology relation , synonym and etc , are different from those features used for conventional noun phrase resolution . syntactic structural information in the parse tree modeled with tree kernel is combined with the above diverse flat features using a composite kernel , which shows more than 10 % f-score improvement over the flat features baseline . in addition , we employed a twin-candidate based model to capture the pair-wise candidate preference knowledge , which further demonstrates a statistically significant improvement . all the above contributes to an encouraging performance of 61.36 % f-score on ontonotes corpus .
in modern mongolian , a content word can be inflected when concatenated with suffixes . identifying the original forms of content words is crucial for natural language processing and information retrieval . we propose a lemmatization method for modern mongolian and apply our lemmatization method to indexing for information retrieval . we use technical abstracts to show the effectiveness of our lemmatization method experimentally .
we propose a new region segmentation method based on non-rigid template matching . we align a binary template to an image by maximizing the likelihood of intensity distributions within a region of interest and its background . the intensity model and the corresponding a posteriori distributions are estimated and updated throughout the alignment . the geometric deformation of the binary template is based on a fluid registration model . unlike contour-based segmenta-tion techniques , this fluid registration model allows for a global regularization of the template variations . this enables the segmentation of irregular shapes while avoiding leaks . we apply our region segmentation method to the segmentation of irregular shapes in computed tomography images , a challenging task due to the high inter-patient variability in the shape of this organ . we show that our segmentation results are equivalent or superior in accuracy to results obtained using existing techniques based on 3d shape models .
this paper addresses the problem of extracting depth information of non-rigid dynamic 3d scenes from multiple synchronized video streams . three main issues are discussed in this context : -lrb- i -rrb- temporally consistent depth estimation , -lrb- ii -rrb- sharp depth discontinuity estimation around object boundaries , and -lrb- iii -rrb- enforcement of the global visibility constraint . we present a framework in which the scene is modeled as a collection of 3d piecewise planar surface patches induced by color based image segmentation . this 3d piecewise planar surface patches is continuously estimated using an incremental formulation in which the 3d geometric , motion , and global visibility constraints are enforced over space and time . the proposed algorithm optimizes a cost function that incorporates the spatial color consistency constraint and a smooth scene motion model .
semantic segmentation aims to parse the scene structure of images by annotating the labels to each pixel so that images can be segmented into different regions . while image structures usually have various scales , it is difficult to use a single scale to model the spatial contexts for all individual pixels . multi-scale convolutional neural networks and their variants have made striking success for modeling the global scene structure for an image . however , multi-scale convolutional neural networks are limited in labeling fine-grained local structures like pixels and patches , since spatial contexts might be blindly mixed up without appropriately customizing their scales . to address this challenge , we develop a novel paradigm of multi-scale deep network to model spatial contexts surrounding different pixels at various scales . multi-scale deep network builds multiple layers of memory cells , learning feature representations for individual pixels at their customized scales by hierarchically absorbing relevant spatial contexts via memory gates between layers . such hierarchically gated deep networks can customize a suitable scale for each pix-el , thereby delivering better performance on labeling scene structures of various scales . we conduct the experiments on two datasets , and show competitive results compared with the other multi-scale deep networks on the semantic seg-mentation task .
image based geolocation aims to answer the question : where was this ground photograph taken ? we present an approach to geoloca-lating a single image based on matching human delineated line segments in the ground image to automatically detected line segments in ortho images . our approach is based on distance transform matching . by observing that the uncertainty of line segments is non-linearly amplified by projective transformations , we develop an uncertainty based representation and incorporate uncertainty based representation into a geometric matching framework . we show that our approach is able to rule out a considerable portion of false candidate regions even in a database composed of geographic areas with similar visual appearances .
given point correspondences in multiple perspective views of a scene containing multiple rigid-body motions , we present an algorithm for segmenting the correspondences according to the multiple motions . we exploit the fact that when the depths of the points are known , the point trajec-tories associated with a single motion live in a subspace of dimension at most four . thus motion segmentation with known depths can be achieved by methods of subspace separation , such as gpca or lsa . when the depths are unknown , we proceed iteratively . given the segmentation , we compute the depths using standard techniques . given the depths , we use gpca or lsa to segment the scene into multiple motions . experiments on the hopkins155 database show that our method outperforms existing affine methods in terms of segmentation error and execution time . our methods achieves an error of 2.5 % on 155 sequences .
the use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters . unfortunately , this tuning is often a '' black art '' requiring expert experience , rules of thumb , or sometimes brute-force search . there is therefore great appeal for automatic approaches that can optimize the performance of any given machine learning algorithms to the problem at hand . in this work , we consider this problem through the framework of bayesian optimization , in which a machine learning algorithms 's generalization performance is modeled as a sample from a gaussian process . we show that certain choices for the nature of the gaussian process , such as the type of kernel and the treatment of its hyperparame-ters , can play a crucial role in obtaining a good optimizer that can achieve expert-level performance . we describe new algorithms that take into account the variable cost -lrb- duration -rrb- of machine learning algorithms experiments and that can leverage the presence of multiple cores for parallel experimentation . we show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent dirichlet allocation , structured svms and convolutional neural networks .
in this paper , we extend the hilbert space embedding approach to handle <i> conditional </i> distributions . we derive a kernel estimate for the conditional embedding , and show its connection to ordinary embeddings . conditional embeddings largely extend our ability to manipulate distributions in hilbert spaces , and as an example , we derive a nonparametric method for modeling dynamical systems where the belief state of the system is maintained as a conditional embedding . our nonparametric method is very general in terms of both the domains and the types of distributions that it can handle , and we demonstrate the effectiveness of our nonparametric method in various dynamical systems . we expect that conditional embeddings will have wider applications beyond modeling dynamical systems .
this paper presents a new type of nonlinear function for independent component analysis to process complex-valued signals , which is used in frequency-domain blind source separation . the new nonlinear function is based on the polar coordinates of a complex number , whereas the conventional one is based on the cartesian coordinates . the new nonlinear function is derived from the probability density function of frequency-domain signals that are assumed to be independent of the phase . we show that the difference between the two types of functions is in the assumed densities of independent components . experimental results for separating speech signals show that the new nonlinear function behaves better than the conventional one .
-- we present a general approach for the hierarchical seg-mentation and labeling of document layout structures . this approach models document layout as a grammar and performs a global search for the optimal parse based on a grammatical cost function . our contribution is to utilize machine learning to discriminatively select features and set all parameters in the parsing process . therefore , and unlike many other approaches for layout analysis , ours can easily adapt itself to a variety of document analysis problems . one need only specify the page grammar and provide a set of correctly labeled pages . experiments demonstrate the effectiveness of this technique on two document image analysis tasks : page layout structure extraction and mathematical expression interpretation . experiments demonstrate that the learned grammars can be used to extract the document structure in 57 files from the uwiii document image database . a second set of experiments demonstrate that the same framework can be used to automatically interpret printed mathematical expressions so as to recreate the original latex .
this paper introduces a new model for automatic speech recognition called temm-temporal episodic memory model . temm is derived from a simulation of human episodic memory called minerva2 , and it not only overcomes the inability of minerva2 to use temporal sequence for recognition flexibly , but it also employs a prediction mechanism as an additional source of information . the performance of temm on an asr task is compared to state-of-the-art hmm/gmm baseline systems , and a first analysis shows both promising results and a need to further stabilise the consistency of the output of the new model .
-- a new decoding algorithm , referred to as min-sum with adaptive message control , is proposed to reduce the decoding complexity of nonbinary ldpc codes . the proposed decoding algorithm adaptively trims the message length of belief information to reduce the amount of arithmetic operations . exploiting the fact that during the decoding iteration , the distribution of belief information will become more concentrated around the correct element in the case of convergence , the messages can be truncated accordingly by considering only a few entries with large likelihood . simulation results with a gf -lrb- 16 -rrb- nonbinary ldpc code indicate that the proposed decoding algorithm can reduce arithmetic operations by up to 65 % compared with non-truncated cases . compared with the state-of-the-art extended ms decoding , the proposed decoding algorithm can reduce the computation by up to 50 % , thereby enabling low-complexity decoding of nonbinary ldpc codes .
a method for blind estimation of static time errors in time inter-leaved a/d converters is investigated . the method assumes that amplitude and gain errors are removed before the time error estimation . even if the amplitude and gain errors are estimated and removed , there will be small errors left . in this paper , we investigate how the amplitude and gain errors influence the time error estimation performance .
current statistical parametric text-to-speech synthesis methods allow production of neutral speech with acceptable quality . however , prosody is often qualified as unsatisfactory and sounding too flat . in this paper , we address intonation modelling for statistical parametric text-to-speech synthesis methods based on physiological aspects of prosody production . a set of gamma distribution shaped atoms is defined and then intonation decomposition is performed using a matching pursuit algorithm . some preliminary experiments show that this model allows easy extraction of physiologically meaningful atoms that could be used to generate intonation in a tts system .
study of the human brain through fmri can potentially benefit the pursuit of artificial intelligence . four examples are presented . first , fmri decoding of the brain activity of subjects watching video clips yields higher accuracy than state-of-the-art computer-vision approaches to activity recognition . second , novel methods are presented that decode aggregate representations of complex visual stimuli by decoding their independent constituents . third , cross-modal studies demonstrate the ability to decode the brain activity induced in subjects watching video stimuli when trained on the brain activity induced in subjects seeing text or hearing speech stimuli and vice versa . fourth , the time course of brain processing while watching video stimuli is probed with scanning that trades off the amount of the brain scanned for the frequency at which it is scanned . techniques like these can be used to study how the human brain grounds language in visual perception and may motivate development of novel approaches in ai .
this paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden markov model , oo-line handwriting recognition system . the recurrent neural network estimates posterior distributions for each of a series of frames representing sections of a handwritten word . the supervised training algorithm , backpropagation through time , requires target outputs to be provided for each frame . three methods for deriving these targets are presented . a novel method based upon the forward-backward algorithm is found to result in the recognizer with the lowest error rate .
young multi-ethnolectal speakers of hamburg-german introduced an alternation of / ç / to -lsb- ʃ -rsb- following a lax front vowel / ɪ / -lsb- 1 -rsb- . we conducted perception studies exploiting this contrast in berlin -lrb- germany -rrb- , a city with large multi-ethnic neighborhoods . this alternation is pervasive and noticeable , it is mocked and stigmatized and there is an awareness that many young speakers -lrb- including ethnically germans -rrb- from neighborhoods with larger migrant populations like kreuzberg -lrb- kb -rrb- substitute / ç / with / ʃ / while speakers from less stigmatized vicinities like zehlendorf -lrb- zd -rrb- do not . the categorization of items on two 14-step synthesized continua from fichte ` spruce ' to fischte ' 3 rd p. sg . to fish ' by 99 listeners shows that the interpretation of fine phonetic detail is strongly influenced by the co-presentation of the label kb or zd in contrast to no label -lrb- control -rrb- . analyses of the reaction times show that significantly more time is needed to process stimuli in kb and less in zd . moreover , younger listeners -lrb- below 30 years -rrb- perceive more / ʃ / variants than older listeners . phonological generalization over phonetic input is dependent on associative information : perceptual divergence is found within the confines of a single large urban area -lsb- 2,3,4 -rsb- .
metric constraints are known to be highly discriminative for many objects , but if training is limited to data captured from a particular 3-d sensor the quantity of training data may be severly limited . in this paper , we show how a crucial aspect of 3-d sensor -- object and feature absolute size -- can be added to models learned from commonly available online imagery , without use of any 3-d sensing or reconstruction at training time . such models can be utilized at test time together with explicit 3-d sensing to perform robust search . our model uses a '' 2.1 d '' local feature , which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window . we show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata fields specifying camera intrinstics . we develop an efficient metric branch-and-bound algorithm for our search task , imposing 3-d size constraints as part of an optimal search for a set of features which indicate the presence of a category . experiments on test scenes captured with a traditional stereo rig are shown , exploiting training data from from purely monocular sources with associated exif metadata .
words undergo various changes when entering new languages . based on the assumption that these linguistic changes follow certain rules , we propose a method for automatically detecting pairs of cog-nates employing an orthographic alignment method which proved relevant for sequence alignment in computational biology . we use aligned subsequences as features for machine learning algorithms in order to infer rules for linguistic changes undergone by words when entering new languages and to discriminate between cognates and non-cognates . given a list of known cognates , our approach does not require any other linguistic information . however , it can be customized to integrate historical information regarding language evolution .
we propose a dynamic bayesian classifier for the socio-situational setting of a conversation . knowledge of the socio-situational setting can be used to search for content recorded in a particular setting or to select context-dependent models in speech recognition . the dynamic bayesian classifier has the advantage -- compared to static classifiers such a naive bayes and support vector machines -- that it can continuously update the classification during a conversation . we experimented with several models that use lexical and part-of-speech information . our results show that the prediction accuracy of the dynamic bayesian classifier using the first 25 % of a conversation is almost 98 % of the final prediction accuracy , which is calculated on the entire conversation . the best final prediction accuracy , 88.85 % , is obtained by bigram dynamic bayesian classification using words and part-of-speech tags .
game-theoretic game-theoretic algorithms for physical security have made an impressive real-world impact . these game-theoretic algorithms compute an optimal strategy for the defender to commit to in a stackelberg game , where the attacker observes the defender 's strategy and best-responds . in order to build the game model , though , the payoffs of potential attackers for various outcomes must be estimated ; inaccurate estimates can lead to significant inefficiencies . we design an algorithm that optimizes the defender 's strategy with no prior information , by observing the attacker 's responses to randomized deployments of resources and learning his priorities . in contrast to previous work , our algorithm requires a number of queries that is polynomial in the representation of the game .
td-falcon is a self-organizing neural network that incorporates temporal difference methods for reinforcement learning . despite the advantages of fast and stable learning , td-falcon still relies on an iterative process to evaluate each available action in a decision cycle . to remove this deficiency , this paper presents a direct code access procedure whereby td-falcon conducts instantaneous searches for cognitive nodes that match with the current states and at the same time provide maximal reward values . our comparative experiments show that td-falcon with direct code access produces comparable performance with the original td-falcon while improving significantly in computation efficiency and network complexity .
fourier transform can be generalized into the fractional fourier transform , linear canonical transform , and simplified fractional fourier transform . they extend the utilities of original fourier transform , and can solve many problems that ca n't be solved well by original fourier transform . in this paper , we will generalize the fourier transform . we will derive fractional cosine transform , canonical cosine transform , and simplified fractional cosine transform . we will show fractional cosine transform are very similar to the frft , linear canonical transform , and sfrft , but fractional cosine transform are much more efficient to deal with the even , real even functions . for fourier transform , frft and sfrft can save 112 of the real number multiplications , and frft can save 314 . we also discuss their applications , such as optical system analysis and space-variant pattem recognition .
in this paper , we present new results on temporal decomposition applied to the line spectral frequencies derived for wideband speech . the paper shows that by incorporating a dynamic programming search algorithm into temporal decomposition , near transparent quantisation of wideband lsfs can be obtained at approximately 1 kbps . we also show that temporal decomposition performs significantly better than split vector quantisation at low bit rates . we propose that temporal decomposition is a promising approach to low rate wideband speech coding for applications such as unicast streaming .
distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of nlp tasks , especially on en-glish . in this work , we innovatively develop two component-enhanced chinese character embedding models and their bi-gram extensions . distinguished from en-glish word embeddings , our component-enhanced chinese character embedding models explore the compositions of chinese characters , which often serve as semantic in-dictors inherently . the evaluations on both word similarity and text classification demonstrate the effectiveness of our component-enhanced chinese character embedding models .
this paper presents a novel segmentation approach for extracting faces from videos . under an active learning framework , the seg-mentation is conducted automatically without human interactions . a small portion of pixels are first labeled as face or non-face . given these labeled samples , a semi-supervised spline regression model is then applied to obtain the face region . based on the segmentation result , new pixels are selected and labeled . these two steps perform iterately until convergence . the main novelty is that color and depth data are combined to provide the labeling information . our segmentation approach is validated via comparisons with state-of-the-art methods on real videos captured from the commodity kinect camera .
in this paper , we show decidability of a rather expressive fragment of the situation calculus . we allow second order quantiication over-nite and innnite sets of situations . we do not impose a domain closure assumption on actions ; therefore , innnite and even uncountable domains are allowed . the domain closure assumption is based on automata accepting innnite trees .
we propose a model for jointly predicting multiple emotions in natural language sentences . our model is based on a low-rank coregionalisation approach , which combines a vector-valued gaussian process with a rich parameterisation scheme . we show that our approach is able to learn correlations and anti-correlations between emotions on a news headlines dataset . the proposed model outperforms both single-task baselines and other multi-task approaches .
we propose a domain adaptation framework , and formally prove that domain adaptation framework generalizes the feature augmentation technique in -lrb- daumé iii , 2007 -rrb- and the multi-task regularization framework in -lrb- evgeniou and pontil , 2004 -rrb- . we show that our domain adaptation framework is strictly more general than these approaches and allows practitioners to tune hyper-parameters to encourage transfer between close domains and avoid negative transfer between distant ones .
dictionary learning has became an increasingly important task in machine learning , as dictionary learning is fundamental to the representation problem . a number of emerging techniques specifically include a codebook learning step , in which a critical knowledge abstraction process is carried out . existing approaches in dictionary learning are either generative -lrb- unsupervised e.g. k-means -rrb- or discriminative -lrb- supervised e.g. extremely randomized forests -rrb- . in this paper , we propose a multiple instance learning -lrb- mil -rrb- strategy -lrb- along the line of weakly supervised learning -rrb- for dictionary learning . each code is represented by a classifier , such as a linear svm , which naturally performs metric fusion for multi-channel features . we design a formulation to simultaneously learn mixtures of codes by maximizing classification margins in mil . state-of-the-art results are observed in image classification benchmarks based on the learned codebooks , which observe both com-pactness and effectiveness .
conventional plda scoring in i-vector speaker verification involves the i-vectors of target speakers and claimants only . we have previously demonstrated that better performance can be achieved by incorporating the information of background speakers in the scoring process via speaker-dependent svms . this is achieved by defining a plda score space with dimension equal to the number of training i-vectors for each target speaker . the new protocol in nist 2012 sre permits systems to use the information of other target-speakers -lrb- called known non-targets -rrb- in each verification trial . in this paper , we exploit this new protocol to enhance the performance of plda-svm scoring by using the score vectors of both known and unknown non-targets as the impostor class data to train the speaker-dependent svms . because some target speakers have one enrollment utterance only , which results in severe imbalance in the speaker-and impostor-class data for svm training . this paper shows that if the enrollment utterance is sufficiently long , a number of target-speaker i-vectors can be generated by an utterance partitioning and resam-pling technique , resulting in much better scoring svms . results on nist 2012 sre demonstrate the advantages of pooling the known and unknown non-targets for training the svms and that the resam-pling techniques can help the svm training algorithm to find better decision boundaries for those speakers with only a small number of enrollment utterances .
this paper presents a rapid voice adaptation algorithm using gmm-based frequency warping and shift with parameters of a sub-band basis spectrum model -lsb- 1 -rsb- . the sub-band basis spectrum model represents a shape of a spectrum of speech . rapid voice adaptation algorithm is calculated by fitting a sub-band basis to the log-spectrum . since the sub-band basis spectrum model is the frequency domain representation , frequency warping can be directly applied to the sub-band basis spectrum model . a frequency warping function that minimize the distance between source and target sub-band basis spectrum model pairs in each mixture component of a gmm is derived using a dp -lrb- dynamic programming -rrb- algorithm . the proposed rapid voice adaptation algorithm is evaluated in an unit-selection based voice adaptation framework applied to a unit-fusion based text-to-speech synthesizer . the experimental results show that the proposed rapid voice adaptation algorithm is effective for rapid voice adaptation using just one sentence , compared to the conventional gmm.-based linear transformation of mel-cepstra .
this article shows how rational analysis can be used to minimize learning cost for a general class of statistical learning problems . we discuss the factors that influence learning cost and show that the problem of effr-cient learning can be cast as a resource optimization problem . solutions found in this way can be significantly more efficient than the best solutions that do not account for these factors . we introduce a heuristic learning algorithm that approximately solves this optimization problem and document its performance improvements on synthetic and real-world problems . se1191 -rsb- -rrb- of these factors to minimize learning cost . we discuss this in the context of parametric hypothesis selection problems , an abstract class of statistical learning problems where a system must select one of a finite set of hypothesized courses of action , where the quality of each hypothesis is described as a function of some unknown parameters -lrb- e.g. . a heuristic learning algorithm determines and refines estimates of these parameters by '' paying for '' training examples .
the purpose of a voice conversion system is to change the perceived speaker identity of a speech signal . in this paper , we propose a new algorithm based on converting the lpc spectrum and predicting the residual as a function of the target envelope parameters . we conduct listening tests based on speaker discrimination of same/difference pairs to measure the accuracy by which the converted voices match the desired target voices . to establish the level of human performance as a baseline , we first measure the ability of listeners to discriminate between original speech utterances under three conditions : normal , fundamental frequency and duration normalized , and lpc coded . additionally , the spectral parameter conversion function is tested in isolation by listening to source , target , and converted speakers as lpc coded speech . the results show that the speaker identity of speech whose lpc spectrum has been converted can be recognized as the target speaker with the same level of performance as discriminating between lpc coded speech . however , the level of discrimination of converted utterances produced by the voice conversion system is significantly below that of speaker discrimination of natural speech .
training machine learning models sometimes needs to be done on large amounts of data that exceed the capacity of a single machine , motivating recent works on developing algorithms that train in a distributed fashion . this paper proposes an efficient box-constrained quadratic optimization algorithm for distributedly training linear support vector machines -lrb- svms -rrb- with large data . our key technical contribution is an analytical solution to the problem of computing the optimal step size at each iteration , using an efficient analytical solution that requires only o communication cost to ensure fast convergence . with this optimal step size , our analytical solution is superior to other methods by possessing global linear convergence , or , equivalently , o -lrb- log -lrb- 1 / / -rrb- -rrb- iteration complexity for an-accurate solution , for dis-tributedly solving the non-strongly-convex linear svm dual problem . experiments also show that our analytical solution is significantly faster than state-of-the-art distributed linear svm algorithms including dsvm-ave , disdca and tron .
in syntax-based machine translation , rule selection is the task of choosing the correct target side of a translation rule among rules with the same source side . we define a discriminative rule selection model for systems that have syntactic annotation on the target language side -lrb- string-to-tree -rrb- . this is a new and clean way to integrate soft source syntactic constraints into string-to-tree systems as features of the rule selection model . we release our implementation as part of moses .
efficient design of wireless networks requires implementation of cross-layer algorithms that exploit channel state information . capitalizing on convex optimization and stochastic approximation tools , this paper develops a stochastic algorithm that allocates resources at network , link , and physical layers so that a sum-utility of the average end-to-end rates is maximized . focus is placed on wireless networks where interference is strong and nodes transmit orthogonally over a set of parallel channels . convergence of the developed stochastic schemes is characterized , and the average queue delays are obtained in closed form .
reverberation distorts human speech and usually has negative effects on speech intelligibility , especially for hearing-impaired listeners . it also causes performance degradation in automatic speech recognition and speaker identification systems . therefore , the dereverberation problem must be dealt with in daily listening environments . we propose to use deep neural networks to learn a spectral mapping from the reverberant speech to the anechoic speech . the trained deep neural networks produces the estimated spectral representation of the corresponding anechoic speech . we demonstrate that distortion caused by reverberation is substantially attenuated by the deep neural networks whose outputs can be resynthesized to the derever-ebrated speech signal . the proposed deep neural networks is simple , and our systematic evaluation shows promising dereverberation results , which are significantly better than those of related systems .
this paper investigates algorithms to automatically adapt the learning rate of neural networks . starting with stochastic gradient descent , a large variety of learning methods has been proposed for the nn setting . however , these methods are usually sensitive to the initial learning rate which has to be chosen by the exper-imenter . we investigate several features and show how an adaptive controller can adjust the learning rate without prior knowledge of the learning problem at hand .