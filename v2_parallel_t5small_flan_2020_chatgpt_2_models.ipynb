{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "vscode": {
      "interpreter": {
        "hash": "2a8badda5171f6c1da75e6dcec216359e8635e393e06f848b1b87b76c1bdea5e"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da6ccd5fd1334160930c2788785709c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72cfe08f152648ceae7ca8909e508f8c",
              "IPY_MODEL_5a7c9ce436ca44f4a2ff935ac040a903",
              "IPY_MODEL_af447e1f96a747108e3138a9ba23fda7"
            ],
            "layout": "IPY_MODEL_7531b8e8a4d94296adfc0d15f65830f6"
          }
        },
        "72cfe08f152648ceae7ca8909e508f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a14e930e89e4c508ee7aefd490ff110",
            "placeholder": "​",
            "style": "IPY_MODEL_1642358f4e98456bb1679de54b8d5df2",
            "value": "Downloading builder script: "
          }
        },
        "5a7c9ce436ca44f4a2ff935ac040a903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f64b9e6a7814fdbae749f6cf8b363ac",
            "max": 2923,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8e25cb2812743b9a369735c81132438",
            "value": 2923
          }
        },
        "af447e1f96a747108e3138a9ba23fda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75c682e6f1ff4fc78b2c56b3d2f8e54f",
            "placeholder": "​",
            "style": "IPY_MODEL_a6e8dd21b6154c96a2174da7ba0f8627",
            "value": " 8.10k/? [00:00&lt;00:00, 109kB/s]"
          }
        },
        "7531b8e8a4d94296adfc0d15f65830f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a14e930e89e4c508ee7aefd490ff110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1642358f4e98456bb1679de54b8d5df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f64b9e6a7814fdbae749f6cf8b363ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e25cb2812743b9a369735c81132438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75c682e6f1ff4fc78b2c56b3d2f8e54f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6e8dd21b6154c96a2174da7ba0f8627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76fcc2f6dba84960b74a3952a3248df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc33b362c2a24642bf2e562d3f00334c",
              "IPY_MODEL_b8cc86ea40ed4429b6999667e173bf78",
              "IPY_MODEL_2dfce6b238c24dc6ae3fbba2c116d980"
            ],
            "layout": "IPY_MODEL_7c1323b6edd64d959f5adb73945dc45c"
          }
        },
        "cc33b362c2a24642bf2e562d3f00334c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fba3d4667ae04b3a9a3c877d1d475c45",
            "placeholder": "​",
            "style": "IPY_MODEL_2c2cb4993c9a4c4788471853c00be17e",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "b8cc86ea40ed4429b6999667e173bf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34f69bb315094dc49af6db69ba10cd02",
            "max": 482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e17c89a2340845e4a0d6cbe7614fc8bc",
            "value": 482
          }
        },
        "2dfce6b238c24dc6ae3fbba2c116d980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62c44981f3a24a88b398fc1973496ec3",
            "placeholder": "​",
            "style": "IPY_MODEL_6201321d348a43b1a057ec331c28a15d",
            "value": " 482/482 [00:00&lt;00:00, 11.7kB/s]"
          }
        },
        "7c1323b6edd64d959f5adb73945dc45c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba3d4667ae04b3a9a3c877d1d475c45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c2cb4993c9a4c4788471853c00be17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34f69bb315094dc49af6db69ba10cd02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17c89a2340845e4a0d6cbe7614fc8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62c44981f3a24a88b398fc1973496ec3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6201321d348a43b1a057ec331c28a15d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7d82664f8644f48b07f0a9c59af6199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3af4bfe4fca64631943c3f1dd65bfe6c",
              "IPY_MODEL_b5974bd9658a4665a41e07435a8da21e",
              "IPY_MODEL_6dfc8b89969d48aeb6994df328e2559b"
            ],
            "layout": "IPY_MODEL_4023a75670224b7f9ab42791435a3223"
          }
        },
        "3af4bfe4fca64631943c3f1dd65bfe6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cdda2f8055f43d7a98ee1d837342ea7",
            "placeholder": "​",
            "style": "IPY_MODEL_c2c3a06573e5439c9d1d6bbb32389161",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "b5974bd9658a4665a41e07435a8da21e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0889bc0ddff840e98685d3026d2f61c1",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f6926ea464a4ea9a6bf20b04e38a49e",
            "value": 898823
          }
        },
        "6dfc8b89969d48aeb6994df328e2559b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49d33bdb5b184f6cad1aeccf7ca72311",
            "placeholder": "​",
            "style": "IPY_MODEL_1be646bb36434524b0f58d417dd78b6e",
            "value": " 899k/899k [00:00&lt;00:00, 1.07MB/s]"
          }
        },
        "4023a75670224b7f9ab42791435a3223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cdda2f8055f43d7a98ee1d837342ea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c3a06573e5439c9d1d6bbb32389161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0889bc0ddff840e98685d3026d2f61c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f6926ea464a4ea9a6bf20b04e38a49e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49d33bdb5b184f6cad1aeccf7ca72311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be646bb36434524b0f58d417dd78b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d058054fd6b49c3995c576295a0292b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0accf9dbffbd49838bea9f6d586c6835",
              "IPY_MODEL_ec40798778b14595938a1c6037441c39",
              "IPY_MODEL_d5b04ceaba854400a61648cd314a2b66"
            ],
            "layout": "IPY_MODEL_20f4fb6f142e4d118ebb995ad2660433"
          }
        },
        "0accf9dbffbd49838bea9f6d586c6835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b99441d172bc4134a6d35411642bd482",
            "placeholder": "​",
            "style": "IPY_MODEL_ed8890ecd08140e6b71b7588439180bd",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "ec40798778b14595938a1c6037441c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8085bcc432cc490fab8e507af0d80a4e",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_075467720b244569a5b86c5be6756ac8",
            "value": 456318
          }
        },
        "d5b04ceaba854400a61648cd314a2b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a06dbfcc9954b3e84861f5c3b34b858",
            "placeholder": "​",
            "style": "IPY_MODEL_9b25337bc5c44110b6804edd5c0d0638",
            "value": " 456k/456k [00:00&lt;00:00, 715kB/s]"
          }
        },
        "20f4fb6f142e4d118ebb995ad2660433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b99441d172bc4134a6d35411642bd482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed8890ecd08140e6b71b7588439180bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8085bcc432cc490fab8e507af0d80a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "075467720b244569a5b86c5be6756ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a06dbfcc9954b3e84861f5c3b34b858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b25337bc5c44110b6804edd5c0d0638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d0b384e63934e7aa1ea89430de630d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72628cdb0702477bab34ce27d045763c",
              "IPY_MODEL_32641245501d42f786eadeb07bea8d9c",
              "IPY_MODEL_f81bbd14e05a47fcbc05a143126137c0"
            ],
            "layout": "IPY_MODEL_7c8f382430574a7980a42a61ac3e984c"
          }
        },
        "72628cdb0702477bab34ce27d045763c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6cabad095104bca8a0ab34af276639f",
            "placeholder": "​",
            "style": "IPY_MODEL_b71ac72e671345f5a048d7f89ae67c76",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "32641245501d42f786eadeb07bea8d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fdf94fcd5234e01b6dd28cc3c4a1a92",
            "max": 1421700479,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35708a4f7145495dad20ebb60f1a1cc9",
            "value": 1421700479
          }
        },
        "f81bbd14e05a47fcbc05a143126137c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a091a8b1bbb4fcd8f3ba1564ff7c671",
            "placeholder": "​",
            "style": "IPY_MODEL_80b92be63b8441e8a057173c2369c1b8",
            "value": " 1.42G/1.42G [00:13&lt;00:00, 112MB/s]"
          }
        },
        "7c8f382430574a7980a42a61ac3e984c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6cabad095104bca8a0ab34af276639f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71ac72e671345f5a048d7f89ae67c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fdf94fcd5234e01b6dd28cc3c4a1a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35708a4f7145495dad20ebb60f1a1cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a091a8b1bbb4fcd8f3ba1564ff7c671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b92be63b8441e8a057173c2369c1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "ahHaXoNB9iBc",
        "execution": {
          "iopub.status.busy": "2023-05-24T20:39:06.853402Z",
          "iopub.execute_input": "2023-05-24T20:39:06.853745Z",
          "iopub.status.idle": "2023-05-24T20:39:06.869756Z",
          "shell.execute_reply.started": "2023-05-24T20:39:06.853716Z",
          "shell.execute_reply": "2023-05-24T20:39:06.868644Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e02616-82b9-4f19-a629-05f6b3b1f1e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJsztL9_PdHs",
        "outputId": "fe826b2f-6733-4bf1-a615-07c5315cabde"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/webnlg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt1y2YXhPiYG",
        "outputId": "9bd7e902-9727-4606-8b2d-a6a4dbcf4262"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/webnlg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets, re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from torch.nn.parallel import DataParallel\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "model_reasoning = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "model_reasoning = DataParallel(model_reasoning)\n",
        "\n",
        "model_output = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "model_output = DataParallel(model_output)\n",
        "\n",
        "new_tokens = ['<H>', '<R>', '<T>', '[SEP]']\n",
        "new_tokens_vocab = {}\n",
        "new_tokens_vocab['additional_special_tokens'] = []\n",
        "for idx, t in enumerate(new_tokens):\n",
        "    new_tokens_vocab['additional_special_tokens'].append(t)\n",
        "num_added_toks = tokenizer.add_special_tokens(new_tokens_vocab)\n",
        "\n",
        "tokenizer.add_tokens(\"[MASK]\")\n",
        "tokenizer.mask_token = \"[MASK]\"\n",
        "tokenizer.mask_token_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")"
      ],
      "metadata": {
        "id": "7Ms8N01X9MlZ",
        "execution": {
          "iopub.status.busy": "2023-07-05T16:43:25.723987Z",
          "iopub.execute_input": "2023-07-05T16:43:25.724337Z",
          "iopub.status.idle": "2023-07-05T16:43:27.316709Z",
          "shell.execute_reply.started": "2023-07-05T16:43:25.724309Z",
          "shell.execute_reply": "2023-07-05T16:43:27.315720Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVYu372wk7Nz",
        "outputId": "bf5dacc2-29d4-46b3-dfa2-742e1f9e1614"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5TokenizerFast(name_or_path='google/flan-t5-small', vocab_size=32100, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]', 'additional_special_tokens': ['<H>', '<R>', '<T>', '[SEP]']}, clean_up_tokenization_spaces=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WebNLGDatasetReasoning(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.prefix = \"translate from Graph to Text: \"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # preprocess the input graph\n",
        "        try:\n",
        "            triples = item['modified_triple_sets']['mtriple_set']\n",
        "            input_text = self.prefix\n",
        "            for outer_list in triples:\n",
        "                for triple in outer_list:\n",
        "                    triple_txt = triple.split(\"|\")\n",
        "                    input_text += \" <H> \" + triple_txt[0] + \" <R> \" + triple_txt[1] + \" <T> \" + triple_txt[2] + \" [SEP]\"\n",
        "\n",
        "        except (KeyError, IndexError):\n",
        "            print(\"1 - WebNLGDatasetReasoning\")\n",
        "            print(item['modified_triple_sets']['mtriple_set'])\n",
        "            print(item['modified_triple_sets']['mtriple_set'][0])\n",
        "            print(triples)\n",
        "            input_text = self.prefix\n",
        "        # preprocess the target text\n",
        "        try:\n",
        "            target_text = item['reasoning']\n",
        "        except (KeyError, IndexError):\n",
        "            print(\"2 - WebNLGDatasetReasoning\")\n",
        "            print(item)\n",
        "            #print(item['original_triple_sets']['otriple_set'])\n",
        "            target_text = \"\"\n",
        "        #print(item)\n",
        "        #print(input_text)\n",
        "        # encode the inputs and targets using the tokenizer\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
        "        target_ids = tokenizer.encode(target_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
        "        #print(input_text)\n",
        "        #print(target_text)\n",
        "        return input_ids.squeeze(0), target_ids.squeeze(0)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T16:43:30.302080Z",
          "iopub.execute_input": "2023-07-05T16:43:30.302835Z",
          "iopub.status.idle": "2023-07-05T16:43:30.319435Z",
          "shell.execute_reply.started": "2023-07-05T16:43:30.302795Z",
          "shell.execute_reply": "2023-07-05T16:43:30.318507Z"
        },
        "trusted": true,
        "id": "nXK6oOgXO0Lt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WebNLGDatasetOutput(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.prefix = \"translate from Graph to Text: \"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        # preprocess the input graph\n",
        "        try:\n",
        "            input_text = item['reasoning']\n",
        "        except (KeyError, IndexError):\n",
        "            print(\"1 - WebNLGDatasetOutput\")\n",
        "            print(item['original_triple_sets']['otriple_set'])\n",
        "            print(item['original_triple_sets']['otriple_set'][0])\n",
        "            print(item['reasoning'])\n",
        "            input_text = self.prefix\n",
        "        # preprocess the target text\n",
        "        try:\n",
        "            target_text = item['lex']['text'][0]\n",
        "        except (KeyError, IndexError):\n",
        "            print(\"2 - WebNLGDatasetOutput\")\n",
        "            print(item)\n",
        "            #print(item['original_triple_sets']['otriple_set'])\n",
        "            target_text = \"\"\n",
        "        #print(item)\n",
        "        #print(input_text)\n",
        "        # encode the inputs and targets using the tokenizer\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
        "        target_ids = tokenizer.encode(target_text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
        "        #print(input_text)\n",
        "        #print(target_text)\n",
        "        return input_ids.squeeze(0), target_ids.squeeze(0)\n"
      ],
      "metadata": {
        "id": "0vJWqTh9uN0i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_INPUT_LENGTH = 128\n",
        "MAX_TARGET_LENGTH = 128\n",
        "tokenizer.model_max_length = MAX_INPUT_LENGTH\n",
        "model_reasoning.module.config.max_length = MAX_TARGET_LENGTH\n",
        "model_output.module.config.max_length = MAX_TARGET_LENGTH\n",
        "\n",
        "# set up the device (GPU or CPU)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model_reasoning.to(device)\n",
        "model_output.to(device)"
      ],
      "metadata": {
        "id": "g1jjIPRO9Mla",
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2023-07-05T16:43:31.148318Z",
          "iopub.execute_input": "2023-07-05T16:43:31.150710Z",
          "iopub.status.idle": "2023-07-05T16:43:31.258440Z",
          "shell.execute_reply.started": "2023-07-05T16:43:31.150676Z",
          "shell.execute_reply": "2023-07-05T16:43:31.257276Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be742835-409a-4a82-9163-657ac657fb39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataParallel(\n",
              "  (module): T5ForConditionalGeneration(\n",
              "    (shared): Embedding(32128, 512)\n",
              "    (encoder): T5Stack(\n",
              "      (embed_tokens): Embedding(32128, 512)\n",
              "      (block): ModuleList(\n",
              "        (0): T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                (relative_attention_bias): Embedding(32, 6)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseGatedActDense(\n",
              "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): NewGELUActivation()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-7): 7 x T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseGatedActDense(\n",
              "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): NewGELUActivation()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_layer_norm): T5LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (decoder): T5Stack(\n",
              "      (embed_tokens): Embedding(32128, 512)\n",
              "      (block): ModuleList(\n",
              "        (0): T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "                (relative_attention_bias): Embedding(32, 6)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerCrossAttention(\n",
              "              (EncDecAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (2): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseGatedActDense(\n",
              "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): NewGELUActivation()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-7): 7 x T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerCrossAttention(\n",
              "              (EncDecAttention): T5Attention(\n",
              "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (2): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseGatedActDense(\n",
              "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): NewGELUActivation()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_layer_norm): T5LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('final_dataset.pkl', 'rb') as f:\n",
        "  final_dataset = pickle.load(f)"
      ],
      "metadata": {
        "id": "rGmMBlDWQDNN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_dataset[0]"
      ],
      "metadata": {
        "id": "W1TdHKeEQYLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.Random(32).shuffle(final_dataset)"
      ],
      "metadata": {
        "id": "Xij8H9Z6Qc_7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_raw_train = final_dataset[:700]\n",
        "dataset_raw_val = final_dataset[700:900]\n",
        "dataset_raw_test = final_dataset[900:]"
      ],
      "metadata": {
        "id": "9vyHhtbMQdDu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset_raw_train[0]"
      ],
      "metadata": {
        "id": "bwCyfJfhRd39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8ee82f-8b15-4298-bc0d-66a3cfbd86e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'category': 'Athlete',\n",
              " 'size': 5,\n",
              " 'eid': 'Id59',\n",
              " 'original_triple_sets': {'otriple_set': [['Moscow | leaderName | Sergey_Sobyanin',\n",
              "    'Aleksandr_Chumakov | placeOfBirth | Moscow',\n",
              "    'Aleksandr_Chumakov | clubs | FC_Torpedo_Moscow',\n",
              "    'Aleksandr_Chumakov | team | Soviet_Union_national_football_team',\n",
              "    'FC_Torpedo_Moscow | manager | Valery_Petrakov']]},\n",
              " 'modified_triple_sets': {'mtriple_set': [['Moscow | leader | Sergey_Sobyanin',\n",
              "    'Aleksandr_Chumakov | birthPlace | Moscow',\n",
              "    'Aleksandr_Chumakov | club | FC_Torpedo_Moscow',\n",
              "    'Aleksandr_Chumakov | club | Soviet_Union_national_football_team',\n",
              "    'FC_Torpedo_Moscow | manager | Valery_Petrakov']]},\n",
              " 'shape': '(X (X) (X (X)) (X (X)))',\n",
              " 'shape_type': 'mixed',\n",
              " 'lex': {'comment': ['good', 'good', 'good'],\n",
              "  'lid': ['Id1', 'Id2', 'Id3'],\n",
              "  'text': ['Aleksandr Chumakov was born in Moscow, Russia which is led by Sergey Sobyanin. His club was FC Torpedo Moscow where Valery Petrakov is the manager, but he also plays for the Soviet Union National football team.',\n",
              "   'Aleksandr Chumakov, who was born in Moscow where the leader is Sergey Sobyanin, was a member of the Soviet Union national football team. His club is FC Torpedo Moscow where the manager is Valery Petrakov.',\n",
              "   'Aleksandr Chumakov was born in Moscow where the leader is Sergey Sobyanin. His clubs are FC Torpedo Moscow (managed by Valery Petrakov) and the Soviet Union national football team.'],\n",
              "  'lang': ['', '', '']},\n",
              " 'test_category': '',\n",
              " 'dbpedia_links': [],\n",
              " 'links': [],\n",
              " 'reasoning': 'Sergey Sobyanin is the leader of Moscow. Aleksandr Chumakov was born in Moscow. Aleksandr Chumakov played for FC Torpedo Moscow and the Soviet Union national football team. Valery Petrakov is the manager of FC Torpedo Moscow.'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_mul_ref_train = []\n",
        "\n",
        "for i in range(len(dataset_raw_train)):\n",
        "  sample = dataset_raw_train[i]\n",
        "  target_list = sample['lex']['text']\n",
        "  reason = sample['output']\n",
        "  mod_text_list = []\n",
        "  for a in target_list:\n",
        "    b = reason + \" [SEP]\" + a\n",
        "    mod_text_list.append(b)\n",
        "  sample['text'] = mod_text_list\n",
        "  dataset_mul_ref_train.append(sample)\n",
        "\n",
        "dataset_mul_ref_val = []\n",
        "\n",
        "for i in range(len(dataset_raw_val)):\n",
        "  sample = dataset_raw_val[i]\n",
        "  target_list = sample['lex']['text']\n",
        "  reason = sample['output']\n",
        "  mod_text_list = []\n",
        "  for a in target_list:\n",
        "    b = reason + \" [SEP]\" + a\n",
        "    mod_text_list.append(b)\n",
        "  sample['text'] = mod_text_list\n",
        "  dataset_mul_ref_val.append(sample)\n",
        "\n",
        "dataset_mul_ref_test = []\n",
        "\n",
        "for i in range(len(dataset_raw_test)):\n",
        "  sample = dataset_raw_test[i]\n",
        "  target_list = sample['lex']['text']\n",
        "  reason = sample['output']\n",
        "  mod_text_list = []\n",
        "  for a in target_list:\n",
        "    b = reason + \" [SEP]\" + a\n",
        "    mod_text_list.append(b)\n",
        "  sample['text'] = mod_text_list\n",
        "  dataset_mul_ref_test.append(sample)"
      ],
      "metadata": {
        "id": "MD__zcyORvm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=dataset_raw_train))\n",
        "val_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=dataset_raw_val))\n",
        "test_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=dataset_raw_test))"
      ],
      "metadata": {
        "id": "rX_XjSOxRKaj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "YxBusLfKunnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_reasoning = WebNLGDatasetReasoning(train_dataset)\n",
        "dataset_val_reasoning = WebNLGDatasetReasoning(val_dataset)\n",
        "dataset_test_reasoning = WebNLGDatasetReasoning(test_dataset)\n",
        "dataset_train_output = WebNLGDatasetOutput(train_dataset)\n",
        "dataset_val_output = WebNLGDatasetOutput(val_dataset)\n",
        "dataset_test_output = WebNLGDatasetOutput(test_dataset)"
      ],
      "metadata": {
        "id": "212F8MYBa-YJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=3, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T16:43:34.258900Z",
          "iopub.execute_input": "2023-07-05T16:43:34.259839Z",
          "iopub.status.idle": "2023-07-05T16:43:34.272312Z",
          "shell.execute_reply.started": "2023-07-05T16:43:34.259794Z",
          "shell.execute_reply": "2023-07-05T16:43:34.270984Z"
        },
        "trusted": true,
        "id": "fowbYhoMO0Lw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive pretraining"
      ],
      "metadata": {
        "id": "3KkcUmcLO0Lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For STA, we fine-tuned the PLMs on a small amount of labeled data from the target task using a maximum likelihood estimation (MLE) objective. This involves training the model to maximize the likelihood of generating the correct output given the input graph and labeled data. This process helps to further adapt the PLM to the specific requirements of the target task and improve its performance on that task."
      ],
      "metadata": {
        "id": "EaMPSnzSO0Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "pretrain_texts = []\n",
        "for sample in dataset_raw_train:\n",
        "    try:\n",
        "        text = sample['lex']['text'][0]\n",
        "        pretrain_texts.append(text)\n",
        "    except (KeyError, IndexError):\n",
        "        continue\n",
        "\n",
        "tokenized_inputs = tokenizer(pretrain_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_mask = tokenized_inputs['attention_mask']\n",
        "\n",
        "pretrain_data = torch.utils.data.TensorDataset(input_ids, attention_mask)\n",
        "\n",
        "pretrain_loader = torch.utils.data.DataLoader(pretrain_data, batch_size=int(60), shuffle=True)\n",
        "\n",
        "pretrain_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "pretrain_criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "pretrain_epochs = 30  # Set the number of pre-training epochs\n",
        "masking_prob = 0.15  # Probability of masking a token\n",
        "\n",
        "\n",
        "# Prepare validation data\n",
        "val_texts = []\n",
        "for sample in dataset_raw_val:\n",
        "    try:\n",
        "        text = sample['text'][0]\n",
        "        val_texts.append(text)\n",
        "    except (KeyError, IndexError):\n",
        "        continue\n",
        "\n",
        "tokenized_inputs_val = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "input_ids_val = tokenized_inputs_val['input_ids']\n",
        "attention_mask_val = tokenized_inputs_val['attention_mask']\n",
        "\n",
        "val_data = TensorDataset(input_ids_val, attention_mask_val)\n",
        "\n",
        "val_loader = DataLoader(val_data, batch_size=int(60), shuffle=True)\n",
        "\n",
        "early_stopping = EarlyStopping(patience=3, verbose=True)\n",
        "\n",
        "if tokenizer.mask_token is None:\n",
        "    # Manually set a mask token if not already defined\n",
        "    tokenizer.add_tokens(\"[MASK]\")\n",
        "    tokenizer.mask_token = \"[MASK]\"\n",
        "    tokenizer.mask_token_id = tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
        "\n",
        "for epoch in range(pretrain_epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, attention_mask in pretrain_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        batch_size, seq_length = inputs.shape\n",
        "\n",
        "        # Create a mask for randomly selected tokens\n",
        "        mask = torch.rand(inputs.shape) < masking_prob\n",
        "\n",
        "        # Randomly replace selected tokens with [MASK] token\n",
        "        masked_inputs = inputs.clone()\n",
        "        masked_inputs[mask] = tokenizer.mask_token_id\n",
        "\n",
        "        pretrain_optimizer.zero_grad()\n",
        "        outputs = model(input_ids=masked_inputs, attention_mask=attention_mask, decoder_input_ids=inputs)\n",
        "\n",
        "        # Compute the loss only for the masked tokens\n",
        "        masked_logits = outputs.logits[mask]\n",
        "        masked_labels = inputs[mask]\n",
        "        loss = pretrain_criterion(masked_logits.view(-1, masked_logits.size(-1)), masked_labels.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        pretrain_optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(pretrain_data)\n",
        "    print(f\"Pretrain Epoch {epoch+1}/{pretrain_epochs} - loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    for val_inputs, val_attention_mask in val_loader:\n",
        "        val_inputs = val_inputs.to(device)\n",
        "        val_attention_mask = val_attention_mask.to(device)\n",
        "        batch_size, seq_length = val_inputs.shape\n",
        "\n",
        "        mask = torch.rand(val_inputs.shape) < masking_prob\n",
        "        masked_inputs = val_inputs.clone()\n",
        "        masked_inputs[mask] = tokenizer.mask_token_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=masked_inputs, attention_mask=val_attention_mask, decoder_input_ids=val_inputs)\n",
        "            masked_logits = outputs.logits[mask]\n",
        "            masked_labels = val_inputs[mask]\n",
        "            val_loss = pretrain_criterion(masked_logits.view(-1, masked_logits.size(-1)), masked_labels.view(-1))\n",
        "\n",
        "        val_running_loss += val_loss.item() * val_inputs.size(0)\n",
        "\n",
        "    epoch_val_loss = val_running_loss / len(val_data)\n",
        "    print(f\"Val Epoch {epoch+1}/{pretrain_epochs} - loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    early_stopping(epoch_val_loss, model)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T16:43:36.977651Z",
          "iopub.execute_input": "2023-07-05T16:43:36.978033Z",
          "iopub.status.idle": "2023-07-05T17:09:15.775897Z",
          "shell.execute_reply.started": "2023-07-05T16:43:36.978005Z",
          "shell.execute_reply": "2023-07-05T17:09:15.774909Z"
        },
        "trusted": true,
        "id": "vT3E0HVCO0Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For LMA, we first fine-tuned the PLMs on a small amount of task-specific data using a masked language modeling objective. This involves randomly masking some tokens in the input sequence and training the model to predict the masked tokens based on the context provided by the unmasked tokens. This process helps to adapt the PLM to the specific characteristics of the target task and improve its performance on that task."
      ],
      "metadata": {
        "id": "oS19HRKDO0L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning"
      ],
      "metadata": {
        "id": "8RA2Ab27O0L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the data loader\n",
        "#train_data = WebNLGDataset(dataset)\n",
        "batch_size = 32\n",
        "train_loader_reasoning = DataLoader(dataset_train_reasoning, batch_size=batch_size, shuffle=True)\n",
        "val_loader_reasoning = DataLoader(dataset_val_reasoning, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "num_epochs = 20\n",
        "optimizer = torch.optim.AdamW(model_reasoning.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "early_stopping = EarlyStopping(patience=2, verbose=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_reasoning.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader_reasoning:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_reasoning(inputs, labels=targets)\n",
        "        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(dataset_train_reasoning)\n",
        "    print(f\"Train loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    model_reasoning.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_targets in val_loader_reasoning:\n",
        "            val_inputs = val_inputs.to(device)\n",
        "            val_targets = val_targets.to(device)\n",
        "            val_outputs = model_reasoning(val_inputs, labels=val_targets)\n",
        "            val_loss = criterion(val_outputs.logits.view(-1, val_outputs.logits.size(-1)), val_targets.view(-1))\n",
        "            running_val_loss += val_loss.item() * val_inputs.size(0)\n",
        "    epoch_val_loss = running_val_loss / len(dataset_val_reasoning)\n",
        "    print(f\"Val loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    early_stopping(epoch_val_loss, model_reasoning)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSzuf23KgCu_",
        "outputId": "ac2540de-096b-43a8-fa13-16b20abdb4b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.2191\n",
            "Val loss: 0.6961\n",
            "Validation loss decreased (inf --> 0.696091).  Saving model ...\n",
            "Train loss: 0.8757\n",
            "Val loss: 0.6067\n",
            "Validation loss decreased (0.696091 --> 0.606667).  Saving model ...\n",
            "Train loss: 0.7590\n",
            "Val loss: 0.5436\n",
            "Validation loss decreased (0.606667 --> 0.543637).  Saving model ...\n",
            "Train loss: 0.6843\n",
            "Val loss: 0.5030\n",
            "Validation loss decreased (0.543637 --> 0.503049).  Saving model ...\n",
            "Train loss: 0.6181\n",
            "Val loss: 0.4725\n",
            "Validation loss decreased (0.503049 --> 0.472490).  Saving model ...\n",
            "Train loss: 0.5699\n",
            "Val loss: 0.4475\n",
            "Validation loss decreased (0.472490 --> 0.447453).  Saving model ...\n",
            "Train loss: 0.5253\n",
            "Val loss: 0.4293\n",
            "Validation loss decreased (0.447453 --> 0.429292).  Saving model ...\n",
            "Train loss: 0.4941\n",
            "Val loss: 0.4115\n",
            "Validation loss decreased (0.429292 --> 0.411468).  Saving model ...\n",
            "Train loss: 0.4717\n",
            "Val loss: 0.4001\n",
            "Validation loss decreased (0.411468 --> 0.400082).  Saving model ...\n",
            "Train loss: 0.4372\n",
            "Val loss: 0.3809\n",
            "Validation loss decreased (0.400082 --> 0.380887).  Saving model ...\n",
            "Train loss: 0.4153\n",
            "Val loss: 0.3685\n",
            "Validation loss decreased (0.380887 --> 0.368521).  Saving model ...\n",
            "Train loss: 0.3997\n",
            "Val loss: 0.3620\n",
            "Validation loss decreased (0.368521 --> 0.362024).  Saving model ...\n",
            "Train loss: 0.3812\n",
            "Val loss: 0.3510\n",
            "Validation loss decreased (0.362024 --> 0.351018).  Saving model ...\n",
            "Train loss: 0.3651\n",
            "Val loss: 0.3489\n",
            "Validation loss decreased (0.351018 --> 0.348887).  Saving model ...\n",
            "Train loss: 0.3514\n",
            "Val loss: 0.3453\n",
            "Validation loss decreased (0.348887 --> 0.345334).  Saving model ...\n",
            "Train loss: 0.3311\n",
            "Val loss: 0.3354\n",
            "Validation loss decreased (0.345334 --> 0.335404).  Saving model ...\n",
            "Train loss: 0.3231\n",
            "Val loss: 0.3327\n",
            "Validation loss decreased (0.335404 --> 0.332706).  Saving model ...\n",
            "Train loss: 0.3096\n",
            "Val loss: 0.3297\n",
            "Validation loss decreased (0.332706 --> 0.329747).  Saving model ...\n",
            "Train loss: 0.2980\n",
            "Val loss: 0.3235\n",
            "Validation loss decreased (0.329747 --> 0.323513).  Saving model ...\n",
            "Train loss: 0.2882\n",
            "Val loss: 0.3235\n",
            "Validation loss decreased (0.323513 --> 0.323501).  Saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the data loader\n",
        "batch_size = 32\n",
        "train_loader_output = DataLoader(dataset_train_output, batch_size=batch_size, shuffle=True)\n",
        "val_loader_output = DataLoader(dataset_val_output, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "num_epochs = 20\n",
        "optimizer = torch.optim.AdamW(model_output.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "early_stopping = EarlyStopping(patience=2, verbose=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_output.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader_output:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_output(inputs, labels=targets)\n",
        "        loss = criterion(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    epoch_loss = running_loss / len(dataset_train_output)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    model_output.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_inputs, val_targets in val_loader_output:\n",
        "            val_inputs = val_inputs.to(device)\n",
        "            val_targets = val_targets.to(device)\n",
        "            val_outputs = model_output(val_inputs, labels=val_targets)\n",
        "            val_loss = criterion(val_outputs.logits.view(-1, val_outputs.logits.size(-1)), val_targets.view(-1))\n",
        "            running_val_loss += val_loss.item() * val_inputs.size(0)\n",
        "    epoch_val_loss = running_val_loss / len(dataset_val_output)\n",
        "    print(f\"Val loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "    early_stopping(epoch_val_loss, model_output)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZSIZRBe3Qxz",
        "outputId": "1a818f5c-8c9c-489f-ab8e-7bfa05e2bad2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - loss: 1.3365\n",
            "Val loss: 0.9906\n",
            "Validation loss decreased (inf --> 0.990579).  Saving model ...\n",
            "Epoch 2/20 - loss: 1.0979\n",
            "Val loss: 0.9321\n",
            "Validation loss decreased (0.990579 --> 0.932098).  Saving model ...\n",
            "Epoch 3/20 - loss: 1.0402\n",
            "Val loss: 0.9202\n",
            "Validation loss decreased (0.932098 --> 0.920215).  Saving model ...\n",
            "Epoch 4/20 - loss: 0.9916\n",
            "Val loss: 0.8988\n",
            "Validation loss decreased (0.920215 --> 0.898799).  Saving model ...\n",
            "Epoch 5/20 - loss: 0.9604\n",
            "Val loss: 0.8839\n",
            "Validation loss decreased (0.898799 --> 0.883905).  Saving model ...\n",
            "Epoch 6/20 - loss: 0.9338\n",
            "Val loss: 0.8772\n",
            "Validation loss decreased (0.883905 --> 0.877161).  Saving model ...\n",
            "Epoch 7/20 - loss: 0.8983\n",
            "Val loss: 0.8725\n",
            "Validation loss decreased (0.877161 --> 0.872455).  Saving model ...\n",
            "Epoch 8/20 - loss: 0.8777\n",
            "Val loss: 0.8622\n",
            "Validation loss decreased (0.872455 --> 0.862167).  Saving model ...\n",
            "Epoch 9/20 - loss: 0.8538\n",
            "Val loss: 0.8646\n",
            "EarlyStopping counter: 1 out of 2\n",
            "Epoch 10/20 - loss: 0.8326\n",
            "Val loss: 0.8579\n",
            "Validation loss decreased (0.862167 --> 0.857941).  Saving model ...\n",
            "Epoch 11/20 - loss: 0.8018\n",
            "Val loss: 0.8674\n",
            "EarlyStopping counter: 1 out of 2\n",
            "Epoch 12/20 - loss: 0.7945\n",
            "Val loss: 0.8566\n",
            "Validation loss decreased (0.857941 --> 0.856603).  Saving model ...\n",
            "Epoch 13/20 - loss: 0.7712\n",
            "Val loss: 0.8594\n",
            "EarlyStopping counter: 1 out of 2\n",
            "Epoch 14/20 - loss: 0.7584\n",
            "Val loss: 0.8558\n",
            "Validation loss decreased (0.856603 --> 0.855792).  Saving model ...\n",
            "Epoch 15/20 - loss: 0.7446\n",
            "Val loss: 0.8558\n",
            "Validation loss decreased (0.855792 --> 0.855775).  Saving model ...\n",
            "Epoch 16/20 - loss: 0.7344\n",
            "Val loss: 0.8582\n",
            "EarlyStopping counter: 1 out of 2\n",
            "Epoch 17/20 - loss: 0.7078\n",
            "Val loss: 0.8541\n",
            "Validation loss decreased (0.855775 --> 0.854100).  Saving model ...\n",
            "Epoch 18/20 - loss: 0.7016\n",
            "Val loss: 0.8575\n",
            "EarlyStopping counter: 1 out of 2\n",
            "Epoch 19/20 - loss: 0.6816\n",
            "Val loss: 0.8651\n",
            "EarlyStopping counter: 2 out of 2\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/kaggle/working/checkpoint.pt'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T17:38:55.252723Z",
          "iopub.execute_input": "2023-07-05T17:38:55.253653Z",
          "iopub.status.idle": "2023-07-05T17:38:55.468523Z",
          "shell.execute_reply.started": "2023-07-05T17:38:55.253621Z",
          "shell.execute_reply": "2023-07-05T17:38:55.467604Z"
        },
        "trusted": true,
        "id": "6534gR0EO0L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(model_reasoning, 'model_T5_flan_small_2020_reasoning_v2_es')\n",
        "print(\"Model saved successfully.\")\n",
        "torch.save(model_output, 'model_T5_flan_small_2020_output_v2_es')\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T17:52:23.630427Z",
          "iopub.execute_input": "2023-07-05T17:52:23.630812Z",
          "iopub.status.idle": "2023-07-05T17:52:24.092727Z",
          "shell.execute_reply.started": "2023-07-05T17:52:23.630782Z",
          "shell.execute_reply": "2023-07-05T17:52:24.091694Z"
        },
        "trusted": true,
        "id": "2Zl-NvP9O0L1",
        "outputId": "4ba3ba5c-5fa7-496a-ab08-16629b91079c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n",
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "#model = torch.load('/kaggle/input/models/model_T5_flan_small_multi')\n",
        "\n",
        "# Print a confirmation message\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T14:36:05.231909Z",
          "iopub.execute_input": "2023-07-05T14:36:05.232475Z",
          "iopub.status.idle": "2023-07-05T14:36:07.655113Z",
          "shell.execute_reply.started": "2023-07-05T14:36:05.232431Z",
          "shell.execute_reply": "2023-07-05T14:36:07.654022Z"
        },
        "trusted": true,
        "id": "M_ptcPIKO0L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "JYrzGmp7PZDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## are we accounting for the multiple texts targets in the bleu? it doesn't look like it"
      ],
      "metadata": {
        "id": "RfWwLt_XO0L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T17:39:05.974459Z",
          "iopub.execute_input": "2023-07-05T17:39:05.974833Z",
          "iopub.status.idle": "2023-07-05T17:39:05.979637Z",
          "shell.execute_reply.started": "2023-07-05T17:39:05.974805Z",
          "shell.execute_reply": "2023-07-05T17:39:05.978402Z"
        },
        "trusted": true,
        "id": "-H73FAjaO0L2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "from random import sample\n",
        "from tqdm import tqdm\n",
        "\n",
        "test_loader_reasoning = DataLoader(dataset_test_reasoning, batch_size=batch_size, shuffle=False)\n",
        "# also we use the test_dataset to get the multiple references\n",
        "\"\"\"\n",
        "# load the WebNLG validation dataset\n",
        "validation_dataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\n",
        "validation_dataset = [sample for sample in validation_dataset if sample['lex']['text']] # filter out samples with empty targets\n",
        "validation_dataset = validation_dataset[:5]\n",
        "# Select a subset of the validation dataset\n",
        "#subset_size = 10  # Choose the desired subset size\n",
        "#validation_subset = sample(list(validation_dataset), subset_size)\n",
        "validation_data = WebNLGDataset(validation_dataset)\n",
        "\n",
        "# set up the validation data loader\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
        "\"\"\"\n",
        "# switch model to evaluation mode\n",
        "model_reasoning.eval()\n",
        "\n",
        "# generate predictions for the validation dataset\n",
        "predictions_reasoning = []\n",
        "references_reasoning = []\n",
        "with torch.no_grad():\n",
        "    for idx, (inputs, targets) in enumerate(tqdm(test_loader_reasoning, desc='Validation Progress', leave=False)):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model_reasoning.module.generate(inputs, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
        "        # convert token IDs to strings\n",
        "        predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        target_texts = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
        "        # append predicted and target texts for BLEU evaluation\n",
        "        predictions_reasoning.extend(predicted_texts)\n",
        "        references_reasoning.extend([np.array(test_dataset)[i][\"reasoning\"] for i in range(idx*batch_size,min((idx+1)*batch_size, len(test_dataset)))])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T17:39:07.514392Z",
          "iopub.execute_input": "2023-07-05T17:39:07.514770Z",
          "iopub.status.idle": "2023-07-05T17:44:17.556933Z",
          "shell.execute_reply.started": "2023-07-05T17:39:07.514740Z",
          "shell.execute_reply": "2023-07-05T17:44:17.555857Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ylb9U5rO0L2",
        "outputId": "a7f4c441-3117-4d5c-b931-b16d87afdec8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidation Progress:   0%|          | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader_output = DataLoader(dataset_test_output, batch_size=batch_size, shuffle=False)\n",
        "# also we use the test_dataset to get the multiple references\n",
        "\"\"\"\n",
        "# load the WebNLG validation dataset\n",
        "validation_dataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\n",
        "validation_dataset = [sample for sample in validation_dataset if sample['lex']['text']] # filter out samples with empty targets\n",
        "validation_dataset = validation_dataset[:5]\n",
        "# Select a subset of the validation dataset\n",
        "#subset_size = 10  # Choose the desired subset size\n",
        "#validation_subset = sample(list(validation_dataset), subset_size)\n",
        "validation_data = WebNLGDataset(validation_dataset)\n",
        "\n",
        "# set up the validation data loader\n",
        "validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
        "\"\"\"\n",
        "# switch model to evaluation mode\n",
        "model_output.eval()\n",
        "\n",
        "# generate predictions for the validation dataset\n",
        "predictions_output = []\n",
        "references_output = []\n",
        "multiple_references_output = []\n",
        "with torch.no_grad():\n",
        "    for idx, (inputs, targets) in enumerate(tqdm(test_loader_output, desc='Validation Progress', leave=False)):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model_output.module.generate(inputs, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
        "        # convert token IDs to strings\n",
        "        predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        target_texts = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
        "        # append predicted and target texts for BLEU evaluation\n",
        "        predictions_output.extend(predicted_texts)\n",
        "        multiple_references_output.extend([np.array(test_dataset)[i][\"lex\"][\"text\"] for i in range(idx*batch_size,min((idx+1)*batch_size, len(test_dataset)))])\n",
        "        references_output.extend([np.array(test_dataset)[i][\"lex\"][\"text\"][0] for i in range(idx*batch_size,min((idx+1)*batch_size, len(test_dataset)))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWOFFNfV073k",
        "outputId": "765bcb31-ff15-4145-e0bc-26b808cca7b9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidation Progress:   0%|          | 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# switch models to evaluation mode\n",
        "model_reasoning.eval()\n",
        "model_output.eval()\n",
        "\n",
        "# generate predictions for the validation dataset\n",
        "predictions_reasoning = []\n",
        "predictions_output = []\n",
        "references_reasoning = []\n",
        "references_output = []\n",
        "multiple_references_output = []\n",
        "\n",
        "# Use tqdm for progress bar\n",
        "with torch.no_grad():\n",
        "    for idx, (inputs, targets) in enumerate(tqdm(test_loader_reasoning, desc='Validation Progress', leave=False)):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # pass inputs through the reasoning model\n",
        "        outputs_reasoning = model_reasoning.module.generate(inputs, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
        "\n",
        "        # convert token IDs to strings\n",
        "        predicted_texts_reasoning = tokenizer.batch_decode(outputs_reasoning, skip_special_tokens=True)\n",
        "        target_texts_reasoning = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
        "\n",
        "        # append predicted and target texts for BLEU evaluation\n",
        "        predictions_reasoning.extend(predicted_texts_reasoning)\n",
        "        references_reasoning.extend([np.array(test_dataset)[i][\"reasoning\"] for i in range(idx*batch_size,min((idx+1)*batch_size, len(test_dataset)))])\n",
        "\n",
        "        # feed the reasoning model's outputs into the output model\n",
        "        outputs_output = model_output.module.generate(outputs_reasoning, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
        "\n",
        "        # convert token IDs to strings\n",
        "        predicted_texts_output = tokenizer.batch_decode(outputs_output, skip_special_tokens=True)\n",
        "        target_texts_output = tokenizer.batch_decode(targets, skip_special_tokens=True)\n",
        "\n",
        "        # append predicted and target texts for BLEU evaluation\n",
        "        predictions_output.extend(predicted_texts_output)\n",
        "        references_output.extend([np.array(test_dataset)[i][\"lex\"][\"text\"][0] for i in range(idx*batch_size,min((idx+1)*batch_size, len(test_dataset)))])\n",
        "        multiple_references_output.extend([np.array(test_dataset)[i][\"lex\"][\"text\"] for i in range(idx*batch_size,min((idx+1)*batch_size, len(test_dataset)))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNdlv239MAXU",
        "outputId": "be31aeb9-47a4-4122-ef6d-f5dc45a19662"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=2\n",
        "print(test_dataset[i]['modified_triple_sets']['mtriple_set'])\n",
        "print('----')\n",
        "print(\"reasoning prediction: \", predictions_reasoning[i])\n",
        "print('----')\n",
        "print(\"reasoning reference: \", references_reasoning[i])\n",
        "print('----')\n",
        "print(\"output prediction: \", predictions_output[i])\n",
        "print('----')\n",
        "print(\"output reference: \", references_output[i])\n",
        "print('----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNyxAtWC7F-G",
        "outputId": "e2f234c9-bb88-4c52-e5cf-8805ef1c40da"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Allen_Forrest | genre | Rhythm_and_blues', 'Allen_Forrest | birthYear | 1981', 'Allen_Forrest | birthPlace | \"Fort Campbell, KY, raised in Dothan, AL\"', 'Allen_Forrest | background | \"solo_singer\"', 'Allen_Forrest | birthPlace | Fort_Campbell']]\n",
            "----\n",
            "reasoning prediction:  Allen Forrest's genre is rhythm and blues. Allen Forrest was born in 1981. Allen Forrest was born in Fort Campbell, KY, raised in Dothan, AL. Allen Forrest has a background as a solo singer. Allen Forrest was born in Fort Campbell, KY.\n",
            "----\n",
            "reasoning reference:  Allen Forrest's genre is Rhythm and blues. Allen Forrest was born in 1981. Allen Forrest was born in Fort Campbell, KY and raised in Dothan, AL. Allen Forrest has a background as a solo singer.\n",
            "----\n",
            "output prediction:  Allen Forrest was born in 1981 in Fort Campbell, KY, raised in Dothan, AL. He is a rhythm and blues performer who was born in 1981 in Fort Campbell, KY. He has a background as a solo singer and is a rhythm and blues performer.\n",
            "----\n",
            "output reference:  Allen Forrest, a solo singer, was born in 1981 in Fort Campbell, Kentucky and was brought up in Dothan, Alabama. Mr. Forrest plays rhythm and blues music.\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing predictions to a .txt file\n",
        "with open(\"predictions_reasoning_version2_es\", \"w\") as f:\n",
        "    for prediction in predictions_reasoning:\n",
        "        f.write(prediction + \"\\n\")\n",
        "# Writing predictions to a .txt file\n",
        "with open(\"predictions_output_version2_es\", \"w\") as f:\n",
        "    for prediction in predictions_output:\n",
        "        f.write(prediction + \"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T15:04:19.924952Z",
          "iopub.execute_input": "2023-07-05T15:04:19.925784Z",
          "iopub.status.idle": "2023-07-05T15:04:19.938436Z",
          "shell.execute_reply.started": "2023-07-05T15:04:19.925750Z",
          "shell.execute_reply": "2023-07-05T15:04:19.937478Z"
        },
        "trusted": true,
        "id": "LEfhJSKRO0L2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate BLEU scores\n",
        "#bleu = corpus_bleu(predictions, [references])\n",
        "\"\"\"\n",
        "multiple_references = []\n",
        "for i in range(len(validation_dataset)):\n",
        "    multiple_references.append(validation_dataset[i]['lex']['text'])\n",
        "\"\"\"\n",
        "bleu = corpus_bleu(predictions_reasoning, references_reasoning)\n",
        "# bleu_multiple = corpus_bleu(predictions, multiple_references)\n",
        "\n",
        "print(f\"BLEU score reasoning: {bleu.score}\")\n",
        "# print(f\"BLEU score with multiple references: {bleu_multiple.score}\")\n",
        "\n",
        "bleu = corpus_bleu(predictions_output, references_output)\n",
        "bleu_multiple = corpus_bleu(predictions_output, multiple_references_output)\n",
        "\n",
        "print(f\"BLEU score output: {bleu.score}\")\n",
        "print(f\"BLEU score with multiple references: {bleu_multiple.score}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T17:50:22.171695Z",
          "iopub.execute_input": "2023-07-05T17:50:22.172092Z",
          "iopub.status.idle": "2023-07-05T17:50:23.702006Z",
          "shell.execute_reply.started": "2023-07-05T17:50:22.172055Z",
          "shell.execute_reply": "2023-07-05T17:50:23.700923Z"
        },
        "trusted": true,
        "id": "JlP8Hs4vO0L2",
        "outputId": "a41dac12-c94d-4274-a076-5673688e72fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score reasoning: 0.02135099363252779\n",
            "BLEU score output: 0.025183204076820096\n",
            "BLEU score with multiple references: 77.68917956332257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the maximum length of the sublists in multiple_references\n",
        "max_length = max(len(sublist) for sublist in multiple_references)\n",
        "\n",
        "# Writing multiple_references to separate .txt files\n",
        "for i in range(max_length):\n",
        "    with open(f\"references{i}\", \"w\") as f:\n",
        "        for ref_list in multiple_references:\n",
        "            # Writing the ith element if it exists, otherwise an empty line\n",
        "            if i < len(ref_list):\n",
        "                f.write(ref_list[i] + \"\\n\")\n",
        "            else:\n",
        "                f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "q12EFQIzO0L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, determine the maximum length of sublists\n",
        "max_len = max(len(refs) for refs in multiple_references)\n",
        "\n",
        "# Then pad all sublists to that length\n",
        "padded_references = [refs * (max_len // len(refs)) + refs[:max_len % len(refs)] for refs in multiple_references]\n",
        "\n",
        "bleu = corpus_bleu(predictions, references)\n",
        "bleu_multiple = corpus_bleu(predictions, padded_references)\n",
        "\n",
        "print(f\"BLEU score: {bleu.score}\")\n",
        "print(f\"BLEU score with padded references: {bleu_multiple.score}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T00:39:08.784027Z",
          "iopub.execute_input": "2023-07-05T00:39:08.784420Z",
          "iopub.status.idle": "2023-07-05T00:39:12.491724Z",
          "shell.execute_reply.started": "2023-07-05T00:39:08.784371Z",
          "shell.execute_reply": "2023-07-05T00:39:12.489939Z"
        },
        "trusted": true,
        "id": "tZCRKZBUO0L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(multiple_references_output)"
      ],
      "metadata": {
        "id": "IYASQHpN94XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_references"
      ],
      "metadata": {
        "id": "yoJ5-2TL-gv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric('sacrebleu')\n",
        "\n",
        "# First, determine the maximum length of sublists\n",
        "max_len = max(len(refs) for refs in multiple_references_output)\n",
        "\n",
        "# Then pad all sublists to that length\n",
        "padded_references = []\n",
        "for ref in multiple_references_output:\n",
        "  for i in range(max_len-len(ref)):\n",
        "    ref.append(\"\")\n",
        "  padded_references.append(ref)\n",
        "# padded_references = [refs * (max_len // len(refs)) + refs[:max_len % len(refs)] for refs in multiple_references_output]\n",
        "\n",
        "# Now 'padded_references' is a list of lists, where each sublist has the same length.\n",
        "# We can now compute the SacreBLEU score.\n",
        "\n",
        "# Note the change in the compute line\n",
        "score = metric.compute(predictions=predictions_output, references = padded_references)\n",
        "\n",
        "print(f\"SacreBLEU score: {score['score']}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T00:39:29.889557Z",
          "iopub.execute_input": "2023-07-05T00:39:29.889918Z",
          "iopub.status.idle": "2023-07-05T00:39:34.151307Z",
          "shell.execute_reply.started": "2023-07-05T00:39:29.889887Z",
          "shell.execute_reply": "2023-07-05T00:39:34.149323Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlbU_MX3O0L3",
        "outputId": "8c80a439-8be8-48e5-d83f-1cb6545852bf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-c2e077e1ff4d>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('sacrebleu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SacreBLEU score: 41.807585882110935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_chrf\n",
        "# Calculate CHR F++ scores\n",
        "chrf = corpus_chrf(predictions_output, [references_output])\n",
        "chrf_multiple = corpus_chrf(predictions_output, multiple_references_output)\n",
        "print(f\"CHR F++ score: {chrf.score}\")\n",
        "print(f\"CHR F++ score with multiple references: {chrf_multiple.score}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T00:39:46.532954Z",
          "iopub.execute_input": "2023-07-05T00:39:46.533322Z",
          "iopub.status.idle": "2023-07-05T00:39:50.531546Z",
          "shell.execute_reply.started": "2023-07-05T00:39:46.533289Z",
          "shell.execute_reply": "2023-07-05T00:39:50.530539Z"
        },
        "trusted": true,
        "id": "7muY_0s_O0L3",
        "outputId": "f8007997-d70f-40ce-88f1-34ac699a3250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHR F++ score: 62.58863281228343\n",
            "CHR F++ score with multiple references: 55.839157211548326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_chrf\n",
        "# Calculate CHR F++ scores\n",
        "chrf = corpus_chrf(predictions_output, [references_output])\n",
        "chrf_multiple = corpus_chrf(predictions_output, padded_references)\n",
        "print(f\"CHR F++ score: {chrf.score}\")\n",
        "print(f\"CHR F++ score with multiple references: {chrf_multiple.score}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T00:40:05.832831Z",
          "iopub.execute_input": "2023-07-05T00:40:05.833194Z",
          "iopub.status.idle": "2023-07-05T00:40:16.362275Z",
          "shell.execute_reply.started": "2023-07-05T00:40:05.833163Z",
          "shell.execute_reply": "2023-07-05T00:40:16.361205Z"
        },
        "trusted": true,
        "id": "XRS9FedkO0L4",
        "outputId": "f57402c6-b432-4230-b66f-6ba260cae3bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHR F++ score: 62.58863281228343\n",
            "CHR F++ score with multiple references: 55.839157211548326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T17:44:17.559215Z",
          "iopub.execute_input": "2023-07-05T17:44:17.559823Z",
          "iopub.status.idle": "2023-07-05T17:44:29.522407Z",
          "shell.execute_reply.started": "2023-07-05T17:44:17.559789Z",
          "shell.execute_reply": "2023-07-05T17:44:29.521219Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "6LnOkrXCO0L5",
        "outputId": "05deab3c-9a87-4e76-beb2-b2d3a7a7c7d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.30.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Installing collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "metric = load_metric('bertscore')\n",
        "\n",
        "assert len(predictions_output) == len(references_output), \"The number of predictions and references should be the same.\"\n",
        "\n",
        "# Compute the score\n",
        "score = metric.compute(predictions=predictions_output, references=references_output, lang='en')\n",
        "\n",
        "print(f\"BERTScore: {np.mean(score['precision'])}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-05T17:44:29.524502Z",
          "iopub.execute_input": "2023-07-05T17:44:29.524902Z",
          "iopub.status.idle": "2023-07-05T17:45:39.851699Z",
          "shell.execute_reply.started": "2023-07-05T17:44:29.524863Z",
          "shell.execute_reply": "2023-07-05T17:45:39.850390Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361,
          "referenced_widgets": [
            "da6ccd5fd1334160930c2788785709c7",
            "72cfe08f152648ceae7ca8909e508f8c",
            "5a7c9ce436ca44f4a2ff935ac040a903",
            "af447e1f96a747108e3138a9ba23fda7",
            "7531b8e8a4d94296adfc0d15f65830f6",
            "7a14e930e89e4c508ee7aefd490ff110",
            "1642358f4e98456bb1679de54b8d5df2",
            "3f64b9e6a7814fdbae749f6cf8b363ac",
            "e8e25cb2812743b9a369735c81132438",
            "75c682e6f1ff4fc78b2c56b3d2f8e54f",
            "a6e8dd21b6154c96a2174da7ba0f8627",
            "76fcc2f6dba84960b74a3952a3248df3",
            "cc33b362c2a24642bf2e562d3f00334c",
            "b8cc86ea40ed4429b6999667e173bf78",
            "2dfce6b238c24dc6ae3fbba2c116d980",
            "7c1323b6edd64d959f5adb73945dc45c",
            "fba3d4667ae04b3a9a3c877d1d475c45",
            "2c2cb4993c9a4c4788471853c00be17e",
            "34f69bb315094dc49af6db69ba10cd02",
            "e17c89a2340845e4a0d6cbe7614fc8bc",
            "62c44981f3a24a88b398fc1973496ec3",
            "6201321d348a43b1a057ec331c28a15d",
            "b7d82664f8644f48b07f0a9c59af6199",
            "3af4bfe4fca64631943c3f1dd65bfe6c",
            "b5974bd9658a4665a41e07435a8da21e",
            "6dfc8b89969d48aeb6994df328e2559b",
            "4023a75670224b7f9ab42791435a3223",
            "0cdda2f8055f43d7a98ee1d837342ea7",
            "c2c3a06573e5439c9d1d6bbb32389161",
            "0889bc0ddff840e98685d3026d2f61c1",
            "5f6926ea464a4ea9a6bf20b04e38a49e",
            "49d33bdb5b184f6cad1aeccf7ca72311",
            "1be646bb36434524b0f58d417dd78b6e",
            "3d058054fd6b49c3995c576295a0292b",
            "0accf9dbffbd49838bea9f6d586c6835",
            "ec40798778b14595938a1c6037441c39",
            "d5b04ceaba854400a61648cd314a2b66",
            "20f4fb6f142e4d118ebb995ad2660433",
            "b99441d172bc4134a6d35411642bd482",
            "ed8890ecd08140e6b71b7588439180bd",
            "8085bcc432cc490fab8e507af0d80a4e",
            "075467720b244569a5b86c5be6756ac8",
            "3a06dbfcc9954b3e84861f5c3b34b858",
            "9b25337bc5c44110b6804edd5c0d0638",
            "2d0b384e63934e7aa1ea89430de630d2",
            "72628cdb0702477bab34ce27d045763c",
            "32641245501d42f786eadeb07bea8d9c",
            "f81bbd14e05a47fcbc05a143126137c0",
            "7c8f382430574a7980a42a61ac3e984c",
            "c6cabad095104bca8a0ab34af276639f",
            "b71ac72e671345f5a048d7f89ae67c76",
            "3fdf94fcd5234e01b6dd28cc3c4a1a92",
            "35708a4f7145495dad20ebb60f1a1cc9",
            "8a091a8b1bbb4fcd8f3ba1564ff7c671",
            "80b92be63b8441e8a057173c2369c1b8"
          ]
        },
        "id": "0cxbPcgxO0L5",
        "outputId": "586fe8ac-44b0-4c80-e8ed-3002e9751ff0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.92k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da6ccd5fd1334160930c2788785709c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76fcc2f6dba84960b74a3952a3248df3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7d82664f8644f48b07f0a9c59af6199"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d058054fd6b49c3995c576295a0292b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d0b384e63934e7aa1ea89430de630d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTScore: 0.9233890253305436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the tokenizer\n",
        "#tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "# Load the WebNLG dataset\n",
        "dataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\n",
        "dataset = [sample for sample in dataset if sample['lex']['text']]\n",
        "\n",
        "# Create an instance of WebNLGDataset\n",
        "webnlg_dataset = WebNLGDataset(dataset)\n",
        "\n",
        "# Define the index of the example you want to test\n",
        "example_index = 70\n",
        "\n",
        "# Get the input and target texts for the example at the specified index\n",
        "input_text, target_text = webnlg_dataset[example_index]\n",
        "\n",
        "# Decode the input and target texts using the tokenizer\n",
        "decoded_input_text = tokenizer.decode(input_text, skip_special_tokens=True)\n",
        "decoded_target_text = tokenizer.decode(target_text, skip_special_tokens=True)\n",
        "\n",
        "# Print the preprocessed input and target texts\n",
        "print(\"Input Text:\", decoded_input_text)\n",
        "print(\"Target Text:\", decoded_target_text)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-19T18:36:30.294773Z",
          "iopub.execute_input": "2023-05-19T18:36:30.295189Z",
          "iopub.status.idle": "2023-05-19T18:36:31.861708Z",
          "shell.execute_reply.started": "2023-05-19T18:36:30.295152Z",
          "shell.execute_reply": "2023-05-19T18:36:31.860533Z"
        },
        "trusted": true,
        "id": "sPIfDJLsO0MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## seeing how many empty targets there are in the testing set"
      ],
      "metadata": {
        "id": "MtkPyoP1O0MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('web_nlg', 'release_v3.0_en')['test']\n",
        "count_empty_text = 0\n",
        "for sample in dataset:\n",
        "    if not sample['lex']['text']:\n",
        "        count_empty_text += 1\n",
        "\n",
        "print(f\"Number of samples with empty 'lex' 'text' field: {count_empty_text}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T18:48:18.862931Z",
          "iopub.execute_input": "2023-05-18T18:48:18.863745Z",
          "iopub.status.idle": "2023-05-18T18:48:20.403634Z",
          "shell.execute_reply.started": "2023-05-18T18:48:18.863701Z",
          "shell.execute_reply": "2023-05-18T18:48:20.402308Z"
        },
        "trusted": true,
        "id": "eyK0jASAO0ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_samples = len(dataset)\n",
        "print(f\"Total number of samples in the test dataset: {total_samples}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-18T18:48:20.405561Z",
          "iopub.execute_input": "2023-05-18T18:48:20.405994Z",
          "iopub.status.idle": "2023-05-18T18:48:20.412699Z",
          "shell.execute_reply.started": "2023-05-18T18:48:20.405940Z",
          "shell.execute_reply": "2023-05-18T18:48:20.411048Z"
        },
        "trusted": true,
        "id": "ZxV0vqdoO0ML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}